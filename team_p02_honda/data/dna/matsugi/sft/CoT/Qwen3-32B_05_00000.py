import json
import os
from vllm import LLM, SamplingParams

# === è¨­å®š ===
INPUT_JSON = "adversarial_harmful_train_part_05_00000.jsonl"
basename = os.path.basename(INPUT_JSON).split('.jsonl')[0]
OUTPUT_JSONL = str(basename)+"_qwen3-32B.jsonl"
NUM_THREADS = 4  # ä½¿ç”¨ã§ãã‚‹ GPU ãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´

# === ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– ===
llm = LLM(
    model="Qwen/Qwen3-32B",
    trust_remote_code=True,
    tensor_parallel_size=8,
    gpu_memory_utilization=0.80,
    max_model_len=8192,
    max_num_seqs=1
)

sampling_params = SamplingParams(
    temperature=0.3,
    top_p=0.95,
    max_tokens=32768
)

# === ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æ—¢å‡ºIDå–å¾— ===
with open(INPUT_JSON, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f if line.strip()]

done_ids = set()
if os.path.exists(OUTPUT_JSONL):
    with open(OUTPUT_JSONL, "r", encoding="utf-8") as f:
        for line in f:
            try:
                done_ids.add(json.loads(line)["id"])
            except:
                continue

print(f"ğŸ” å†é–‹å‡¦ç†: ã™ã§ã« {len(done_ids)} ä»¶å®Œäº†æ¸ˆã¿")

# === æ¨è«–ãƒ«ãƒ¼ãƒ— ===
for item in data:
    if item["id"] in done_ids:
        continue

    prompt = f"""Problem: {item['question']}

Answer: {item['answer']}

Explain your reasoning step-by-step that would lead to this answer.
<think>"""

    try:
        result = llm.generate(prompt, sampling_params=sampling_params)[0].outputs[0].text
        think_text = result.split("</think>")[0].strip()
        answer = item["answer"].strip()

        item["output"] = f"<think>{think_text}</think>{answer}"

        # ä¿å­˜ï¼ˆå³æ™‚ãƒ‡ã‚£ã‚¹ã‚¯åæ˜ ï¼‰
        with open(OUTPUT_JSONL, "a", encoding="utf-8") as f:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
            f.flush()
            os.fsync(f.fileno())

        done_ids.add(item["id"])
        print(f"âœ… ID {item['id']} å®Œäº†")

    except Exception as e:
        print(f"âš ï¸ ID {item['id']} å¤±æ•—: {e}")
