import os
import pandas as pd
from tqdm import tqdm
import datasets
from huggingface_hub import upload_file

# --- Hugging Face Upload Settings: Repository Name ---
OUTPUT_DATASET_ID = "neko-llm/HLE_SFT_OlympiadBench"

def main():
    """
    HLE_SFT_OlympiadBench Dataset ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã† Main Scriptã€‚

    NOTE:
    é–¢æ•°ã®æ©Ÿèƒ½:
    1. HLE_SFT_OlympiadBench Dataset ã‚’èª­ã¿è¾¼ã‚€ã€‚
    2. HLE_SFT_OlympiadBench Dataset ã® output, answer ã® [] ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    3. ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ Hugging Face ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    """
    print("ğŸš€ HLE_SFT_OlympiadBench Dataset ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
    output_filename = "hle_sft_olympiadbench_fix.csv"

    # --- Resume Logic ---
    processed_ids = []
    if os.path.exists(output_filename):
        print(f"ğŸ“„ æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: '{output_filename}'")
        try:
            existing_df = pd.read_csv(output_filename)
            if 'id' in existing_df.columns:
                processed_ids = existing_df['id'].dropna().tolist()
                print(f"âœ… {len(processed_ids)} ä»¶ã®å‡¦ç†æ¸ˆã¿å•é¡Œã‚’ç™ºè¦‹ã€‚ã“ã‚Œã‚‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        except pd.errors.EmptyDataError:
            print("âš ï¸ æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚æ–°è¦ã§é–‹å§‹ã—ã¾ã™ã€‚")
        except Exception as e:
            print(f"âš ï¸ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚æ–°è¦ã§é–‹å§‹ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")

    # --- HLE_SFT_OlympiadBench Dataset Loading ---
    print("ğŸ“¥ HLE_SFT_OlympiadBench Dataset ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    try:
        # ã¾ãšãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if os.path.exists("hle_sft_olympiadbench_fix.jsonl"):
            print("ğŸ“„ ãƒ­ãƒ¼ã‚«ãƒ«ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™...")
            olympiadbench_dataset = datasets.load_dataset('json', data_files='hle_sft_olympiadbench_fix.jsonl', split='train')
        else:
            # Hugging Face Dataset ã‚’èª­ã¿è¾¼ã¿ï¼ˆtrainãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
            # ã‚¹ã‚­ãƒ¼ãƒå•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€ãƒ­ãƒ¼ã‚«ãƒ«èª­ã¿è¾¼ã¿ã‚’å„ªå…ˆ
            print("ğŸŒ Hugging Face ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
            olympiadbench_dataset = datasets.load_dataset("neko-llm/HLE_SFT_OlympiadBench", split='train')

        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†ã€‚{len(olympiadbench_dataset)} ä»¶ã®å•é¡Œã‚’ç™ºè¦‹ã€‚")

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        print("ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
        sample = olympiadbench_dataset[0]
        for key in sample.keys():
            print(f"  {key}: {str(sample[key])[:100]}...")

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ”„ ãƒ­ãƒ¼ã‚«ãƒ«JSONLãƒ•ã‚¡ã‚¤ãƒ«ã§ã®èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œä¸­...")
        try:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥èª­ã¿è¾¼ã¿
            olympiadbench_dataset = datasets.load_dataset('json', data_files='hle_sft_olympiadbench_fix.jsonl', split='train')
            print(f"âœ… ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®èª­ã¿è¾¼ã¿æˆåŠŸã€‚{len(olympiadbench_dataset)} ä»¶ã®å•é¡Œã‚’ç™ºè¦‹ã€‚")
        except Exception as fallback_error:
            print(f"âŒ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚‚å¤±æ•—: {fallback_error}")
            return


    # --- ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° ---
    print("ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
    cleaned_data = []

    for i in tqdm(range(len(olympiadbench_dataset))):
        sample = olympiadbench_dataset[i]

        # outputã¨answerãŒãƒªã‚¹ãƒˆå‹ã®å ´åˆã€æœ€åˆã®è¦ç´ ã‚’å–ã‚Šå‡ºã™
        if isinstance(sample['output'], list) and len(sample['output']) > 0:
            output_text = sample['output'][0]
        else:
            output_text = sample['output']

        if isinstance(sample['answer'], list) and len(sample['answer']) > 0:
            answer_text = sample['answer'][0]
        else:
            answer_text = sample['answer']

        # output, answer ãã‚Œãã‚Œæ–‡å­—åˆ—ã®å ´åˆã®ã¿ã€å…ˆé ­ã¨æœ«å°¾ã® [ ] ã‚’å‰Šé™¤
        if isinstance(output_text, str):
            # å…ˆé ­ã® [ ã‚’å‰Šé™¤
            if output_text.startswith('['):
                output_text = output_text[1:]
            # æœ«å°¾ã® ] ã‚’å‰Šé™¤
            if output_text.endswith(']'):
                output_text = output_text[:-1]
            # NOTE: output ã®å ´åˆã¯ Prefixã« <think></think> ã‚’è¿½åŠ ï¼ˆå…ƒã®æ¨è«–ãŒãªã„å ´åˆï¼‰
            if not output_text.strip().startswith('<think>'):
                output_text = f"<think></think>{output_text}"

        if isinstance(answer_text, str):
            # å…ˆé ­ã® [ ã‚’å‰Šé™¤
            if answer_text.startswith('['):
                answer_text = answer_text[1:]
            # æœ«å°¾ã® ] ã‚’å‰Šé™¤
            if answer_text.endswith(']'):
                answer_text = answer_text[:-1]

        # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        cleaned_sample = {
            'id': sample['id'],
            'question': sample['question'],
            'output': output_text,
            'answer': answer_text
        }
        cleaned_data.append(cleaned_sample)

    # --- ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ ---
    print(f"ğŸ’¾ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­: {output_filename}")
    cleaned_df = pd.DataFrame(cleaned_data)
    cleaned_df.to_csv(output_filename, index=False, encoding='utf-8')

    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†!")
    print(f"   - å‡¦ç†ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(cleaned_data)}")
    print(f"   - ä¿å­˜å…ˆ: {output_filename}")

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    print("\nğŸ“‹ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
    for i in range(min(3, len(cleaned_data))):
        sample = cleaned_data[i]
        print(f"  ã‚µãƒ³ãƒ—ãƒ« {i+1}:")
        print(f"    ID: {sample['id']}")
        print(f"    Answer: {repr(sample['answer'])}")
        print(f"    Output (å…ˆé ­100æ–‡å­—): {repr(sample['output'][:100])}...")

    # --- è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã®ä¿å­˜ã¨Hugging Face ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    try:
        print("\nğŸ”„ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹...")
        final_df = cleaned_df  # æ—¢ã«DataFrameã¨ã—ã¦æº–å‚™ã•ã‚Œã¦ã„ã‚‹

        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        success_count = len(final_df)
        total_count = len(cleaned_data)

        print(f"ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ:")
        print(f"   ä¿å­˜ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {success_count}")
        print(f"   å‡¦ç†å¯¾è±¡ç·æ•°: {total_count}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ª: 100% (ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿)")

        # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚’å–å¾—
        base_filename = os.path.splitext(output_filename)[0]

        # --- 2. Parquetå½¢å¼ã§ã®ä¿å­˜ ---
        parquet_filename = f"{base_filename}.parquet"
        print(f"ğŸ’¾ Parquetå½¢å¼ã§ä¿å­˜ä¸­: {parquet_filename}")
        final_df.to_parquet(parquet_filename, index=False)

        # --- 3. JSONLå½¢å¼ã§ã®ä¿å­˜ ---
        jsonl_filename = f"{base_filename}.jsonl"
        print(f"ğŸ’¾ JSONLå½¢å¼ã§ä¿å­˜ä¸­: {jsonl_filename}")
        final_df.to_json(jsonl_filename, orient='records', lines=True, force_ascii=False)

        # --- 4. /data ã« physics_qa_dataset.jsonl ã‚’ä¿å­˜ ---
        # dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        os.makedirs("data", exist_ok=True)

        physics_qa_dataset_filename = "data/physics_qa_dataset.jsonl"
        print(f"ğŸ’¾ physics_qa_dataset.jsonl ã‚’ä¿å­˜ä¸­: {physics_qa_dataset_filename}")
        final_df.to_json(physics_qa_dataset_filename, orient='records', lines=True, force_ascii=False)

        # --- Hugging Face Datasetsã«3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
        print(f"ğŸ“¤ {OUTPUT_DATASET_ID} ã«3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")

        # 1. CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“„ CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=output_filename,
            path_in_repo=output_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        # 2. Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“Š Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=parquet_filename,
            path_in_repo=parquet_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        # 3. JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“ JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=jsonl_filename,
            path_in_repo=jsonl_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        # 4. /data ã« physics_qa_dataset.jsonl ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“ data/physics_qa_dataset.jsonl ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=physics_qa_dataset_filename,
            path_in_repo=physics_qa_dataset_filename,  # ã“ã‚Œã§ "data/physics_qa_dataset.jsonl" ã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        print(f"âœ… 3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {OUTPUT_DATASET_ID}")

        # --- çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º ---
        print(f"\nğŸ“Š å‡¦ç†çµ±è¨ˆ:")
        print(f"  - ç·å‡¦ç†æ•°: {total_count}")
        print(f"  - æˆåŠŸæ•°: {success_count}")
        print(f"  - æˆåŠŸç‡: {success_count/total_count*100:.1f}%")

        print(f"\nğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - CSV: {output_filename}")
        print(f"  - Parquet: {parquet_filename}")
        print(f"  - JSONL: {jsonl_filename}")
        print(f"  - Physics QA Dataset: {physics_qa_dataset_filename}")

        print(f"\nğŸŒ Hugging Face Datasetsã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆ:")
        print(f"  - Repository: {OUTPUT_DATASET_ID}")
        print(f"    â”œâ”€ {output_filename}")
        print(f"    â”œâ”€ {parquet_filename}")
        print(f"    â”œâ”€ {jsonl_filename}")
        print(f"    â””â”€ {physics_qa_dataset_filename}")

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¾ãŸã¯Hugging Face Hubã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

    print("\n--- Processing complete ---")

if __name__ == "__main__":
    main()
