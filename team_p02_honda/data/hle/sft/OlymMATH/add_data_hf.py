from datasets import load_dataset, concatenate_datasets, DatasetDict

# Hugging Faceãƒªãƒã‚¸ãƒˆãƒªID
repo_id = "tentatani/HLE_SFT_OlymMATH"
# ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚ã‚‹ã€Œè¿½åŠ ã—ãŸã„ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
new_data_file_path = "/Users/tentatani/uv_project/evaluate_cot/dataset_8.jsonl" 

# 1. Hubã‹ã‚‰æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
print(f"'{repo_id}' ã‹ã‚‰æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
existing_dataset = load_dataset(repo_id)

# 2. ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰è¿½åŠ ã—ãŸã„æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
print(f"'{new_data_file_path}' ã‹ã‚‰æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
new_data = load_dataset("json", data_files=new_data_file_path)

# 3. æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã™ã‚‹
print("ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦ã„ã¾ã™...")
# 'train' ã‚¹ãƒ—ãƒªãƒƒãƒˆåŒå£«ã‚’çµåˆ
combined_dataset = concatenate_datasets([existing_dataset['train'], new_data['train']])

# çµåˆã—ãŸã‚‚ã®ã‚’å†åº¦ 'train' ã‚¹ãƒ—ãƒªãƒƒãƒˆã¨ã—ã¦DatasetDictã«æ ¼ç´ã™ã‚‹
final_dataset = DatasetDict({
    'train': combined_dataset
})

# 4. çµåˆå¾Œã®å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hubã«ãƒ—ãƒƒã‚·ãƒ¥ï¼ˆä¸Šæ›¸ãï¼‰ã™ã‚‹
print(f"çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ '{repo_id}' ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦ã„ã¾ã™...")
final_dataset.push_to_hub(repo_id)

print("\nğŸš€ ãƒ‡ãƒ¼ã‚¿è¿½åŠ å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
print(f"å…ƒã®ãƒ‡ãƒ¼ã‚¿æ•°: {len(existing_dataset['train'])}")
print(f"è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿æ•°: {len(new_data['train'])}")
print(f"åˆè¨ˆãƒ‡ãƒ¼ã‚¿æ•°: {len(final_dataset['train'])}")
