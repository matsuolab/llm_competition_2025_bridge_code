"""
HLE ã® SFT ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ æ¾å°¾ç ”ã‚³ãƒ³ãƒšã®æå‡ºç”¨ Hugging Face ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã® Scriptã€‚
"""

import os
from tqdm import tqdm
import json
import datasets
from huggingface_hub import upload_file, create_repo, HfApi

# --- Hugging Face Upload Settings: Repository Name ---
OUTPUT_DATASET_ORG = "weblab-llm-competition-2025-bridge"

# ãƒãƒ¼ãƒ  ã­ã“ğŸ± Prefix
PREFIX_NAME = "neko-prelim-"

# --- Target Dataset ID ---
TARGET_DATASET_LIST = [
  # id: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID, config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®Subsetå, split: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®Splitå
  # å­¦ç¿’Configã‚’å‚ç…§: https://www.notion.so/Config-2479dd6b4cc28002b151f788920d51b4
  {"id": "neko-llm/HLE_SFT_OlymMATH",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_medical",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_PHYBench",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_OlympiadBench",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_biology",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_OpenMathReasoning", "config": "cot", "split": "train"},
  {"id": "neko-llm/HLE_SFT_MixtureOfThoughts", "config": "MoT_science", "split": "train"},
  {"id": "neko-llm/HLE_SFT_PhysReason",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_GPQA_Diamond",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_OpenThoughts-114k",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_general", "config": "medcal", "split": "train"},
  {"id": "neko-llm/HLE_RL_Olympiadbench-v2",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_LIMO",  "split": "train"},
  {"id": "neko-llm/HLE_SFT_LIMO-v2",  "split": "train"},
]

# è¦ä»¶å®šç¾©
# 1. 1ã¤ãšã¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ã€‚
# 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æŒ‡å®šã®åå‰ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§Hugging Faceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
  # SUBMIT_DATASET_FORMAT/{ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå}
  # ãŸã ã—ã€neko-llm/ ã¨ã„ã†Prefixã¯å‰Šã‚‹ã€‚

# Read Dataset -> Upload Dataset
for dataset_id in TARGET_DATASET_LIST:

  print(f"ğŸ” Reading dataset: {dataset_id['id']}")

  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã€configã€splitã‚’æŒ‡å®šã—ã¦èª­ã¿è¾¼ã‚€ã€‚
  if dataset_id.get('config'):
    dataset = datasets.load_dataset(dataset_id['id'], dataset_id['config'], split=dataset_id['split'])
  else:
    dataset = datasets.load_dataset(dataset_id['id'], split=dataset_id['split'])

  print(f"âœ… Dataset read successfully: {dataset_id['id']}")
  print(f"Dataset size: {len(dataset)}")

  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‹ã‚‰neko-llm/ Prefixã‚’å‰Šã‚‹
  dataset_name = dataset_id['id'].replace('neko-llm/', '')

  # JSONLãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ
  jsonl_filename = f"{PREFIX_NAME}{dataset_name}.jsonl"

  # å®Œå…¨ãªãƒªãƒã‚¸ãƒˆãƒªIDã‚’ä½œæˆ
  full_repo_id = f"{OUTPUT_DATASET_ORG}/{jsonl_filename.replace('.jsonl', '')}"

  print(f"ğŸ’¾ Saving dataset to JSONL: {jsonl_filename}")

  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’JSONLå½¢å¼ã§ä¿å­˜
  with open(jsonl_filename, 'w', encoding='utf-8') as f:
    for item in tqdm(dataset, desc="Saving to JSONL"):
      json_line = json.dumps(item, ensure_ascii=False)
      f.write(json_line + '\n')

  print(f"âœ… JSONL file saved: {jsonl_filename}")
  print(f"ğŸ“¤ Uploading to Hugging Face: {full_repo_id}")

  try:
    # ãƒªãƒã‚¸ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    api = HfApi()
    try:
      api.repo_info(repo_id=full_repo_id, repo_type="dataset")
      print(f"ğŸ“ Repository exists: {full_repo_id}")
    except Exception:
      print(f"ğŸ†• Creating new repository: {full_repo_id}")
      create_repo(
        repo_id=full_repo_id,
        repo_type="dataset",
        private=False
      )
      print(f"âœ… Repository created: {full_repo_id}")

    # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Hugging Faceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    upload_file(
      path_or_fileobj=jsonl_filename,
      path_in_repo=jsonl_filename,
      repo_id=full_repo_id,
      repo_type="dataset"
    )

    print(f"âœ… Successfully uploaded: {jsonl_filename}")
    print(f"ğŸ”— Dataset URL: https://huggingface.co/datasets/{full_repo_id}")

  except Exception as e:
    print(f"âŒ Failed to upload {dataset_id['id']}: {str(e)}")
    print(f"ğŸ” Error details: {type(e).__name__}")
    continue

  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
  try:
    os.remove(jsonl_filename)
    print(f"ğŸ—‘ï¸ Cleaned up local file: {jsonl_filename}")
  except Exception as e:
    print(f"âš ï¸ Could not remove local file {jsonl_filename}: {str(e)}")

  print("-" * 80)

print("ğŸ‰ All datasets processing completed!")
