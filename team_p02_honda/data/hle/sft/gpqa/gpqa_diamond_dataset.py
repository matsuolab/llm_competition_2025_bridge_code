import os
import pandas as pd
import requests
from tqdm import tqdm
import time
import json
import re
import datasets
from huggingface_hub import upload_file

# --- OpenRouter API Settings ---
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"
MODEL_NAME = "deepseek/deepseek-r1-0528:free"
YOUR_SITE_URL = "http://localhost"

# --- App Name ---
APP_NAME = "GPQA add CoT Dataset"

# --- Hugging Face Upload Settings: Repository Name ---
OUTPUT_DATASET_ID = "neko-llm/HLE_SFT_GPQA_Diamond"


def generate_prompt_for_gpqa_dataset(question: str, explanation: str, subdomain: str) -> str:
  """
  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°

  - GPQA Dataset ã®ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã£ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
  """

  return f"""
    You are an expert in {subdomain}.

    ## Rules
    - Using the information from question/explanation, please generate a reasoning process (Reasoning/CoT) and answer.
      - question: {question}
      - explanation: {explanation}

    ## Output Format
    - Please output the reasoning process (Reasoning/CoT) and answer in the following format:
    ```json
    {{
        "reasoning": "reasoning process (Reasoning/CoT) content",
        "answer": "answer content"
    }}
    ```
  """

def generate_judge_prompt(generated_answer: str, correct_answer: str, question: str, subdomain: str) -> str:
    """
    LLMåˆ¤å®šç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°

    Args:
        generated_answer: ç”Ÿæˆã•ã‚ŒãŸå›ç­”
        correct_answer: æ­£è§£
        question: å…ƒã®è³ªå•
        subdomain: åˆ†é‡

    Returns:
        str: åˆ¤å®šç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    return f"""
You are an expert judge for evaluating the correctness of answers to scientific questions in {subdomain}.
Decide if the generated answer is semantically equivalent to the reference answer.

## Inputs
[Question]
{question}

[Generated Answer]
{generated_answer}

[Reference Answer (Correct)]
{correct_answer}

## What to do (follow silently)
1) Extract the final asserted answer from each text:
   - Ignore explanations, apologies, or prefaces.
   - If multiple values appear, use the final, explicitly stated conclusion (e.g., after "Answer:", or the last value/choice).
2) Normalize notation for BOTH answers before comparing:
   - Trim spaces, punctuation, and surrounding quotes.
   - Scientific notation: treat these as equivalent forms:
     10^-4, 10^{{-4}}, 1e-4, 1Ã—10^-4, 1Â·10^-4, 0.0001
   - Minus sign: treat "-" and "âˆ’" as identical.
   - Multiplication sign: treat "Ã—", "Â·", "*" as identical.
   - LaTeX vs plain text: interpret \(10^{{-4}}\) == 10^-4, \frac{{1}}{{2}} == 0.5, etc.
   - Units:
     * Be case-insensitive where standard (eV == electronvolt).
     * Accept SI prefixes & micro symbol variants (Î¼ == u).
     * If units are convertible and dimensionally the same (e.g., eV vs J), convert conceptually and compare values.
     * If the generated answer omits an unambiguous unit but the numeric value matches within tolerance, treat as equivalent.
   - Percent/fraction equivalence: 10% == 0.10.
   - Thousands separators and locale decimals: ignore commas, treat "." as decimal point.
3) Compare using these criteria:
   - Multiple choice: if the question lists options, map letters (A/B/C/â€¦) to their option text and compare to the reference.
   - Exact strings (non-numeric): compare case-insensitively after normalization; accept common synonyms (True/Yes, False/No).
   - Numbers:
     * Consider correct if values match exactly AFTER normalization OR
       |generated - reference| â‰¤ max(1e-12, 0.01Ã—|reference|)  (â‰ˆ1% relative tolerance).
     * Also accept rounding consistent with the significant figures of the reference.
   - Chemical formulas: compare structural/stoichiometric equivalence; ignore whitespace and typical state annotations.
4) Mark INCORRECT if there is a sign error, wrong order of magnitude, wrong (non-convertible) units/dimensions, or a different option than the reference.

## Output Format
Respond with EXACTLY one token:
- "CORRECT"  (semantically equivalent after normalization)
- "INCORRECT" (otherwise)

Only output "CORRECT" or "INCORRECT".
"""



def judge_answer_by_llm(generated_answer, correct_answer: str, question: str = "", subdomain: str = "") -> bool:
    """
    OpenRouterçµŒç”±ã§LLMã«å›ç­”ã®æ­£å½“æ€§ã‚’åˆ¤å®šã•ã›ã‚‹é–¢æ•°

    Args:
        generated_answer: ç”Ÿæˆã•ã‚ŒãŸå›ç­”ï¼ˆstr ã¾ãŸã¯ listï¼‰
        correct_answer: æ­£è§£
        question: å…ƒã®è³ªå•ï¼ˆåˆ¤å®šã®å‚è€ƒç”¨ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        bool: å›ç­”ãŒæ­£ã—ã„å ´åˆTrueã€é–“é•ã„ã®å ´åˆFalse
    """
    # generated_answerãŒãƒªã‚¹ãƒˆå‹ã®å ´åˆã¯æ–‡å­—åˆ—ã«å¤‰æ›
    if isinstance(generated_answer, list):
        generated_answer = str(generated_answer)
    elif not isinstance(generated_answer, str):
        generated_answer = str(generated_answer)

    if not OPENROUTER_API_KEY:
        print("âš ï¸ OPENROUTER_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å˜ç´”ãªæ–‡å­—åˆ—æ¯”è¼ƒã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return generated_answer.strip().lower() == correct_answer.strip().lower()

    # åˆ¤å®šç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
    judge_prompt = generate_judge_prompt(generated_answer, correct_answer, question, subdomain)

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "HTTP-Referer": YOUR_SITE_URL,
        "X-Title": f"{APP_NAME} - Answer Judge",
        "Content-Type": "application/json"
    }

    data = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": judge_prompt}],
        "temperature": 0.1,  # åˆ¤å®šã®ä¸€è²«æ€§ã®ãŸã‚ä½ã„æ¸©åº¦ã‚’ä½¿ç”¨
        "max_tokens": 10     # "CORRECT" ã¾ãŸã¯ "INCORRECT" ã®ã¿æœŸå¾…
    }

    last_error = ""

    for attempt in range(3):  # 3å›ã¾ã§å†è©¦è¡Œ
        try:
            response = requests.post(OPENROUTER_API_URL, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            json_response = response.json()

            if 'choices' in json_response and len(json_response['choices']) > 0:
                content = json_response['choices'][0]['message']['content'].strip().upper()

                print(f"ğŸ” Judge API Response: {content}")

                if "CORRECT" in content:
                    return True
                elif "INCORRECT" in content:
                    return False
                else:
                    print(f"âš ï¸ äºˆæœŸã—ãªã„åˆ¤å®šçµæœ: {content}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ãªæ–‡å­—åˆ—æ¯”è¼ƒ
                    return generated_answer.strip().lower() == correct_answer.strip().lower()
            else:
                last_error = f"âŒ Judge API response missing valid content. Response: {json_response}"
                print(last_error)

        except requests.exceptions.HTTPError as e:
            if e.response.status_code in [402, 429]:
                try:
                    error_details = e.response.json().get('error', {}).get('message', '')
                except json.JSONDecodeError:
                    error_details = e.response.text
                print(f"âŒ Judge API credit/rate limit error ({e.response.status_code}): {error_details}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ãªæ–‡å­—åˆ—æ¯”è¼ƒ
                return generated_answer.strip().lower() == correct_answer.strip().lower()
            else:
                last_error = f"âŒ Judge API HTTP Error: {e}"
        except Exception as e:
            last_error = f"âŒ Judge API unknown error: {e}"

        time.sleep(1)  # å†è©¦è¡Œå‰ã«1ç§’å¾…æ©Ÿ

    print(f"âŒ Judge API failed after 3 attempts: {last_error}")
    print("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦å˜ç´”ãªæ–‡å­—åˆ—æ¯”è¼ƒã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    return generated_answer.strip().lower() == correct_answer.strip().lower()

def extract_final_answer_from_cot(cot_response: str) -> str:
    """
    CoTãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰æœ€çµ‚å›ç­”ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
    JSONå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰"answer"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹

    Args:
        cot_response: CoTç”ŸæˆAPIã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆJSONå½¢å¼ï¼‰

    Returns:
        str: æŠ½å‡ºã•ã‚ŒãŸæœ€çµ‚å›ç­”
    """
    # ã¾ãšJSONå½¢å¼ã§ã®è§£æã‚’è©¦è¡Œ
    parsed_json = parse_json_response(cot_response)
    if parsed_json and 'answer' in parsed_json:
        return parsed_json['answer'].strip()

    # JSONã§ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
    # <think>ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã€</think>ã®å¾Œã®å†…å®¹ã‚’æœ€çµ‚å›ç­”ã¨ã™ã‚‹
    think_end_pattern = r'</think>\s*(.*?)$'
    match = re.search(think_end_pattern, cot_response, re.DOTALL | re.IGNORECASE)
    if match:
        final_answer = match.group(1).strip()
        if final_answer:
            return final_answer

    # <think>ã‚¿ã‚°ãŒãªã„å ´åˆã€æœ€å¾Œã®è¡Œã¾ãŸã¯æ®µè½ã‚’æœ€çµ‚å›ç­”ã¨ã™ã‚‹
    lines = cot_response.strip().split('\n')
    for line in reversed(lines):
        line = line.strip()
        if line and not line.startswith('#') and not line.startswith('-'):
            return line

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ä½“ã‚’ãã®ã¾ã¾è¿”ã™
    return cot_response.strip()


def generate_cot_with_openrouter(prompt: str) -> tuple[str, str]:
    """
    OpenRouter API ã‚’ä½¿ã£ã¦ CoT ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°

    Returns:
        tuple[status, response]: (æˆåŠŸ/å¤±æ•—, APIå¿œç­”ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
    """
    if not OPENROUTER_API_KEY:
        return "failure", "âŒ OPENROUTER_API_KEY environment variable not set."

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "HTTP-Referer": YOUR_SITE_URL,
        "X-Title": APP_NAME,
        "Content-Type": "application/json"
    }

    data = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": prompt}]
    }

    last_error = ""

    for _attempt in range(3):  # 3å›ã¾ã§å†è©¦è¡Œ
        try:
            response = requests.post(OPENROUTER_API_URL, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            json_response = response.json()
            # print(f"ğŸ” OpenRouter API ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {json_response}")

            if 'choices' in json_response and len(json_response['choices']) > 0:
                content = json_response['choices'][0]['message']['content']

                # reasoningã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆDeepSeek R1ç‰¹æœ‰ï¼‰
                reasoning = ""
                if 'reasoning' in json_response['choices'][0]:
                    reasoning = json_response['choices'][0]['reasoning']
                else:
                    print("âš ï¸ reasoning ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚contentã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

                print(f"ğŸ” OpenRouter API Content: {content}")
                if reasoning:
                    print(f"ğŸ” OpenRouter API Reasoning: {reasoning}")

                # contentã®ã¿ã‚’è¿”ã™ï¼ˆçµ±ä¸€ã•ã‚ŒãŸæˆ»ã‚Šå€¤ï¼‰
                return "success", content.strip()
            else:
                last_error = f"âŒ API response missing valid content. Response: {json_response}"
                print(f"âŒ API response missing valid content. Response:: {json_response}")

        except requests.exceptions.HTTPError as e:
            if e.response.status_code in [402, 429]:
                try:
                    error_details = e.response.json().get('error', {}).get('message', '')
                except json.JSONDecodeError:
                    error_details = e.response.text
                final_error_message = f"âŒ Possible credit exhaustion or rate limit ({e.response.status_code}): {error_details}"
                return "failure", final_error_message
            else:
                last_error = f"âŒ HTTP Error: {e}"
        except Exception as e:
            last_error = f"âŒ An unknown error occurred: {e}"

        time.sleep(1)  # å†è©¦è¡Œå‰ã«1ç§’å¾…æ©Ÿ

    return "failure", last_error


def parse_json_response(response_text: str) -> dict:
    """
    API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ JSON ã‚’æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹ã™ã‚‹é–¢æ•°

    Args:
        response_text: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        dict: ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸ JSON ãƒ‡ãƒ¼ã‚¿ã€å¤±æ•—æ™‚ã¯ç©ºã®è¾æ›¸
    """
    if not response_text or not response_text.strip():
        print("âš ï¸ Empty response text provided to JSON parser")
        return {}

    # è¤‡æ•°ã®JSONæŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
    json_patterns = [
        # ```json ... ``` ã®å½¢å¼
        r'```json\s*(.*?)\s*```',
        # ``` ... ``` ã®å½¢å¼ï¼ˆjsonã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—ï¼‰
        r'```\s*(.*?)\s*```',
        # { ... } ã®å½¢å¼ã‚’ç›´æ¥æ¢ã™
        r'(\{.*?\})',
    ]

    for pattern in json_patterns:
        try:
            json_match = re.search(pattern, response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
                parsed = json.loads(json_str)

                # æœŸå¾…ã™ã‚‹ã‚­ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if isinstance(parsed, dict) and ('reasoning' in parsed or 'answer' in parsed):
                    return parsed

        except (json.JSONDecodeError, AttributeError) as e:
            continue  # æ¬¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ

    # æœ€å¾Œã®æ‰‹æ®µ: å…¨ä½“ã‚’JSONã¨ã—ã¦è©¦è¡Œ
    try:
        parsed = json.loads(response_text.strip())
        if isinstance(parsed, dict):
            return parsed
    except json.JSONDecodeError:
        pass

    # JSONè§£æã«å®Œå…¨ã«å¤±æ•—ã—ãŸå ´åˆ
    print(f"âš ï¸ ã™ã¹ã¦ã®JSONè§£æãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_text[:200]}...")
    return {}


def generate_readme_content(df: pd.DataFrame, csv_filename: str, parquet_filename: str, jsonl_filename: str, total_count: int, success_count: int) -> str:
    """
    README.md ã®å†…å®¹ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°

    Args:
        df: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®DataFrame
        csv_filename: CSVãƒ•ã‚¡ã‚¤ãƒ«å
        parquet_filename: Parquetãƒ•ã‚¡ã‚¤ãƒ«å
        jsonl_filename: JSONLãƒ•ã‚¡ã‚¤ãƒ«å
        total_count: ç·å‡¦ç†æ•°
        success_count: æˆåŠŸæ•°

    Returns:
        str: README.mdã®å†…å®¹
    """
    # ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã‚’å–å¾—
    columns = list(df.columns) if not df.empty else []

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆæœ€åˆã®1ä»¶ï¼‰
    sample_data = df.iloc[0].to_dict() if not df.empty else {}

    readme_content = f"""# HLE SFT GPQA Diamond Dataset

## æ¦‚è¦

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€GPQA (Graduate-level Google-proof Q&A) Diamond ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åŸºã«ã€Chain of Thought (CoT) æ¨è«–ã‚’è¿½åŠ ã—ã¦ç”Ÿæˆã•ã‚ŒãŸSupervised Fine-Tuning (SFT) ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚

å°‚é–€çš„ãªç§‘å­¦åˆ†é‡ï¼ˆç‰©ç†å­¦ã€åŒ–å­¦ã€ç”Ÿç‰©å­¦ï¼‰ã«ãŠã‘ã‚‹é«˜åº¦ãªè³ªå•ã«å¯¾ã—ã¦ã€æ®µéšçš„ãªæ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’å«ã‚€å›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ

- **ç·å•é¡Œæ•°**: {total_count:,} å•
- **æˆåŠŸç”Ÿæˆæ•°**: {success_count:,} å•
- **æˆåŠŸç‡**: {success_count/total_count*100:.1f}%

## ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»¥ä¸‹ã®3ã¤ã®å½¢å¼ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ï¼š

### 1. CSVå½¢å¼ (`{csv_filename}`)
- ä¸€èˆ¬çš„ãªè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿
- Excel ã‚„ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚½ãƒ•ãƒˆã§é–‹ã‘ã¾ã™
- Pandas ã§ç°¡å˜ã«èª­ã¿è¾¼ã¿å¯èƒ½

### 2. Parquetå½¢å¼ (`{parquet_filename}`)
- é«˜åŠ¹ç‡ãªã‚«ãƒ©ãƒ å‹ãƒ‡ãƒ¼ã‚¿å½¢å¼
- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®é«˜é€Ÿèª­ã¿è¾¼ã¿ã«æœ€é©
- Apache Arrow ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã§æ¨å¥¨

### 3. JSONLå½¢å¼ (`{jsonl_filename}`)
- JSON Lines å½¢å¼ï¼ˆ1è¡Œ1ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã«é©ã—ã¦ã„ã‚‹
- æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§åºƒãå¯¾å¿œ

## ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã«ã¯ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼š

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å | å‹ | èª¬æ˜ |
|-------------|----|----|"""

    # åˆ—æƒ…å ±ã‚’å‹•çš„ã«è¿½åŠ 
    for col in columns:
        if col == 'id':
            readme_content += f"\n| `{col}` | int | å•é¡Œã®ä¸€æ„è­˜åˆ¥å­ |"
        elif col == 'question':
            readme_content += f"\n| `{col}` | str | å…ƒã®è³ªå•æ–‡ |"
        elif col == 'output':
            readme_content += f"\n| `{col}` | str | CoTæ¨è«–éç¨‹ã‚’å«ã‚€å®Œå…¨ãªå›ç­”ï¼ˆ&lt;think&gt;...&lt;/think&gt;å½¢å¼ï¼‰ |"
        elif col == 'answer':
            readme_content += f"\n| `{col}` | str | æœ€çµ‚çš„ãªæ­£è§£ |"
        else:
            readme_content += f"\n| `{col}` | str | {col} |"

    readme_content += f"""

## ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿

```json
{json.dumps(sample_data, ensure_ascii=False, indent=2)}
```

## ä½¿ç”¨æ–¹æ³•

### Python (Pandas)

```python
import pandas as pd

# CSVå½¢å¼ã§èª­ã¿è¾¼ã¿
df = pd.read_csv('{csv_filename}')

# Parquetå½¢å¼ã§èª­ã¿è¾¼ã¿ï¼ˆæ¨å¥¨ï¼‰
df = pd.read_parquet('{parquet_filename}')

# JSONLå½¢å¼ã§èª­ã¿è¾¼ã¿
df = pd.read_json('{jsonl_filename}', lines=True)

print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {{len(df)}} å•")
print(df.head())
```

### Hugging Face Datasets

```python
from datasets import load_dataset

# ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
dataset = load_dataset("neko-llm/HLE_SFT_GPQA_Diamond")

# ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æŒ‡å®š
dataset = load_dataset("neko-llm/HLE_SFT_GPQA_Diamond", data_files="{parquet_filename}")
```

### PyTorch DataLoader

```python
import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd

class GPQADataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        return {{
            'question': row['question'],
            'output': row['output'],
            'answer': row['answer']
        }}

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_parquet('{parquet_filename}')
dataset = GPQADataset(df)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

## ç”Ÿæˆæ–¹æ³•

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»¥ä¸‹ã®æ‰‹é †ã§ç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼š

1. **å…ƒãƒ‡ãƒ¼ã‚¿**: [GPQA Diamond Dataset](https://huggingface.co/datasets/Idavidrein/gpqa) ã® train split ã‚’ä½¿ç”¨
2. **CoTç”Ÿæˆ**: DeepSeek-R1-0528:free ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¨è«–éç¨‹ã‚’ç”Ÿæˆ
3. **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: `<think>æ¨è«–éç¨‹</think>æœ€çµ‚å›ç­”` ã®å½¢å¼ã§æ§‹é€ åŒ–
4. **å“è³ªç®¡ç†**: APIå‘¼ã³å‡ºã—ã®æˆåŠŸ/å¤±æ•—ã‚’è¨˜éŒ²ã—ã€å“è³ªã‚’æ‹…ä¿

### ç”Ÿæˆã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«
- **ãƒ¢ãƒ‡ãƒ«**: `{MODEL_NAME}`
- **API**: OpenRouter API
- **ç”Ÿæˆæ–¹å¼**: JSONæ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹æ¨è«–ç”Ÿæˆ
- **å‡ºåŠ›å½¢å¼**: `{{"reasoning": "æ¨è«–éç¨‹", "answer": "å›ç­”"}}`ã®JSONå½¢å¼

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯å…ƒã®GPQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚å­¦è¡“ç ”ç©¶ç›®çš„ã§ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

## å¼•ç”¨

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ï¼š

```bibtex
@dataset{{hle_sft_gpqa_diamond,
  title={{HLE SFT GPQA Diamond Dataset with Chain of Thought}},
  author={{neko-llm}},
  year={{2024}},
  url={{https://huggingface.co/datasets/neko-llm/HLE_SFT_GPQA_Diamond}}
}}
```

å…ƒã®GPQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¼•ç”¨ï¼š
```bibtex
@article{{rein2023gpqa,
  title={{GPQA: A Graduate-Level Google-Proof Q&A Benchmark}},
  author={{Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R}},
  journal={{arXiv preprint arXiv:2311.12022}},
  year={{2023}}
}}
```

## å•ã„åˆã‚ã›

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é–¢ã™ã‚‹è³ªå•ã‚„å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€[Issues](https://huggingface.co/datasets/neko-llm/HLE_SFT_GPQA_Diamond/discussions) ã§ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚
"""

    return readme_content


def main():
    """
    GPQA Dataset ã® CoT (Reasoning) ç”Ÿæˆ & ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã‚’è¡Œã† Main Scriptã€‚

    NOTE:
    é–¢æ•°ã®æ©Ÿèƒ½:
    1. GPQA Dataset ã‚’èª­ã¿è¾¼ã‚€ã€‚
    2. GPQA Dataset ã‚’å‡¦ç†ã—ã¦ã€ä½œæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    3. ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ local ã«ä¿å­˜ã™ã‚‹ã€‚
    4. ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ Hugging Face ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    """
    print("ğŸš€ GPQA Dataset CoT ç”Ÿæˆå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
    output_filename = "gpqa_diamond_cot_dataset.csv"

    # --- Resume Logic ---
    processed_ids = []
    if os.path.exists(output_filename):
        print(f"ğŸ“„ æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: '{output_filename}'")
        try:
            existing_df = pd.read_csv(output_filename)
            if 'id' in existing_df.columns:
                processed_ids = existing_df['id'].dropna().tolist()
                print(f"âœ… {len(processed_ids)} ä»¶ã®å‡¦ç†æ¸ˆã¿å•é¡Œã‚’ç™ºè¦‹ã€‚ã“ã‚Œã‚‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        except pd.errors.EmptyDataError:
            print("âš ï¸ æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚æ–°è¦ã§é–‹å§‹ã—ã¾ã™ã€‚")
        except Exception as e:
            print(f"âš ï¸ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚æ–°è¦ã§é–‹å§‹ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")

    # --- GPQA Dataset Loading ---
    print("ğŸ“¥ GPQA Dataset (gpqa_diamond) ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    try:
        # GPQA ã® gpqa_diamond subset ã‚’èª­ã¿è¾¼ã¿ï¼ˆtrainãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
        diamond_dataset = datasets.load_dataset("Idavidrein/gpqa", "gpqa_diamond", split='train')
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†ã€‚{len(diamond_dataset)} ä»¶ã®å•é¡Œã‚’ç™ºè¦‹ã€‚")

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        print("ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
        sample = diamond_dataset[0]
        for key in sample.keys():
            print(f"  {key}: {str(sample[key])[:100]}...")

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # --- Main Processing Loop ---
    batch_results = []
    total_problems = len(diamond_dataset)

    # çµ±è¨ˆæƒ…å ±ã®åˆæœŸåŒ–
    total_processed = 0
    total_generated = 0
    total_judged_correct = 0
    total_api_failures = 0

    print(f"\nğŸš€ {total_problems} ä»¶ã®å•é¡Œã«å¯¾ã—ã¦CoTç”Ÿæˆã‚’é–‹å§‹...")
    print(f"ğŸ’¾ çµæœã¯ '{output_filename}' ã«5ä»¶ãšã¤ãƒãƒƒãƒã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚")
    print(f"ğŸ” å›ç­”ã®æ­£å½“æ€§åˆ¤å®šã‚’æœ‰åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚æ­£ã—ã„å›ç­”ã®ã¿ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚")

    for i, item in enumerate(tqdm(diamond_dataset, desc="CoTç”Ÿæˆä¸­")):
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        item_id = i + 1  # IDã‚’1ã‹ã‚‰é–‹å§‹
        if item_id in processed_ids:
            continue

        total_processed += 1
        print(f"\n--- å•é¡Œ {i + 1}/{total_problems} ã‚’å‡¦ç†ä¸­ ---")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = generate_prompt_for_gpqa_dataset(
            question=item['Question'],
            explanation=item['Explanation'],
            subdomain=item['Subdomain']
        )
        print(f"ğŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt[:200]}...")

        # OpenRouter API ã§CoTç”Ÿæˆ
        status, content = generate_cot_with_openrouter(prompt)
        print(f"ğŸ” API ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")

        if status == "success":
            total_generated += 1
            print(f"ğŸ” API ãƒ¬ã‚¹ãƒãƒ³ã‚¹(Content): {content[:200]}...")

            # JSONå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            parsed_json = parse_json_response(content)

            if parsed_json and 'reasoning' in parsed_json and 'answer' in parsed_json:
                # JSONå½¢å¼ã®å ´åˆ
                reasoning = parsed_json['reasoning']
                generated_answer = parsed_json['answer']
                print(f"ğŸ” JSONè§£ææˆåŠŸ:")
                print(f"   æ¨è«–éç¨‹: {reasoning[:100]}...")
                print(f"   ç”Ÿæˆå›ç­”: {generated_answer}")

                # å›ç­”ã®æ­£å½“æ€§ã‚’åˆ¤å®š
                print(f"âš–ï¸ å›ç­”ã®æ­£å½“æ€§ã‚’åˆ¤å®šä¸­...")
                is_correct = judge_answer_by_llm(
                    generated_answer=generated_answer,
                    correct_answer=item['Correct Answer'],
                    question=item['Question'],
                    subdomain=item['Subdomain']
                )

                if is_correct:
                    total_judged_correct += 1
                    print(f"âœ… åˆ¤å®šçµæœ: æ­£è§£! ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã™ã€‚")

                    # æ­£è§£ã®å ´åˆã®ã¿çµæœã‚’ä¿å­˜
                    batch_results.append({
                        "id": item_id,
                        "question": item['Question'],
                        # æ¨è«–éç¨‹(Reasoning/CoT)ã‚’<think>...</think>ã‚¿ã‚°ã§å›²ã¿ã€æœ€çµ‚å›ç­”ã‚’å¾Œã«è¨˜è¼‰
                        "output": f"<think>{reasoning}</think>{item['Correct Answer']}",
                        "answer": item['Correct Answer'],
                        "generated_answer": generated_answer,
                        "judgment_status": "correct"
                    })
                else:
                    print(f"âŒ åˆ¤å®šçµæœ: ä¸æ­£è§£ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã€‚")
                    print(f"   ç”Ÿæˆå›ç­”: {generated_answer}")
                    print(f"   æ­£è§£: {item['Correct Answer']}")
            else:
                # JSONè§£æå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                print(f"âš ï¸ JSONè§£æå¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ...")
                generated_answer = extract_final_answer_from_cot(content)
                print(f"ğŸ” ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§æŠ½å‡ºã•ã‚ŒãŸæœ€çµ‚å›ç­”: {generated_answer}")

                # å›ç­”ã®æ­£å½“æ€§ã‚’åˆ¤å®š
                print(f"âš–ï¸ å›ç­”ã®æ­£å½“æ€§ã‚’åˆ¤å®šä¸­...")
                is_correct = judge_answer_by_llm(
                    generated_answer=generated_answer,
                    correct_answer=item['Correct Answer'],
                    question=item['Question']
                )

                if is_correct:
                    total_judged_correct += 1
                    print(f"âœ… åˆ¤å®šçµæœ: æ­£è§£! ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã™ã€‚")

                    # æ­£è§£ã®å ´åˆã®ã¿çµæœã‚’ä¿å­˜ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼‰
                    batch_results.append({
                        "id": item_id,
                        "question": item['Question'],
                        # æ¨è«–éç¨‹(Reasoning/CoT)ã‚’ç”Ÿæˆã—ã¦ã€<think>...</think> ã‚¿ã‚°ã§å›²ã¿ã€æœ€çµ‚çš„ãªå›ç­”(answer)ã‚’ </think> ã‚¿ã‚°ã®å¾Œã«è¨˜è¼‰ã™ã‚‹ã€‚
                        "output": f"<think>{content}</think>{item['Correct Answer']}",
                        "answer": item['Correct Answer'],
                        "generated_answer": generated_answer
                    })
                else:
                    print(f"âŒ åˆ¤å®šçµæœ: ä¸æ­£è§£ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã€‚")
                    print(f"   ç”Ÿæˆå›ç­”: {generated_answer}")
                    print(f"   æ­£è§£: {item['Correct Answer']}")
        else:
            total_api_failures += 1
            print(f"âŒ CoTç”Ÿæˆã«å¤±æ•—: {content}")

        # 5ä»¶ã”ã¨ã¾ãŸã¯æœ€å¾Œã®å‡¦ç†ã§CSVã«ä¿å­˜ï¼ˆæ­£è§£ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
        if len(batch_results) >= 5 or (i + 1) == total_problems:
            if batch_results:  # ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
                print(f"ğŸ’¾ {len(batch_results)} ä»¶ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜ä¸­...")
                temp_df = pd.DataFrame(batch_results)
                temp_df.to_csv(
                    output_filename,
                    mode='a',
                    header=not os.path.exists(output_filename) or os.path.getsize(output_filename) == 0,
                    index=False,
                    encoding='utf-8-sig'
                )
                batch_results.clear()

        # é€²æ—çµ±è¨ˆã‚’è¡¨ç¤º
        if (i + 1) % 10 == 0:
            success_rate = (total_judged_correct / total_processed * 100) if total_processed > 0 else 0
            print(f"ğŸ“Š é€²æ—çµ±è¨ˆ ({i + 1}/{total_problems}):")
            print(f"   å‡¦ç†æ¸ˆã¿: {total_processed}, ç”ŸæˆæˆåŠŸ: {total_generated}, åˆ¤å®šæ­£è§£: {total_judged_correct}")
            print(f"   æ­£è§£ç‡: {success_rate:.1f}%, APIå¤±æ•—: {total_api_failures}")

        # API ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
        time.sleep(0.5)

    # æœ€çµ‚çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    final_success_rate = (total_judged_correct / total_processed * 100) if total_processed > 0 else 0
    generation_success_rate = (total_generated / total_processed * 100) if total_processed > 0 else 0

    print(f"\nâœ… å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æœ€çµ‚çµæœã¯ '{output_filename}' ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚")
    print(f"\nğŸ“Š æœ€çµ‚çµ±è¨ˆæƒ…å ±:")
    print(f"   ç·å‡¦ç†æ•°: {total_processed}")
    print(f"   CoTç”ŸæˆæˆåŠŸ: {total_generated} ({generation_success_rate:.1f}%)")
    print(f"   åˆ¤å®šæ­£è§£æ•°: {total_judged_correct} ({final_success_rate:.1f}%)")
    print(f"   APIå¤±æ•—æ•°: {total_api_failures}")
    print(f"   ä¿å­˜ãƒ‡ãƒ¼ã‚¿æ•°: {total_judged_correct} (æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®ã¿)")

    # å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
    if total_generated > 0:
        judgment_accuracy = (total_judged_correct / total_generated * 100)
        print(f"   ç”Ÿæˆå“è³ª: {judgment_accuracy:.1f}% (ç”ŸæˆæˆåŠŸãƒ‡ãƒ¼ã‚¿ä¸­ã®æ­£è§£ç‡)")

    if total_judged_correct == 0:
        print("âš ï¸ æ­£è§£ã¨åˆ¤å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    # --- è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã®ä¿å­˜ã¨Hugging Face ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    try:
        print("\nğŸ”„ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹...")
        final_df: pd.DataFrame = pd.read_csv(output_filename)

        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        # å®Ÿéš›ã«ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆå…¨ã¦æ­£è§£ãƒ‡ãƒ¼ã‚¿ï¼‰
        success_count = len(final_df)
        total_count = total_processed  # å®Ÿéš›ã«å‡¦ç†ã—ãŸç·æ•°

        print(f"ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ:")
        print(f"   ä¿å­˜ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {success_count}")
        print(f"   å‡¦ç†å¯¾è±¡ç·æ•°: {total_count}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ª: 100% (æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®ã¿)")

        # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚’å–å¾—
        base_filename = os.path.splitext(output_filename)[0]

        # --- 2. Parquetå½¢å¼ã§ã®ä¿å­˜ ---
        parquet_filename = f"{base_filename}.parquet"
        print(f"ğŸ’¾ Parquetå½¢å¼ã§ä¿å­˜ä¸­: {parquet_filename}")
        final_df.to_parquet(parquet_filename, index=False)

        # --- 3. JSONLå½¢å¼ã§ã®ä¿å­˜ ---
        jsonl_filename = f"{base_filename}.jsonl"
        print(f"ğŸ’¾ JSONLå½¢å¼ã§ä¿å­˜ä¸­: {jsonl_filename}")
        final_df.to_json(jsonl_filename, orient='records', lines=True, force_ascii=False)

        # --- Hugging Face Datasetsã«3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
        print(f"ğŸ“¤ {OUTPUT_DATASET_ID} ã«3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")

        # 1. CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“„ CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=output_filename,
            path_in_repo=output_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        # 2. Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“Š Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=parquet_filename,
            path_in_repo=parquet_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        # 3. JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“ JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=jsonl_filename,
            path_in_repo=jsonl_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        print(f"âœ… 3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {OUTPUT_DATASET_ID}")

        # --- README.md ã®ç”Ÿæˆã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
        print("ğŸ“ README.md ã‚’ç”Ÿæˆä¸­...")
        readme_content = generate_readme_content(final_df, output_filename, parquet_filename, jsonl_filename, total_count, success_count)
        readme_filename = "README.md"

        with open(readme_filename, 'w', encoding='utf-8') as f:
            f.write(readme_content)

        print("ğŸ“¤ README.md ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=readme_filename,
            path_in_repo=readme_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )
        print("âœ… README.md ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

        # --- çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º ---
        print(f"\nğŸ“Š å‡¦ç†çµ±è¨ˆ:")
        print(f"  - ç·å‡¦ç†æ•°: {total_count}")
        print(f"  - æˆåŠŸæ•°: {success_count}")
        print(f"  - æˆåŠŸç‡: {success_count/total_count*100:.1f}%")

        print(f"\nğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - CSV: {output_filename}")
        print(f"  - Parquet: {parquet_filename}")
        print(f"  - JSONL: {jsonl_filename}")
        print(f"  - README: {readme_filename}")

        print(f"\nğŸŒ Hugging Face Datasetsã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆ:")
        print(f"  - Repository: {OUTPUT_DATASET_ID}")
        print(f"    â”œâ”€ {output_filename}")
        print(f"    â”œâ”€ {parquet_filename}")
        print(f"    â”œâ”€ {jsonl_filename}")
        print(f"    â””â”€ {readme_filename}")

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¾ãŸã¯Hugging Face Hubã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

    print("\n--- Processing complete ---")

if __name__ == "__main__":
    main()
