# ==========================================================
#  Qwen/Qwen3-32B  ── GRPO + vLLM-server + ZeRO-3 (accelerate)
#  ＊   train ノード：2 台 × 8 GPU (=16 GPU)
#  ＊   vLLM ノード：1 台 × 8 GPU
# ==========================================================

# ----------------------------------------------------------
# Model / Tokenizer
# ----------------------------------------------------------
model_name_or_path: Qwen/Qwen3-32B
model_revision: main
torch_dtype: bfloat16
attn_implementation: eager
trust_remote_code: true          # tokenizer.chat_template を正しく取得

# ----------------------------------------------------------
# Data
# ----------------------------------------------------------
dataset_name: open-r1/OpenR1-Math-220k
dataset_prompt_column: problem

system_prompt: |
  You are a helpful AI Assistant that provides well-reasoned and detailed responses.
  First think in <think>…</think>, then answer in <answer>…</answer>.

# Qwen 既定の chat_template を使用（上書き不要）
# chat_template: null

# ----------------------------------------------------------
# GRPO Trainer
# ----------------------------------------------------------
# GPU 80 GB × 16 / ZeRO-3: 
# Parameters that control generation
per_device_train_batch_size: 2
gradient_accumulation_steps: 32
per_device_eval_batch_size: 4
# Parameters that control the data preprocessing
num_generations: 8
max_prompt_length: 512
max_completion_length: 512      # vLLM サーバ側 --max-model-len=4096 内

bf16: true
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# Parameters that control the model and reference model
disable_dropout: true

# Parameters that control generation acceleration powered by vLLM
use_vllm: true
vllm_mode: server                 # train_grpo.py 側引数が override
# vllm_server_host/port はシェルスクリプトから CLI 引数で渡すのでここには書かない

# 学習ハイパーパラメータ
learning_rate: 1.0e-5            # 大規模モデルなので LR を下げる
warmup_ratio: 0.05
lr_scheduler_type: cosine
num_train_epochs: 1              # 例：1 epoch（ステップ指定なら max_steps へ）
max_steps: -1

# Parameters that control the training
loss_type: bnpo
# beta: 0.1 KL divergence coefficient
epsilon: 0.2 
epsilon_high: 0.28
mask_truncated_completions: true

reward_funcs: [accuracy, format, tag_count]
reward_weights: [1.0, 1.0, 1.0]

# ----------------------------------------------------------
# ロギング & 保存
# ----------------------------------------------------------
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps

save_strategy: epoch
save_total_limit: 1
output_dir: data/Qwen3-32B-Open-R1-GRPO
overwrite_output_dir: true

push_to_hub: false
# report_to:
# - wandb
hub_model_id: Qwen3-32B-Open-R1-GRPO
hub_strategy: every_save

seed: 42
