# === ScriptArguments ===
dataset_name: neko-llm/dna_dpo_hh-rlhf                    # HF上のデータセット名
#dataset_config: main                                     # サブセット名
dataset_num_proc: 12                                      # 前処理に使う CPU プロセス数
dataset_train_split: train                                # 学習用スプリット名
dataset_test_split: test                                  # 評価用スプリット名

# === ModelConfig ===
model_name_or_path: Qwen/Qwen3-235B-A22B                  # モデル名
model_revision: main                                     
torch_dtype: bfloat16                                     # 数値の型
#attn_implementation: flash_attention_2                   # なぜか導入すると学習時に"nan"が発生


# === LoRA/PEFT (get_peft_config) ===
# フルファインチューニング時はORPOTrainerの引数"peft_config"を切る
use_peft: true                                            # PEFT(LoRA)を使うかどうか
lora_r: 16                                                # LoRA の rank
lora_alpha: 32                                            # LoRA の alpha
lora_dropout: 0.05                                        # LoRA のドロップアウト率
lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]       # LoRA を適用するモジュール
use_rslora: false                                         # Rank-Stabilized LoRA
use_dora: false                                           # Decomposed LoRA

# === DPOConfig ===
use_cpu: false                                            # CPUを使うかどうか
data_seed: 42                                             # データセットの乱数シード
bf16: true                                                # bf16 演算を有効にするかどうか
do_eval: false                                            # 学習中の評価を行うかどうか
eval_strategy: "no"                                       # 評価タイミング (no, steps, epoch)
per_device_train_batch_size: 1                            # 1GPUあたりの学習バッチサイズ
per_device_eval_batch_size: 1                             # 1GPUあたりの評価バッチサイズ
gradient_accumulation_steps: 8                            # 勾配累積のステップ数
gradient_checkpointing: true                              # 勾配累積チェックポイント
gradient_checkpointing_kwargs:
  use_reentrant: True
learning_rate: 2.0e-5                                     # 学習率
lr_scheduler_type: cosine_with_min_lr                     # 学習率スケジューラの指定
lr_scheduler_kwargs:
  min_lr_rate: 0.1
max_grad_norm: 0.3                                        # 勾配クリッピングの閾値
num_train_epochs: 1                                       # エポック数
warmup_ratio: 0.03                                        # ウォームアップ比率
seed: 42                                                  # 乱数シード値
beta: 0.1                                                 # DPO の beta パラメータ
optim: adamw_torch                                        # Optimizer
max_steps: 5

# === Save and Log ===
output_dir: data/Qwen3-235B-ORPO-test                     # 出力ディレクトリ
overwrite_output_dir: true                                # 上書き可能かどうか
save_strategy: epoch                                      # 保存タイミング (no, steps, epoch)
save_steps: 1                                             # 保存間隔
save_total_limit: 1                                       # 保存ファイル保持上限数
logging_steps: 1                                          # ログ出力間隔
logging_strategy: steps                                   # ログの出力タイミング
log_level: info                                           # ログレベル

# === HF Hub ===
push_to_hub: true                                         # HF Hub へ push するかどうか
hub_model_id: Qwen3-235B-ORPO-test                        # 登録するモデルID
hub_strategy: every_save                                  # 保存時に毎回 push

# === Other ===
max_length: 128                                           # トークンの最大系列長