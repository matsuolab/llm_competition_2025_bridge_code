# HuggingFace Trainerのコンフィグと、trl特有のコンフィグと、open-r1特有のコンフィグをまとめたもの
# なので詳細は以下サイトを参照

# Training Arguments: https://huggingface.co/docs/transformers/v4.53.1/en/main_classes/trainer#transformers.TrainingArguments
# SFT Config: https://huggingface.co/docs/trl/v0.19.1/en/sft_trainer#trl.SFTConfig
# Qiita https://qiita.com/m__k/items/2c4e476d7ac81a3a44af 日本語で分かりやすい

# Model arguments
model_name_or_path: Qwen/Qwen3-235B-A22B # テスト用のため0.5bにしている
model_revision: main
torch_dtype: bfloat16 # bf16を使う
attn_implementation: flash_attention_2 # eager, flash-attnention_2

# LoRA arguments https://github.com/huggingface/trl/blob/v0.19.1/trl/trainer/model_config.py#L19
use_peft: true # PEFT (Lora) を使うかどうか
lora_r: 16 # LoRAのr値
lora_alpha: 32 # LoRAのalpha値
lora_dropout: 0.05 # LoRAのドロップアウト率
lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj'] # LoRAを適用するモジュール
use_rslora: false # Rank-Stabilized LoRAを使うかどうか
use_dora: false # Weight-Decomposed Low-Rank Adaptationを使うかどうか

# QLoRA arguments
# load_in_4bit: true               # QLoRAを有効化し、モデルを4ビットで読み込みます
# bnb_4bit_compute_dtype: bfloat16 # 計算時のデータ型をbfloat16に設定します (A100/H100 GPUで高速)
# bnb_4bit_use_double_quant: true  # 二重量子化を有効にし、わずかなメモリ増で精度を向上させます
# bnb_4bit_quant_type: nf4         # NF4という、性能劣化が少ない4ビットのデータ型を使用します
# use_bnb_nested_quant : true

# Data training arguments
# chat_template: "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Open-R1, a language model trained by Hugging Face to help users. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Open-R1, a language model trained by Hugging Face to help users. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n"
dataset_name: openai/gsm8k # データセット名
dataset_config: main # データセットのサブセット
dataset_num_proc: 12 # データセットの前処理に使うプロセス(CPUコア)数
dataset_text_field: answer # データセットのテキストフィールド名
eos_token: <|im_end|> # シーケンスの終端トークン

# SFT trainer config
use_cpu: false # CPUを使うかどうか (GPUがない場合はtrueにする)
data_seed: 42 # データセットの乱数シード
bf16: true # bf16を使う
do_eval: false # 評価を行うかどうか
eval_strategy: 'no' # 評価を行うタイミング ('no' or 'steps' or 'epoch')
gradient_accumulation_steps: 8 # 勾配蓄積のステップ数
gradient_checkpointing: true # 勾配チェックポイントを使うかどうか
gradient_checkpointing_kwargs:
  use_reentrant: True
hub_model_id: neko-llm/Qwen3-235B-main-sft-epoch1 # モデルをHuggingFace Hubにpushする際のモデルID
hub_strategy: end # モデルをHuggingFace Hubにpushするタイミング (every_save: 保存する時毎回, end: 最後の保存時)
learning_rate: 4.0e-05 # 学習率
log_level: info # ログレベル(ログの詳細度)
logging_steps: 1 # ログを出力するステップ数
logging_strategy: steps # ログを出力するタイミング ('no' or 'steps' or 'epoch')
lr_scheduler_type: cosine_with_min_lr # 学習率スケジューラのタイプ 
lr_scheduler_kwargs:
  min_lr_rate: 0.1
packing: true
max_grad_norm: 0.2
max_length: 32768 # 32768 # 最大シーケンス長
#max_steps: 2 # -1
num_train_epochs: 1 # 学習エポック数
output_dir: data/Qwen3-235B-main-sft # 結果の出力ディレクトリ
optim: adamw_torch # オプティマイザの種類  何があるかはhttps://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py
overwrite_output_dir: true
per_device_eval_batch_size: 1 # 評価時の1GPUあたりのバッチサイズ
per_device_train_batch_size: 1 # 学習時の1GPUあたりのバッチサイズ
push_to_hub: true # trueにするとHuggingFace Hubにモデルをpushする ログインしている必要がある
report_to: # wandbにログインしているならこれをオンにするとwandbにログが送られる
- none # wandbがあると、モデルが保存できない。
save_strategy: epoch # モデルをローカルに保存するタイミング ('no' or 'steps' or 'epoch')
save_steps: 500 # モデルをローカルに保存するステップ数(間隔)
save_total_limit: 5
seed: 42 # 乱数シード
use_liger_kernel: true # エラーが出たので一旦無効化
warmup_ratio: 0.03