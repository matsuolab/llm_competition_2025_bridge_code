# HuggingFace Trainerのコンフィグと、trl特有のコンフィグと、open-r1特有のコンフィグをまとめたもの
# なので詳細は以下サイトを参照

# Training Arguments: https://huggingface.co/docs/transformers/v4.53.1/en/main_classes/trainer#transformers.TrainingArguments
# SFT Config: https://huggingface.co/docs/trl/v0.19.1/en/sft_trainer#trl.SFTConfig
# Qiita https://qiita.com/m__k/items/2c4e476d7ac81a3a44af 日本語で分かりやすい

# Model arguments
model_name_or_path: neko-llm/DeepSeek-R1-0528-bf16 # テスト用のため0.5bにしている
model_revision: main
torch_dtype: bfloat16 # bf16を使う
# attn_implementation: flash_attention_2 # flash-attnention_2は使えなかった。

# LoRA arguments https://github.com/huggingface/trl/blob/v0.19.1/trl/trainer/model_config.py#L19
use_peft: true # PEFT (Lora) を使うかどうか
lora_r: 16 # LoRAのr値
lora_alpha: 32 # LoRAのalpha値
lora_dropout: 0.05 # LoRAのドロップアウト率
lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj'] # LoRAを適用するモジュール
use_rslora: false # Rank-Stabilized LoRAを使うかどうか
use_dora: false # Weight-Decomposed Low-Rank Adaptationを使うかどうか

# Data training arguments
# chat_template: "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}"
dataset_name: openai/gsm8k # データセット名
dataset_config: main # データセットのサブセット
dataset_num_proc: 12 # データセットの前処理に使うプロセス(CPUコア)数
#dataset_text_field: answer # データセットのテキストフィールド名
# eos_token: <|im_end|> # シーケンスの終端トークン

# SFT trainer config
use_cpu: false # CPUを使うかどうか (GPUがない場合はtrueにする)
data_seed: 42 # データセットの乱数シード
bf16: true # bf16を使う
do_eval: false # 評価を行うかどうか
eval_strategy: 'no' # 評価を行うタイミング ('no' or 'steps' or 'epoch')
gradient_accumulation_steps: 8 # 勾配蓄積のステップ数
gradient_checkpointing: true # 勾配チェックポイントを使うかどうか
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: r1-test # モデルをHuggingFace Hubにpushする際のモデルID
hub_strategy: every_save # モデルをHuggingFace Hubにpushするタイミング (every_save: 保存する時毎回, end: 最後の保存時)
learning_rate: 4.0e-05 # 学習率
log_level: info # ログレベル(ログの詳細度)
logging_steps: 1 # ログを出力するステップ数
logging_strategy: steps # ログを出力するタイミング ('no' or 'steps' or 'epoch')
lr_scheduler_type: cosine_with_min_lr # 学習率スケジューラのタイプ 
lr_scheduler_kwargs:
  min_lr_rate: 0.1
packing: false
max_grad_norm: 0.2
max_length: 32768 # 最大シーケンス長
max_steps: -1
num_train_epochs: 5 # 学習エポック数
output_dir: data/r1-test # 結果の出力ディレクトリ
optim: adamw_torch # オプティマイザの種類  何があるかはhttps://github.com/huggingface/transformers/blob/main/src/transformers/training_args.pyを参照
overwrite_output_dir: true
per_device_eval_batch_size: 1 # 評価時の1GPUあたりのバッチサイズ
per_device_train_batch_size: 1 # 学習時の1GPUあたりのバッチサイズ
push_to_hub: false # trueにするとHuggingFace Hubにモデルをpushする ログインしている必要がある
report_to: # wandbにログインしているならこれをオンにするとwandbにログが送られる
- none
save_strategy: steps # モデルをローカルに保存するタイミング ('no' or 'steps' or 'epoch')
save_steps: 500 # モデルをローカルに保存するステップ数(間隔)
save_total_limit: 1
seed: 42 # 乱数シード
use_liger_kernel: false # エラーが出たので一旦無効化
warmup_ratio: 0.03