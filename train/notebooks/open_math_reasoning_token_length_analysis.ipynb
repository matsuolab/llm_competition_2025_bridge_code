{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12589540,"sourceType":"datasetVersion","datasetId":7951358}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# OpenMathReasoning データセットのトークン長分析\n\nこのノートブックでは、train.parquetファイルのprompt+responseを連結し、Qwen3でトークン化して最長トークン数を求めます。","metadata":{}},{"cell_type":"code","source":"# 必要なライブラリのインストール\n!pip install transformers torch pandas datasets pyarrow matplotlib seaborn tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:57:44.294661Z","iopub.execute_input":"2025-07-27T08:57:44.295857Z","iopub.status.idle":"2025-07-27T08:57:49.477281Z","shell.execute_reply.started":"2025-07-27T08:57:44.295811Z","shell.execute_reply":"2025-07-27T08:57:49.476133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoTokenizer\nimport os\nfrom tqdm import tqdm\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:57:49.479760Z","iopub.execute_input":"2025-07-27T08:57:49.480141Z","iopub.status.idle":"2025-07-27T08:57:49.486839Z","shell.execute_reply.started":"2025-07-27T08:57:49.480097Z","shell.execute_reply":"2025-07-27T08:57:49.485824Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. データの読み込み","metadata":{}},{"cell_type":"code","source":"# データファイルのパス\ndata_path = os.path.expanduser(\"/kaggle/input/open-math-reasoning-10fold/train.parquet\")\n\n# parquetファイルの読み込み\nprint(f\"データファイルを読み込み中: {data_path}\")\ndf = pd.read_parquet(data_path)\nprint(f\"データ件数: {len(df)}\")\nprint(f\"カラム: {df.columns.tolist()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:57:49.488090Z","iopub.execute_input":"2025-07-27T08:57:49.488643Z","iopub.status.idle":"2025-07-27T08:58:11.617460Z","shell.execute_reply.started":"2025-07-27T08:57:49.488586Z","shell.execute_reply":"2025-07-27T08:58:11.616034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# データの構造を確認\nprint(\"最初のサンプルの構造:\")\nsample = df.iloc[0]\nprint(f\"prompt型: {type(sample['prompt'])}\")\nprint(f\"extra_info型: {type(sample['extra_info'])}\")\n\n# promptの内容確認\nprint(\"\\npromptの構造:\")\nprint(sample['prompt'])\n\n# extra_infoの内容確認\nprint(\"\\nextra_infoの構造:\")\nif isinstance(sample['extra_info'], str):\n    extra_info = json.loads(sample['extra_info'])\nelse:\n    extra_info = sample['extra_info']\nprint(extra_info.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:58:11.618491Z","iopub.execute_input":"2025-07-27T08:58:11.618838Z","iopub.status.idle":"2025-07-27T08:58:11.637000Z","shell.execute_reply.started":"2025-07-27T08:58:11.618813Z","shell.execute_reply":"2025-07-27T08:58:11.635620Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Prompt + Response の抽出と連結","metadata":{}},{"cell_type":"code","source":"def extract_prompt_response(row):\n    \"\"\"各行からpromptとresponseを抽出して連結\"\"\"\n    # promptからユーザーメッセージを抽出\n    prompt_data = row['prompt']\n    if isinstance(prompt_data, str):\n        prompt_data = json.loads(prompt_data)\n    \n    user_content = \"\"\n    for msg in prompt_data:\n        if msg.get('role') == 'user':\n            user_content = msg.get('content', '')\n            break\n    \n    # extra_infoからresponseを抽出\n    extra_info = row['extra_info']\n    if isinstance(extra_info, str):\n        extra_info = json.loads(extra_info)\n    \n    response = extra_info.get('answer', '')\n    \n    # チャット形式で連結\n    combined_text = f\"User: {user_content}\\n\\nAssistant: {response}\"\n    \n    return combined_text, user_content, response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:58:11.639724Z","iopub.execute_input":"2025-07-27T08:58:11.640116Z","iopub.status.idle":"2025-07-27T08:58:11.650804Z","shell.execute_reply.started":"2025-07-27T08:58:11.640081Z","shell.execute_reply":"2025-07-27T08:58:11.649660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 全データに対してprompt+response抽出を実行\nprint(\"prompt + response を抽出中...\")\nresults = []\n\nfor idx, row in tqdm(df.iterrows(), total=len(df), desc=\"データ処理中\"):\n    combined_text, prompt, response = extract_prompt_response(row)\n    results.append({\n        'index': idx,\n        'combined_text': combined_text,\n        'prompt': prompt,\n        'response': response,\n        'prompt_length': len(prompt),\n        'response_length': len(response),\n        'combined_length': len(combined_text)\n    })\n\n# 結果をDataFrameに変換\ntext_df = pd.DataFrame(results)\nprint(f\"処理完了: {len(text_df)} サンプル\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:58:11.652062Z","iopub.execute_input":"2025-07-27T08:58:11.652352Z","iopub.status.idle":"2025-07-27T08:58:20.783434Z","shell.execute_reply.started":"2025-07-27T08:58:11.652330Z","shell.execute_reply":"2025-07-27T08:58:20.782011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 文字数統計を表示\nprint(\"=== 文字数統計 ===\")\nprint(f\"Prompt文字数 - 平均: {text_df['prompt_length'].mean():.1f}, 最大: {text_df['prompt_length'].max()}, 最小: {text_df['prompt_length'].min()}\")\nprint(f\"Response文字数 - 平均: {text_df['response_length'].mean():.1f}, 最大: {text_df['response_length'].max()}, 最小: {text_df['response_length'].min()}\")\nprint(f\"Combined文字数 - 平均: {text_df['combined_length'].mean():.1f}, 最大: {text_df['combined_length'].max()}, 最小: {text_df['combined_length'].min()}\")\n\n# 最長文字数のサンプルを表示\nmax_idx = text_df['combined_length'].idxmax()\nprint(f\"\\n最長文字数サンプル (インデックス: {max_idx}, 文字数: {text_df.loc[max_idx, 'combined_length']})\")\nprint(\"=\" * 50)\nprint(text_df.loc[max_idx, 'combined_text'][:1000] + \"...\" if len(text_df.loc[max_idx, 'combined_text']) > 1000 else text_df.loc[max_idx, 'combined_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:58:20.784458Z","iopub.execute_input":"2025-07-27T08:58:20.785119Z","iopub.status.idle":"2025-07-27T08:58:20.801646Z","shell.execute_reply.started":"2025-07-27T08:58:20.785085Z","shell.execute_reply":"2025-07-27T08:58:20.800684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Qwen3 トークナイザーの設定","metadata":{}},{"cell_type":"code","source":"# Qwen3トークナイザーをロード\n# 利用可能なQwen3系モデルを試す\nmodel_names = [\n    \"Qwen/Qwen3-0.6B\",\n    #\"Qwen/Qwen2.5-1.5B\", \n    #\"Qwen/Qwen2.5-3B\",\n    #\"Qwen/Qwen2.5-7B\"\n]\n\ntokenizer = None\nused_model = None\n\nfor model_name in model_names:\n    try:\n        print(f\"トークナイザーを試行中: {model_name}\")\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        used_model = model_name\n        print(f\"成功: {model_name} のトークナイザーを使用\")\n        break\n    except Exception as e:\n        print(f\"失敗: {model_name} - {e}\")\n        continue\n\nif tokenizer is None:\n    raise RuntimeError(\"利用可能なQwen3トークナイザーが見つかりませんでした\")\n\nprint(f\"\\n使用するトークナイザー: {used_model}\")\nprint(f\"語彙サイズ: {tokenizer.vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:58:20.802971Z","iopub.execute_input":"2025-07-27T08:58:20.803291Z","iopub.status.idle":"2025-07-27T08:58:23.433540Z","shell.execute_reply.started":"2025-07-27T08:58:20.803266Z","shell.execute_reply":"2025-07-27T08:58:23.432450Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. トークン化とトークン長分析","metadata":{}},{"cell_type":"code","source":"def tokenize_text(text):\n    \"\"\"テキストをトークン化してトークン数を返す\"\"\"\n    try:\n        tokens = tokenizer.encode(text, add_special_tokens=True)\n        return len(tokens)\n    except Exception as e:\n        print(f\"トークン化エラー: {e}\")\n        return 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:58:23.434969Z","iopub.execute_input":"2025-07-27T08:58:23.435362Z","iopub.status.idle":"2025-07-27T08:58:23.441651Z","shell.execute_reply.started":"2025-07-27T08:58:23.435339Z","shell.execute_reply":"2025-07-27T08:58:23.440414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 全サンプルのトークン数を計算\nprint(\"トークン化を実行中...\")\n\n# バッチ処理でメモリ効率を向上\nbatch_size = 100\ntoken_counts = []\n\nfor i in tqdm(range(0, len(text_df), batch_size), desc=\"トークン化中\"):\n    batch = text_df.iloc[i:i+batch_size]\n    batch_token_counts = []\n    \n    for _, row in batch.iterrows():\n        token_count = tokenize_text(row['combined_text'])\n        batch_token_counts.append(token_count)\n    \n    token_counts.extend(batch_token_counts)\n\n# 結果をDataFrameに追加\ntext_df['token_count'] = token_counts\nprint(f\"トークン化完了: {len(text_df)} サンプル\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:58:23.442974Z","iopub.execute_input":"2025-07-27T08:58:23.443423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# トークン数統計を計算・表示\nprint(\"=== トークン数統計 ===\")\nprint(f\"平均トークン数: {text_df['token_count'].mean():.1f}\")\nprint(f\"最大トークン数: {text_df['token_count'].max()}\")\nprint(f\"最小トークン数: {text_df['token_count'].min()}\")\nprint(f\"中央値: {text_df['token_count'].median():.1f}\")\nprint(f\"標準偏差: {text_df['token_count'].std():.1f}\")\n\n# パーセンタイル情報\npercentiles = [50, 75, 90, 95, 99]\nprint(\"\\n=== パーセンタイル分布 ===\")\nfor p in percentiles:\n    value = np.percentile(text_df['token_count'], p)\n    print(f\"{p}%tile: {value:.0f} tokens\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 最長トークンのサンプルを表示\nmax_token_idx = text_df['token_count'].idxmax()\nmax_token_row = text_df.loc[max_token_idx]\n\nprint(f\"\\n=== 最長トークンサンプル ===\")\nprint(f\"インデックス: {max_token_idx}\")\nprint(f\"トークン数: {max_token_row['token_count']}\")\nprint(f\"文字数: {max_token_row['combined_length']}\")\nprint(f\"文字数/トークン数比: {max_token_row['combined_length']/max_token_row['token_count']:.2f}\")\nprint(\"\\n=== サンプル内容 ===\")\nprint(\"=\" * 80)\nsample_text = max_token_row['combined_text']\nif len(sample_text) > 2000:\n    print(sample_text[:1000])\n    print(\"\\n... [中略] ...\\n\")\n    print(sample_text[-1000:])\nelse:\n    print(sample_text)\nprint(\"=\" * 80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. 結果の可視化","metadata":{}},{"cell_type":"code","source":"# トークン数分布のヒストグラム\nplt.figure(figsize=(15, 10))\n\n# 全体の分布\nplt.subplot(2, 2, 1)\nplt.hist(text_df['token_count'], bins=50, alpha=0.7, edgecolor='black')\nplt.title('トークン数分布（全体）')\nplt.xlabel('トークン数')\nplt.ylabel('頻度')\nplt.axvline(text_df['token_count'].mean(), color='red', linestyle='--', label=f'平均: {text_df[\"token_count\"].mean():.0f}')\nplt.axvline(text_df['token_count'].median(), color='green', linestyle='--', label=f'中央値: {text_df[\"token_count\"].median():.0f}')\nplt.legend()\n\n# 95%ile以下に絞った分布\nplt.subplot(2, 2, 2)\np95 = np.percentile(text_df['token_count'], 95)\nfiltered_tokens = text_df[text_df['token_count'] <= p95]['token_count']\nplt.hist(filtered_tokens, bins=50, alpha=0.7, edgecolor='black')\nplt.title(f'トークン数分布（95%ile以下: ≤{p95:.0f}）')\nplt.xlabel('トークン数')\nplt.ylabel('頻度')\n\n# 文字数 vs トークン数の散布図\nplt.subplot(2, 2, 3)\nsample_idx = np.random.choice(len(text_df), min(1000, len(text_df)), replace=False)\nsample_data = text_df.iloc[sample_idx]\nplt.scatter(sample_data['combined_length'], sample_data['token_count'], alpha=0.6)\nplt.title('文字数 vs トークン数（サンプル1000件）')\nplt.xlabel('文字数')\nplt.ylabel('トークン数')\n\n# 文字数/トークン数比の分布\nplt.subplot(2, 2, 4)\nchar_token_ratio = text_df['combined_length'] / text_df['token_count']\nplt.hist(char_token_ratio, bins=50, alpha=0.7, edgecolor='black')\nplt.title('文字数/トークン数比の分布')\nplt.xlabel('文字数/トークン数')\nplt.ylabel('頻度')\nplt.axvline(char_token_ratio.mean(), color='red', linestyle='--', label=f'平均: {char_token_ratio.mean():.2f}')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 上位トークン数サンプルの一覧\nprint(\"=== 上位20サンプル（トークン数順） ===\")\ntop_samples = text_df.nlargest(20, 'token_count')[['index', 'token_count', 'combined_length', 'prompt_length', 'response_length']]\ntop_samples['char_token_ratio'] = top_samples['combined_length'] / top_samples['token_count']\nprint(top_samples.to_string(index=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# トークン長区間別の分布\nbins = [0, 1000, 2000, 4000, 8000, 16000, float('inf')]\nlabels = ['~1K', '1K-2K', '2K-4K', '4K-8K', '8K-16K', '16K+']\ntext_df['token_range'] = pd.cut(text_df['token_count'], bins=bins, labels=labels, right=False)\n\nprint(\"=== トークン長区間別分布 ===\")\nrange_counts = text_df['token_range'].value_counts().sort_index()\nrange_percentage = (range_counts / len(text_df) * 100).round(2)\n\nfor range_name, count in range_counts.items():\n    percentage = range_percentage[range_name]\n    print(f\"{range_name:>8}: {count:>6}件 ({percentage:>5.1f}%)\")\n\nprint(f\"\\n総件数: {len(text_df)}\")\nprint(f\"最大トークン数: {text_df['token_count'].max()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. 結果のまとめ","metadata":{}},{"cell_type":"code","source":"# 結果サマリーを作成\nsummary = {\n    'dataset_info': {\n        'total_samples': len(text_df),\n        'data_source': 'OpenMathReasoning train.parquet',\n        'tokenizer_model': used_model\n    },\n    'token_statistics': {\n        'max_tokens': int(text_df['token_count'].max()),\n        'min_tokens': int(text_df['token_count'].min()),\n        'mean_tokens': float(text_df['token_count'].mean()),\n        'median_tokens': float(text_df['token_count'].median()),\n        'std_tokens': float(text_df['token_count'].std())\n    },\n    'percentiles': {\n        f'{p}th': float(np.percentile(text_df['token_count'], p))\n        for p in [50, 75, 90, 95, 99]\n    },\n    'character_statistics': {\n        'max_chars': int(text_df['combined_length'].max()),\n        'mean_chars': float(text_df['combined_length'].mean()),\n        'char_token_ratio': float((text_df['combined_length'] / text_df['token_count']).mean())\n    }\n}\n\nprint(\"=== 分析結果サマリー ===\")\nprint(json.dumps(summary, indent=2, ensure_ascii=False))\n\n# 結果をファイルに保存\noutput_path = \"/Users/kumakura/tmp/llm-bridge-sahara/ttoken_analysis_summary.json\"\nwith open(output_path, 'w', encoding='utf-8') as f:\n    json.dump(summary, f, indent=2, ensure_ascii=False)\nprint(f\"\\n結果を保存しました: {output_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}