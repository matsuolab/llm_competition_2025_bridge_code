{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 697,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0014347202295552368,
      "grad_norm": 2.887868024381613,
      "learning_rate": 0.0,
      "loss": 0.488,
      "step": 1
    },
    {
      "epoch": 0.0028694404591104736,
      "grad_norm": 3.1415068295744373,
      "learning_rate": 1.949590218937863e-06,
      "loss": 0.4918,
      "step": 2
    },
    {
      "epoch": 0.00430416068866571,
      "grad_norm": 2.802690188631139,
      "learning_rate": 3.0900273887892622e-06,
      "loss": 0.5173,
      "step": 3
    },
    {
      "epoch": 0.005738880918220947,
      "grad_norm": 3.312503805694743,
      "learning_rate": 3.899180437875726e-06,
      "loss": 0.5365,
      "step": 4
    },
    {
      "epoch": 0.007173601147776184,
      "grad_norm": 2.278601370530902,
      "learning_rate": 4.526808302869428e-06,
      "loss": 0.5156,
      "step": 5
    },
    {
      "epoch": 0.00860832137733142,
      "grad_norm": 1.3224024991777052,
      "learning_rate": 5.039617607727125e-06,
      "loss": 0.4707,
      "step": 6
    },
    {
      "epoch": 0.010043041606886656,
      "grad_norm": 1.2601801130649466,
      "learning_rate": 5.473191697130572e-06,
      "loss": 0.4684,
      "step": 7
    },
    {
      "epoch": 0.011477761836441894,
      "grad_norm": 1.437612332755914,
      "learning_rate": 5.84877065681359e-06,
      "loss": 0.4677,
      "step": 8
    },
    {
      "epoch": 0.01291248206599713,
      "grad_norm": 1.5004343047519,
      "learning_rate": 6.1800547775785244e-06,
      "loss": 0.4561,
      "step": 9
    },
    {
      "epoch": 0.014347202295552367,
      "grad_norm": 1.152586705872783,
      "learning_rate": 6.476398521807292e-06,
      "loss": 0.4907,
      "step": 10
    },
    {
      "epoch": 0.015781922525107604,
      "grad_norm": 0.9988645171088364,
      "learning_rate": 6.744474046779655e-06,
      "loss": 0.4755,
      "step": 11
    },
    {
      "epoch": 0.01721664275466284,
      "grad_norm": 0.9376474709573723,
      "learning_rate": 6.989207826664989e-06,
      "loss": 0.4263,
      "step": 12
    },
    {
      "epoch": 0.018651362984218076,
      "grad_norm": 0.7850630346922591,
      "learning_rate": 7.214341080257057e-06,
      "loss": 0.4335,
      "step": 13
    },
    {
      "epoch": 0.020086083213773313,
      "grad_norm": 0.678664988110216,
      "learning_rate": 7.422781916068435e-06,
      "loss": 0.4464,
      "step": 14
    },
    {
      "epoch": 0.021520803443328552,
      "grad_norm": 0.7280980177347023,
      "learning_rate": 7.6168356916586906e-06,
      "loss": 0.4418,
      "step": 15
    },
    {
      "epoch": 0.02295552367288379,
      "grad_norm": 0.856085039539974,
      "learning_rate": 7.798360875751452e-06,
      "loss": 0.4023,
      "step": 16
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 0.7040367746681054,
      "learning_rate": 7.96887757557363e-06,
      "loss": 0.4182,
      "step": 17
    },
    {
      "epoch": 0.02582496413199426,
      "grad_norm": 0.7154290378451243,
      "learning_rate": 8.129644996516386e-06,
      "loss": 0.4413,
      "step": 18
    },
    {
      "epoch": 0.027259684361549498,
      "grad_norm": 0.6349378996263568,
      "learning_rate": 8.281717930966652e-06,
      "loss": 0.3962,
      "step": 19
    },
    {
      "epoch": 0.028694404591104734,
      "grad_norm": 0.6579836542662504,
      "learning_rate": 8.425988740745155e-06,
      "loss": 0.4506,
      "step": 20
    },
    {
      "epoch": 0.03012912482065997,
      "grad_norm": 0.6308084417293527,
      "learning_rate": 8.563219085919834e-06,
      "loss": 0.4433,
      "step": 21
    },
    {
      "epoch": 0.03156384505021521,
      "grad_norm": 0.6412985117550073,
      "learning_rate": 8.694064265717519e-06,
      "loss": 0.4063,
      "step": 22
    },
    {
      "epoch": 0.03299856527977044,
      "grad_norm": 0.5451064676797084,
      "learning_rate": 8.819092144288181e-06,
      "loss": 0.3997,
      "step": 23
    },
    {
      "epoch": 0.03443328550932568,
      "grad_norm": 0.5701385408185329,
      "learning_rate": 8.938798045602852e-06,
      "loss": 0.3993,
      "step": 24
    },
    {
      "epoch": 0.035868005738880916,
      "grad_norm": 0.5288970462912204,
      "learning_rate": 9.053616605738856e-06,
      "loss": 0.3847,
      "step": 25
    },
    {
      "epoch": 0.03730272596843615,
      "grad_norm": 0.5917027933413244,
      "learning_rate": 9.16393129919492e-06,
      "loss": 0.4173,
      "step": 26
    },
    {
      "epoch": 0.03873744619799139,
      "grad_norm": 0.563133250503113,
      "learning_rate": 9.270082166367786e-06,
      "loss": 0.4328,
      "step": 27
    },
    {
      "epoch": 0.040172166427546625,
      "grad_norm": 0.644845155236506,
      "learning_rate": 9.372372135006298e-06,
      "loss": 0.4452,
      "step": 28
    },
    {
      "epoch": 0.04160688665710186,
      "grad_norm": 0.522257424635601,
      "learning_rate": 9.471072231886741e-06,
      "loss": 0.4303,
      "step": 29
    },
    {
      "epoch": 0.043041606886657105,
      "grad_norm": 0.5971300834305431,
      "learning_rate": 9.566425910596553e-06,
      "loss": 0.4113,
      "step": 30
    },
    {
      "epoch": 0.04447632711621234,
      "grad_norm": 0.5353092252564484,
      "learning_rate": 9.658652669428302e-06,
      "loss": 0.4074,
      "step": 31
    },
    {
      "epoch": 0.04591104734576758,
      "grad_norm": 0.47194138037498123,
      "learning_rate": 9.747951094689317e-06,
      "loss": 0.4124,
      "step": 32
    },
    {
      "epoch": 0.047345767575322814,
      "grad_norm": 0.5837214110708607,
      "learning_rate": 9.834501435568917e-06,
      "loss": 0.3874,
      "step": 33
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 0.4704767317300024,
      "learning_rate": 9.918467794511493e-06,
      "loss": 0.4139,
      "step": 34
    },
    {
      "epoch": 0.05021520803443329,
      "grad_norm": 0.5669458342952137,
      "learning_rate": 9.999999999999999e-06,
      "loss": 0.4069,
      "step": 35
    },
    {
      "epoch": 0.05164992826398852,
      "grad_norm": 0.5608390858456043,
      "learning_rate": 1e-05,
      "loss": 0.3829,
      "step": 36
    },
    {
      "epoch": 0.05308464849354376,
      "grad_norm": 0.5171982538119767,
      "learning_rate": 1e-05,
      "loss": 0.4136,
      "step": 37
    },
    {
      "epoch": 0.054519368723098996,
      "grad_norm": 0.49842283793446085,
      "learning_rate": 1e-05,
      "loss": 0.41,
      "step": 38
    },
    {
      "epoch": 0.05595408895265423,
      "grad_norm": 0.5342090086706823,
      "learning_rate": 1e-05,
      "loss": 0.4417,
      "step": 39
    },
    {
      "epoch": 0.05738880918220947,
      "grad_norm": 0.535899050154069,
      "learning_rate": 1e-05,
      "loss": 0.4519,
      "step": 40
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.5269174636008211,
      "learning_rate": 1e-05,
      "loss": 0.4228,
      "step": 41
    },
    {
      "epoch": 0.06025824964131994,
      "grad_norm": 0.5113164885449137,
      "learning_rate": 1e-05,
      "loss": 0.4359,
      "step": 42
    },
    {
      "epoch": 0.06169296987087518,
      "grad_norm": 0.4895841872216081,
      "learning_rate": 1e-05,
      "loss": 0.4089,
      "step": 43
    },
    {
      "epoch": 0.06312769010043041,
      "grad_norm": 0.5095148424240585,
      "learning_rate": 1e-05,
      "loss": 0.4458,
      "step": 44
    },
    {
      "epoch": 0.06456241032998565,
      "grad_norm": 0.5172910948949236,
      "learning_rate": 1e-05,
      "loss": 0.3909,
      "step": 45
    },
    {
      "epoch": 0.06599713055954089,
      "grad_norm": 0.5191884481442816,
      "learning_rate": 1e-05,
      "loss": 0.3753,
      "step": 46
    },
    {
      "epoch": 0.06743185078909612,
      "grad_norm": 0.5139856112786377,
      "learning_rate": 1e-05,
      "loss": 0.4269,
      "step": 47
    },
    {
      "epoch": 0.06886657101865136,
      "grad_norm": 0.5453195606516773,
      "learning_rate": 1e-05,
      "loss": 0.4033,
      "step": 48
    },
    {
      "epoch": 0.0703012912482066,
      "grad_norm": 0.5537211521771134,
      "learning_rate": 1e-05,
      "loss": 0.4414,
      "step": 49
    },
    {
      "epoch": 0.07173601147776183,
      "grad_norm": 0.5879773165092422,
      "learning_rate": 1e-05,
      "loss": 0.4517,
      "step": 50
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 0.5404955074631255,
      "learning_rate": 1e-05,
      "loss": 0.4438,
      "step": 51
    },
    {
      "epoch": 0.0746054519368723,
      "grad_norm": 0.5704173564956098,
      "learning_rate": 1e-05,
      "loss": 0.3864,
      "step": 52
    },
    {
      "epoch": 0.07604017216642754,
      "grad_norm": 0.5214965933380558,
      "learning_rate": 1e-05,
      "loss": 0.4153,
      "step": 53
    },
    {
      "epoch": 0.07747489239598278,
      "grad_norm": 0.5080295715201099,
      "learning_rate": 1e-05,
      "loss": 0.3999,
      "step": 54
    },
    {
      "epoch": 0.07890961262553801,
      "grad_norm": 0.5918241477826316,
      "learning_rate": 1e-05,
      "loss": 0.4131,
      "step": 55
    },
    {
      "epoch": 0.08034433285509325,
      "grad_norm": 0.5487791021469408,
      "learning_rate": 1e-05,
      "loss": 0.444,
      "step": 56
    },
    {
      "epoch": 0.08177905308464849,
      "grad_norm": 0.470556423067002,
      "learning_rate": 1e-05,
      "loss": 0.4111,
      "step": 57
    },
    {
      "epoch": 0.08321377331420372,
      "grad_norm": 0.4858597012414932,
      "learning_rate": 1e-05,
      "loss": 0.4366,
      "step": 58
    },
    {
      "epoch": 0.08464849354375897,
      "grad_norm": 0.4626382668694628,
      "learning_rate": 1e-05,
      "loss": 0.4231,
      "step": 59
    },
    {
      "epoch": 0.08608321377331421,
      "grad_norm": 0.4724260321197251,
      "learning_rate": 1e-05,
      "loss": 0.405,
      "step": 60
    },
    {
      "epoch": 0.08751793400286945,
      "grad_norm": 0.5029060369167712,
      "learning_rate": 1e-05,
      "loss": 0.4321,
      "step": 61
    },
    {
      "epoch": 0.08895265423242468,
      "grad_norm": 0.5034653469757331,
      "learning_rate": 1e-05,
      "loss": 0.4032,
      "step": 62
    },
    {
      "epoch": 0.09038737446197992,
      "grad_norm": 0.46813508071825277,
      "learning_rate": 1e-05,
      "loss": 0.4272,
      "step": 63
    },
    {
      "epoch": 0.09182209469153516,
      "grad_norm": 0.46843814824859425,
      "learning_rate": 1e-05,
      "loss": 0.4083,
      "step": 64
    },
    {
      "epoch": 0.09325681492109039,
      "grad_norm": 0.46212872202664507,
      "learning_rate": 1e-05,
      "loss": 0.4441,
      "step": 65
    },
    {
      "epoch": 0.09469153515064563,
      "grad_norm": 0.48298262973571104,
      "learning_rate": 1e-05,
      "loss": 0.427,
      "step": 66
    },
    {
      "epoch": 0.09612625538020086,
      "grad_norm": 0.5360920099952687,
      "learning_rate": 1e-05,
      "loss": 0.394,
      "step": 67
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 0.492656226273033,
      "learning_rate": 1e-05,
      "loss": 0.3893,
      "step": 68
    },
    {
      "epoch": 0.09899569583931134,
      "grad_norm": 0.5119827461656543,
      "learning_rate": 1e-05,
      "loss": 0.452,
      "step": 69
    },
    {
      "epoch": 0.10043041606886657,
      "grad_norm": 0.48247897945828366,
      "learning_rate": 1e-05,
      "loss": 0.3617,
      "step": 70
    },
    {
      "epoch": 0.10186513629842181,
      "grad_norm": 0.5028980274093359,
      "learning_rate": 1e-05,
      "loss": 0.4296,
      "step": 71
    },
    {
      "epoch": 0.10329985652797705,
      "grad_norm": 0.5032849820295575,
      "learning_rate": 1e-05,
      "loss": 0.4069,
      "step": 72
    },
    {
      "epoch": 0.10473457675753228,
      "grad_norm": 0.45861806039836933,
      "learning_rate": 1e-05,
      "loss": 0.4246,
      "step": 73
    },
    {
      "epoch": 0.10616929698708752,
      "grad_norm": 0.5449687271236857,
      "learning_rate": 1e-05,
      "loss": 0.3854,
      "step": 74
    },
    {
      "epoch": 0.10760401721664276,
      "grad_norm": 0.519894042477304,
      "learning_rate": 1e-05,
      "loss": 0.3638,
      "step": 75
    },
    {
      "epoch": 0.10903873744619799,
      "grad_norm": 0.4951581642888761,
      "learning_rate": 1e-05,
      "loss": 0.4083,
      "step": 76
    },
    {
      "epoch": 0.11047345767575323,
      "grad_norm": 0.49513014143612527,
      "learning_rate": 1e-05,
      "loss": 0.4079,
      "step": 77
    },
    {
      "epoch": 0.11190817790530846,
      "grad_norm": 0.5739506861889976,
      "learning_rate": 1e-05,
      "loss": 0.403,
      "step": 78
    },
    {
      "epoch": 0.1133428981348637,
      "grad_norm": 0.5336693273491505,
      "learning_rate": 1e-05,
      "loss": 0.4382,
      "step": 79
    },
    {
      "epoch": 0.11477761836441894,
      "grad_norm": 0.5655472826966493,
      "learning_rate": 1e-05,
      "loss": 0.4268,
      "step": 80
    },
    {
      "epoch": 0.11621233859397417,
      "grad_norm": 0.40218941425497107,
      "learning_rate": 1e-05,
      "loss": 0.4012,
      "step": 81
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.5247428077273156,
      "learning_rate": 1e-05,
      "loss": 0.3903,
      "step": 82
    },
    {
      "epoch": 0.11908177905308465,
      "grad_norm": 0.5017632656506401,
      "learning_rate": 1e-05,
      "loss": 0.3782,
      "step": 83
    },
    {
      "epoch": 0.12051649928263988,
      "grad_norm": 0.5210193716056686,
      "learning_rate": 1e-05,
      "loss": 0.4345,
      "step": 84
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 0.5532191989999315,
      "learning_rate": 1e-05,
      "loss": 0.3839,
      "step": 85
    },
    {
      "epoch": 0.12338593974175036,
      "grad_norm": 0.531106269713775,
      "learning_rate": 1e-05,
      "loss": 0.4209,
      "step": 86
    },
    {
      "epoch": 0.12482065997130559,
      "grad_norm": 0.4686674714825169,
      "learning_rate": 1e-05,
      "loss": 0.4149,
      "step": 87
    },
    {
      "epoch": 0.12625538020086083,
      "grad_norm": 0.4848651677324731,
      "learning_rate": 1e-05,
      "loss": 0.4305,
      "step": 88
    },
    {
      "epoch": 0.12769010043041606,
      "grad_norm": 0.44603207734668493,
      "learning_rate": 1e-05,
      "loss": 0.3867,
      "step": 89
    },
    {
      "epoch": 0.1291248206599713,
      "grad_norm": 0.5410676919750385,
      "learning_rate": 1e-05,
      "loss": 0.4045,
      "step": 90
    },
    {
      "epoch": 0.13055954088952654,
      "grad_norm": 0.49269737639876543,
      "learning_rate": 1e-05,
      "loss": 0.3841,
      "step": 91
    },
    {
      "epoch": 0.13199426111908177,
      "grad_norm": 0.6384960924257534,
      "learning_rate": 1e-05,
      "loss": 0.4264,
      "step": 92
    },
    {
      "epoch": 0.133428981348637,
      "grad_norm": 0.4713483667271366,
      "learning_rate": 1e-05,
      "loss": 0.4389,
      "step": 93
    },
    {
      "epoch": 0.13486370157819225,
      "grad_norm": 0.47308053329034383,
      "learning_rate": 1e-05,
      "loss": 0.4205,
      "step": 94
    },
    {
      "epoch": 0.13629842180774748,
      "grad_norm": 0.4882961639871336,
      "learning_rate": 1e-05,
      "loss": 0.391,
      "step": 95
    },
    {
      "epoch": 0.13773314203730272,
      "grad_norm": 0.45397589701611624,
      "learning_rate": 1e-05,
      "loss": 0.385,
      "step": 96
    },
    {
      "epoch": 0.13916786226685796,
      "grad_norm": 0.5073949995300743,
      "learning_rate": 1e-05,
      "loss": 0.4302,
      "step": 97
    },
    {
      "epoch": 0.1406025824964132,
      "grad_norm": 0.4685197293295307,
      "learning_rate": 1e-05,
      "loss": 0.4466,
      "step": 98
    },
    {
      "epoch": 0.14203730272596843,
      "grad_norm": 0.6052979044976298,
      "learning_rate": 1e-05,
      "loss": 0.4148,
      "step": 99
    },
    {
      "epoch": 0.14347202295552366,
      "grad_norm": 0.49438242945421923,
      "learning_rate": 1e-05,
      "loss": 0.3878,
      "step": 100
    },
    {
      "epoch": 0.1449067431850789,
      "grad_norm": 0.44855159486364193,
      "learning_rate": 1e-05,
      "loss": 0.4062,
      "step": 101
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 0.533435378226646,
      "learning_rate": 1e-05,
      "loss": 0.4069,
      "step": 102
    },
    {
      "epoch": 0.14777618364418937,
      "grad_norm": 0.4760861258394508,
      "learning_rate": 1e-05,
      "loss": 0.4078,
      "step": 103
    },
    {
      "epoch": 0.1492109038737446,
      "grad_norm": 0.6058699527963349,
      "learning_rate": 1e-05,
      "loss": 0.4179,
      "step": 104
    },
    {
      "epoch": 0.15064562410329985,
      "grad_norm": 0.5680088076635874,
      "learning_rate": 1e-05,
      "loss": 0.4582,
      "step": 105
    },
    {
      "epoch": 0.15208034433285508,
      "grad_norm": 0.5178800704665681,
      "learning_rate": 1e-05,
      "loss": 0.4349,
      "step": 106
    },
    {
      "epoch": 0.15351506456241032,
      "grad_norm": 0.4689375451223767,
      "learning_rate": 1e-05,
      "loss": 0.4241,
      "step": 107
    },
    {
      "epoch": 0.15494978479196556,
      "grad_norm": 0.4584786404454533,
      "learning_rate": 1e-05,
      "loss": 0.4513,
      "step": 108
    },
    {
      "epoch": 0.1563845050215208,
      "grad_norm": 0.4723897926671805,
      "learning_rate": 1e-05,
      "loss": 0.409,
      "step": 109
    },
    {
      "epoch": 0.15781922525107603,
      "grad_norm": 0.48849721289533726,
      "learning_rate": 1e-05,
      "loss": 0.409,
      "step": 110
    },
    {
      "epoch": 0.15925394548063126,
      "grad_norm": 0.469812768056165,
      "learning_rate": 1e-05,
      "loss": 0.4031,
      "step": 111
    },
    {
      "epoch": 0.1606886657101865,
      "grad_norm": 0.5298777215433308,
      "learning_rate": 1e-05,
      "loss": 0.4202,
      "step": 112
    },
    {
      "epoch": 0.16212338593974174,
      "grad_norm": 0.5119723430015735,
      "learning_rate": 1e-05,
      "loss": 0.4259,
      "step": 113
    },
    {
      "epoch": 0.16355810616929697,
      "grad_norm": 0.4459142338423788,
      "learning_rate": 1e-05,
      "loss": 0.4241,
      "step": 114
    },
    {
      "epoch": 0.1649928263988522,
      "grad_norm": 0.4242096380234043,
      "learning_rate": 1e-05,
      "loss": 0.4057,
      "step": 115
    },
    {
      "epoch": 0.16642754662840745,
      "grad_norm": 0.5508460802795664,
      "learning_rate": 1e-05,
      "loss": 0.3965,
      "step": 116
    },
    {
      "epoch": 0.1678622668579627,
      "grad_norm": 0.49988518357906786,
      "learning_rate": 1e-05,
      "loss": 0.4254,
      "step": 117
    },
    {
      "epoch": 0.16929698708751795,
      "grad_norm": 0.5169332440191942,
      "learning_rate": 1e-05,
      "loss": 0.396,
      "step": 118
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 0.48742816194456434,
      "learning_rate": 1e-05,
      "loss": 0.4215,
      "step": 119
    },
    {
      "epoch": 0.17216642754662842,
      "grad_norm": 0.5129478544407357,
      "learning_rate": 1e-05,
      "loss": 0.4169,
      "step": 120
    },
    {
      "epoch": 0.17360114777618366,
      "grad_norm": 0.43349407941406176,
      "learning_rate": 1e-05,
      "loss": 0.4021,
      "step": 121
    },
    {
      "epoch": 0.1750358680057389,
      "grad_norm": 0.46164880554365495,
      "learning_rate": 1e-05,
      "loss": 0.3901,
      "step": 122
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.5376777676647662,
      "learning_rate": 1e-05,
      "loss": 0.4062,
      "step": 123
    },
    {
      "epoch": 0.17790530846484937,
      "grad_norm": 0.45765058393277736,
      "learning_rate": 1e-05,
      "loss": 0.4279,
      "step": 124
    },
    {
      "epoch": 0.1793400286944046,
      "grad_norm": 0.5016167986973149,
      "learning_rate": 1e-05,
      "loss": 0.4024,
      "step": 125
    },
    {
      "epoch": 0.18077474892395984,
      "grad_norm": 0.5385052674526711,
      "learning_rate": 1e-05,
      "loss": 0.3956,
      "step": 126
    },
    {
      "epoch": 0.18220946915351507,
      "grad_norm": 0.5621912856761463,
      "learning_rate": 1e-05,
      "loss": 0.401,
      "step": 127
    },
    {
      "epoch": 0.1836441893830703,
      "grad_norm": 0.5182970531168064,
      "learning_rate": 1e-05,
      "loss": 0.4156,
      "step": 128
    },
    {
      "epoch": 0.18507890961262555,
      "grad_norm": 0.49269634700750026,
      "learning_rate": 1e-05,
      "loss": 0.4146,
      "step": 129
    },
    {
      "epoch": 0.18651362984218078,
      "grad_norm": 0.4291828914371975,
      "learning_rate": 1e-05,
      "loss": 0.4032,
      "step": 130
    },
    {
      "epoch": 0.18794835007173602,
      "grad_norm": 0.5007280037909055,
      "learning_rate": 1e-05,
      "loss": 0.4287,
      "step": 131
    },
    {
      "epoch": 0.18938307030129126,
      "grad_norm": 0.49595504865879997,
      "learning_rate": 1e-05,
      "loss": 0.4098,
      "step": 132
    },
    {
      "epoch": 0.1908177905308465,
      "grad_norm": 0.5117109039945522,
      "learning_rate": 1e-05,
      "loss": 0.4449,
      "step": 133
    },
    {
      "epoch": 0.19225251076040173,
      "grad_norm": 0.5394851908547341,
      "learning_rate": 1e-05,
      "loss": 0.4021,
      "step": 134
    },
    {
      "epoch": 0.19368723098995697,
      "grad_norm": 0.5427832503151591,
      "learning_rate": 1e-05,
      "loss": 0.4226,
      "step": 135
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 0.5483453501508279,
      "learning_rate": 1e-05,
      "loss": 0.4634,
      "step": 136
    },
    {
      "epoch": 0.19655667144906744,
      "grad_norm": 0.5000901283269746,
      "learning_rate": 1e-05,
      "loss": 0.4145,
      "step": 137
    },
    {
      "epoch": 0.19799139167862267,
      "grad_norm": 0.5076294621476616,
      "learning_rate": 1e-05,
      "loss": 0.4095,
      "step": 138
    },
    {
      "epoch": 0.1994261119081779,
      "grad_norm": 0.5457287353206532,
      "learning_rate": 1e-05,
      "loss": 0.4309,
      "step": 139
    },
    {
      "epoch": 0.20086083213773315,
      "grad_norm": 0.48602867316436477,
      "learning_rate": 1e-05,
      "loss": 0.4009,
      "step": 140
    },
    {
      "epoch": 0.20229555236728838,
      "grad_norm": 0.5193999303229946,
      "learning_rate": 1e-05,
      "loss": 0.4112,
      "step": 141
    },
    {
      "epoch": 0.20373027259684362,
      "grad_norm": 0.4666813433091568,
      "learning_rate": 1e-05,
      "loss": 0.4144,
      "step": 142
    },
    {
      "epoch": 0.20516499282639886,
      "grad_norm": 0.589705403169264,
      "learning_rate": 1e-05,
      "loss": 0.4094,
      "step": 143
    },
    {
      "epoch": 0.2065997130559541,
      "grad_norm": 0.4205476618216609,
      "learning_rate": 1e-05,
      "loss": 0.4052,
      "step": 144
    },
    {
      "epoch": 0.20803443328550933,
      "grad_norm": 0.44154904403896944,
      "learning_rate": 1e-05,
      "loss": 0.393,
      "step": 145
    },
    {
      "epoch": 0.20946915351506457,
      "grad_norm": 0.5032258132883796,
      "learning_rate": 1e-05,
      "loss": 0.3743,
      "step": 146
    },
    {
      "epoch": 0.2109038737446198,
      "grad_norm": 0.476004515433623,
      "learning_rate": 1e-05,
      "loss": 0.4073,
      "step": 147
    },
    {
      "epoch": 0.21233859397417504,
      "grad_norm": 0.5281874181443527,
      "learning_rate": 1e-05,
      "loss": 0.4505,
      "step": 148
    },
    {
      "epoch": 0.21377331420373027,
      "grad_norm": 0.47192427932749426,
      "learning_rate": 1e-05,
      "loss": 0.4452,
      "step": 149
    },
    {
      "epoch": 0.2152080344332855,
      "grad_norm": 0.5135594246713516,
      "learning_rate": 1e-05,
      "loss": 0.4005,
      "step": 150
    },
    {
      "epoch": 0.21664275466284075,
      "grad_norm": 0.46806189440845425,
      "learning_rate": 1e-05,
      "loss": 0.387,
      "step": 151
    },
    {
      "epoch": 0.21807747489239598,
      "grad_norm": 0.521953740953707,
      "learning_rate": 1e-05,
      "loss": 0.4099,
      "step": 152
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 0.5149936068850601,
      "learning_rate": 1e-05,
      "loss": 0.4358,
      "step": 153
    },
    {
      "epoch": 0.22094691535150646,
      "grad_norm": 0.5199313418236045,
      "learning_rate": 1e-05,
      "loss": 0.4423,
      "step": 154
    },
    {
      "epoch": 0.2223816355810617,
      "grad_norm": 0.43816600516148296,
      "learning_rate": 1e-05,
      "loss": 0.4211,
      "step": 155
    },
    {
      "epoch": 0.22381635581061693,
      "grad_norm": 0.47732611752171494,
      "learning_rate": 1e-05,
      "loss": 0.3806,
      "step": 156
    },
    {
      "epoch": 0.22525107604017217,
      "grad_norm": 0.48353431351710396,
      "learning_rate": 1e-05,
      "loss": 0.3932,
      "step": 157
    },
    {
      "epoch": 0.2266857962697274,
      "grad_norm": 0.43214050509829394,
      "learning_rate": 1e-05,
      "loss": 0.3928,
      "step": 158
    },
    {
      "epoch": 0.22812051649928264,
      "grad_norm": 0.4780339084736589,
      "learning_rate": 1e-05,
      "loss": 0.3974,
      "step": 159
    },
    {
      "epoch": 0.22955523672883787,
      "grad_norm": 0.46265921813455124,
      "learning_rate": 1e-05,
      "loss": 0.398,
      "step": 160
    },
    {
      "epoch": 0.2309899569583931,
      "grad_norm": 0.560222443655787,
      "learning_rate": 1e-05,
      "loss": 0.418,
      "step": 161
    },
    {
      "epoch": 0.23242467718794835,
      "grad_norm": 0.433856387379962,
      "learning_rate": 1e-05,
      "loss": 0.4468,
      "step": 162
    },
    {
      "epoch": 0.23385939741750358,
      "grad_norm": 0.5042579012119037,
      "learning_rate": 1e-05,
      "loss": 0.4205,
      "step": 163
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.49272429307856214,
      "learning_rate": 1e-05,
      "loss": 0.4415,
      "step": 164
    },
    {
      "epoch": 0.23672883787661406,
      "grad_norm": 0.5563105485420717,
      "learning_rate": 1e-05,
      "loss": 0.39,
      "step": 165
    },
    {
      "epoch": 0.2381635581061693,
      "grad_norm": 0.4477838044079509,
      "learning_rate": 1e-05,
      "loss": 0.3947,
      "step": 166
    },
    {
      "epoch": 0.23959827833572453,
      "grad_norm": 0.47496938202753497,
      "learning_rate": 1e-05,
      "loss": 0.4055,
      "step": 167
    },
    {
      "epoch": 0.24103299856527977,
      "grad_norm": 0.47770561710466647,
      "learning_rate": 1e-05,
      "loss": 0.399,
      "step": 168
    },
    {
      "epoch": 0.242467718794835,
      "grad_norm": 0.47153811165342135,
      "learning_rate": 1e-05,
      "loss": 0.4162,
      "step": 169
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 0.5089141770239528,
      "learning_rate": 1e-05,
      "loss": 0.4208,
      "step": 170
    },
    {
      "epoch": 0.24533715925394547,
      "grad_norm": 0.4419832458856324,
      "learning_rate": 1e-05,
      "loss": 0.3988,
      "step": 171
    },
    {
      "epoch": 0.2467718794835007,
      "grad_norm": 0.4720541052557969,
      "learning_rate": 1e-05,
      "loss": 0.4672,
      "step": 172
    },
    {
      "epoch": 0.24820659971305595,
      "grad_norm": 0.4164990074891946,
      "learning_rate": 1e-05,
      "loss": 0.4073,
      "step": 173
    },
    {
      "epoch": 0.24964131994261118,
      "grad_norm": 0.45387320453504126,
      "learning_rate": 1e-05,
      "loss": 0.4264,
      "step": 174
    },
    {
      "epoch": 0.25107604017216645,
      "grad_norm": 0.42265782686982,
      "learning_rate": 1e-05,
      "loss": 0.4084,
      "step": 175
    },
    {
      "epoch": 0.25251076040172166,
      "grad_norm": 0.46842186356458354,
      "learning_rate": 1e-05,
      "loss": 0.4299,
      "step": 176
    },
    {
      "epoch": 0.2539454806312769,
      "grad_norm": 0.4985318334217286,
      "learning_rate": 1e-05,
      "loss": 0.3966,
      "step": 177
    },
    {
      "epoch": 0.25538020086083213,
      "grad_norm": 0.48912814514157765,
      "learning_rate": 1e-05,
      "loss": 0.4146,
      "step": 178
    },
    {
      "epoch": 0.2568149210903874,
      "grad_norm": 0.4621349908017437,
      "learning_rate": 1e-05,
      "loss": 0.4448,
      "step": 179
    },
    {
      "epoch": 0.2582496413199426,
      "grad_norm": 0.44227402306246866,
      "learning_rate": 1e-05,
      "loss": 0.4179,
      "step": 180
    },
    {
      "epoch": 0.25968436154949787,
      "grad_norm": 0.46641392360112827,
      "learning_rate": 1e-05,
      "loss": 0.4171,
      "step": 181
    },
    {
      "epoch": 0.2611190817790531,
      "grad_norm": 0.5310942700444391,
      "learning_rate": 1e-05,
      "loss": 0.3657,
      "step": 182
    },
    {
      "epoch": 0.26255380200860834,
      "grad_norm": 0.49863046896201396,
      "learning_rate": 1e-05,
      "loss": 0.3782,
      "step": 183
    },
    {
      "epoch": 0.26398852223816355,
      "grad_norm": 0.4100428324158607,
      "learning_rate": 1e-05,
      "loss": 0.4035,
      "step": 184
    },
    {
      "epoch": 0.2654232424677188,
      "grad_norm": 0.4304175482633679,
      "learning_rate": 1e-05,
      "loss": 0.4112,
      "step": 185
    },
    {
      "epoch": 0.266857962697274,
      "grad_norm": 0.5385291183459494,
      "learning_rate": 1e-05,
      "loss": 0.4321,
      "step": 186
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 0.4875817891364632,
      "learning_rate": 1e-05,
      "loss": 0.4327,
      "step": 187
    },
    {
      "epoch": 0.2697274031563845,
      "grad_norm": 0.4332581814565724,
      "learning_rate": 1e-05,
      "loss": 0.4091,
      "step": 188
    },
    {
      "epoch": 0.27116212338593976,
      "grad_norm": 0.45230802879182336,
      "learning_rate": 1e-05,
      "loss": 0.429,
      "step": 189
    },
    {
      "epoch": 0.27259684361549497,
      "grad_norm": 0.45354828837723166,
      "learning_rate": 1e-05,
      "loss": 0.3965,
      "step": 190
    },
    {
      "epoch": 0.27403156384505023,
      "grad_norm": 0.4816106661409351,
      "learning_rate": 1e-05,
      "loss": 0.3989,
      "step": 191
    },
    {
      "epoch": 0.27546628407460544,
      "grad_norm": 0.4445953686087989,
      "learning_rate": 1e-05,
      "loss": 0.4206,
      "step": 192
    },
    {
      "epoch": 0.2769010043041607,
      "grad_norm": 0.45685389030935525,
      "learning_rate": 1e-05,
      "loss": 0.412,
      "step": 193
    },
    {
      "epoch": 0.2783357245337159,
      "grad_norm": 0.48802845801774014,
      "learning_rate": 1e-05,
      "loss": 0.4148,
      "step": 194
    },
    {
      "epoch": 0.2797704447632712,
      "grad_norm": 0.4257789860878407,
      "learning_rate": 1e-05,
      "loss": 0.4244,
      "step": 195
    },
    {
      "epoch": 0.2812051649928264,
      "grad_norm": 0.6356432393036265,
      "learning_rate": 1e-05,
      "loss": 0.4645,
      "step": 196
    },
    {
      "epoch": 0.28263988522238165,
      "grad_norm": 0.4945032862273697,
      "learning_rate": 1e-05,
      "loss": 0.4215,
      "step": 197
    },
    {
      "epoch": 0.28407460545193686,
      "grad_norm": 0.5525909507692311,
      "learning_rate": 1e-05,
      "loss": 0.4106,
      "step": 198
    },
    {
      "epoch": 0.2855093256814921,
      "grad_norm": 0.49277639308637305,
      "learning_rate": 1e-05,
      "loss": 0.4104,
      "step": 199
    },
    {
      "epoch": 0.28694404591104733,
      "grad_norm": 0.506829601330288,
      "learning_rate": 1e-05,
      "loss": 0.4052,
      "step": 200
    },
    {
      "epoch": 0.2883787661406026,
      "grad_norm": 0.5277691250978517,
      "learning_rate": 1e-05,
      "loss": 0.454,
      "step": 201
    },
    {
      "epoch": 0.2898134863701578,
      "grad_norm": 0.47074468745557896,
      "learning_rate": 1e-05,
      "loss": 0.3917,
      "step": 202
    },
    {
      "epoch": 0.29124820659971307,
      "grad_norm": 0.453346181624346,
      "learning_rate": 1e-05,
      "loss": 0.3732,
      "step": 203
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 0.42316196722289606,
      "learning_rate": 1e-05,
      "loss": 0.3623,
      "step": 204
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.47925464239902377,
      "learning_rate": 1e-05,
      "loss": 0.4138,
      "step": 205
    },
    {
      "epoch": 0.29555236728837875,
      "grad_norm": 0.45072661614927206,
      "learning_rate": 1e-05,
      "loss": 0.3789,
      "step": 206
    },
    {
      "epoch": 0.296987087517934,
      "grad_norm": 0.5652073624817923,
      "learning_rate": 1e-05,
      "loss": 0.4287,
      "step": 207
    },
    {
      "epoch": 0.2984218077474892,
      "grad_norm": 0.4780283333572154,
      "learning_rate": 1e-05,
      "loss": 0.4404,
      "step": 208
    },
    {
      "epoch": 0.2998565279770445,
      "grad_norm": 0.5250066789120631,
      "learning_rate": 1e-05,
      "loss": 0.4373,
      "step": 209
    },
    {
      "epoch": 0.3012912482065997,
      "grad_norm": 0.4441645846540194,
      "learning_rate": 1e-05,
      "loss": 0.3867,
      "step": 210
    },
    {
      "epoch": 0.30272596843615496,
      "grad_norm": 0.4582077413555811,
      "learning_rate": 1e-05,
      "loss": 0.4139,
      "step": 211
    },
    {
      "epoch": 0.30416068866571017,
      "grad_norm": 0.48279289977973994,
      "learning_rate": 1e-05,
      "loss": 0.4207,
      "step": 212
    },
    {
      "epoch": 0.30559540889526543,
      "grad_norm": 0.44894097328107335,
      "learning_rate": 1e-05,
      "loss": 0.3938,
      "step": 213
    },
    {
      "epoch": 0.30703012912482064,
      "grad_norm": 0.4923092410377479,
      "learning_rate": 1e-05,
      "loss": 0.4196,
      "step": 214
    },
    {
      "epoch": 0.3084648493543759,
      "grad_norm": 0.3993514860223951,
      "learning_rate": 1e-05,
      "loss": 0.3985,
      "step": 215
    },
    {
      "epoch": 0.3098995695839311,
      "grad_norm": 0.4463456532297698,
      "learning_rate": 1e-05,
      "loss": 0.3927,
      "step": 216
    },
    {
      "epoch": 0.3113342898134864,
      "grad_norm": 0.4659298089410052,
      "learning_rate": 1e-05,
      "loss": 0.3854,
      "step": 217
    },
    {
      "epoch": 0.3127690100430416,
      "grad_norm": 0.4128631720885418,
      "learning_rate": 1e-05,
      "loss": 0.4112,
      "step": 218
    },
    {
      "epoch": 0.31420373027259685,
      "grad_norm": 0.47471431924985036,
      "learning_rate": 1e-05,
      "loss": 0.4131,
      "step": 219
    },
    {
      "epoch": 0.31563845050215206,
      "grad_norm": 0.5449122990177167,
      "learning_rate": 1e-05,
      "loss": 0.4047,
      "step": 220
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 0.40704652152680676,
      "learning_rate": 1e-05,
      "loss": 0.4063,
      "step": 221
    },
    {
      "epoch": 0.31850789096126253,
      "grad_norm": 0.45011817900791345,
      "learning_rate": 1e-05,
      "loss": 0.4045,
      "step": 222
    },
    {
      "epoch": 0.3199426111908178,
      "grad_norm": 0.5025706189461868,
      "learning_rate": 1e-05,
      "loss": 0.4175,
      "step": 223
    },
    {
      "epoch": 0.321377331420373,
      "grad_norm": 0.4938256725903272,
      "learning_rate": 1e-05,
      "loss": 0.3885,
      "step": 224
    },
    {
      "epoch": 0.32281205164992827,
      "grad_norm": 0.4532806480326304,
      "learning_rate": 1e-05,
      "loss": 0.3713,
      "step": 225
    },
    {
      "epoch": 0.3242467718794835,
      "grad_norm": 0.42080437622451083,
      "learning_rate": 1e-05,
      "loss": 0.4022,
      "step": 226
    },
    {
      "epoch": 0.32568149210903874,
      "grad_norm": 0.49657970225693115,
      "learning_rate": 1e-05,
      "loss": 0.396,
      "step": 227
    },
    {
      "epoch": 0.32711621233859395,
      "grad_norm": 0.4404631527551926,
      "learning_rate": 1e-05,
      "loss": 0.4353,
      "step": 228
    },
    {
      "epoch": 0.3285509325681492,
      "grad_norm": 0.5949127020543797,
      "learning_rate": 1e-05,
      "loss": 0.4311,
      "step": 229
    },
    {
      "epoch": 0.3299856527977044,
      "grad_norm": 0.5377322475931535,
      "learning_rate": 1e-05,
      "loss": 0.4363,
      "step": 230
    },
    {
      "epoch": 0.3314203730272597,
      "grad_norm": 0.45134026296084556,
      "learning_rate": 1e-05,
      "loss": 0.3756,
      "step": 231
    },
    {
      "epoch": 0.3328550932568149,
      "grad_norm": 0.4423840322032375,
      "learning_rate": 1e-05,
      "loss": 0.4353,
      "step": 232
    },
    {
      "epoch": 0.33428981348637016,
      "grad_norm": 0.3968813499491523,
      "learning_rate": 1e-05,
      "loss": 0.4114,
      "step": 233
    },
    {
      "epoch": 0.3357245337159254,
      "grad_norm": 0.49470039107050273,
      "learning_rate": 1e-05,
      "loss": 0.4051,
      "step": 234
    },
    {
      "epoch": 0.33715925394548063,
      "grad_norm": 0.47083231293791195,
      "learning_rate": 1e-05,
      "loss": 0.3713,
      "step": 235
    },
    {
      "epoch": 0.3385939741750359,
      "grad_norm": 0.5339439398862439,
      "learning_rate": 1e-05,
      "loss": 0.4259,
      "step": 236
    },
    {
      "epoch": 0.3400286944045911,
      "grad_norm": 0.4555834639219456,
      "learning_rate": 1e-05,
      "loss": 0.3765,
      "step": 237
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 0.4924171012792959,
      "learning_rate": 1e-05,
      "loss": 0.4476,
      "step": 238
    },
    {
      "epoch": 0.3428981348637016,
      "grad_norm": 0.4766198091086857,
      "learning_rate": 1e-05,
      "loss": 0.3482,
      "step": 239
    },
    {
      "epoch": 0.34433285509325684,
      "grad_norm": 0.4236926628835053,
      "learning_rate": 1e-05,
      "loss": 0.3908,
      "step": 240
    },
    {
      "epoch": 0.34576757532281205,
      "grad_norm": 0.46644991763202026,
      "learning_rate": 1e-05,
      "loss": 0.43,
      "step": 241
    },
    {
      "epoch": 0.3472022955523673,
      "grad_norm": 0.4518266563362077,
      "learning_rate": 1e-05,
      "loss": 0.4237,
      "step": 242
    },
    {
      "epoch": 0.3486370157819225,
      "grad_norm": 0.4479407823088453,
      "learning_rate": 1e-05,
      "loss": 0.4223,
      "step": 243
    },
    {
      "epoch": 0.3500717360114778,
      "grad_norm": 0.5225076151825736,
      "learning_rate": 1e-05,
      "loss": 0.4284,
      "step": 244
    },
    {
      "epoch": 0.351506456241033,
      "grad_norm": 0.5193149769743607,
      "learning_rate": 1e-05,
      "loss": 0.4401,
      "step": 245
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 0.4305841463049054,
      "learning_rate": 1e-05,
      "loss": 0.41,
      "step": 246
    },
    {
      "epoch": 0.35437589670014347,
      "grad_norm": 0.45877108576210046,
      "learning_rate": 1e-05,
      "loss": 0.4234,
      "step": 247
    },
    {
      "epoch": 0.35581061692969873,
      "grad_norm": 0.4677189938027688,
      "learning_rate": 1e-05,
      "loss": 0.3929,
      "step": 248
    },
    {
      "epoch": 0.35724533715925394,
      "grad_norm": 0.46218089612484536,
      "learning_rate": 1e-05,
      "loss": 0.4417,
      "step": 249
    },
    {
      "epoch": 0.3586800573888092,
      "grad_norm": 0.507119030904443,
      "learning_rate": 1e-05,
      "loss": 0.4406,
      "step": 250
    },
    {
      "epoch": 0.3601147776183644,
      "grad_norm": 0.47586617415300425,
      "learning_rate": 1e-05,
      "loss": 0.3941,
      "step": 251
    },
    {
      "epoch": 0.3615494978479197,
      "grad_norm": 0.48598911194560424,
      "learning_rate": 1e-05,
      "loss": 0.4368,
      "step": 252
    },
    {
      "epoch": 0.3629842180774749,
      "grad_norm": 0.499964283311635,
      "learning_rate": 1e-05,
      "loss": 0.3955,
      "step": 253
    },
    {
      "epoch": 0.36441893830703015,
      "grad_norm": 0.5524647264728955,
      "learning_rate": 1e-05,
      "loss": 0.4235,
      "step": 254
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 0.47755927599015213,
      "learning_rate": 1e-05,
      "loss": 0.4019,
      "step": 255
    },
    {
      "epoch": 0.3672883787661406,
      "grad_norm": 0.5139108347396366,
      "learning_rate": 1e-05,
      "loss": 0.4159,
      "step": 256
    },
    {
      "epoch": 0.36872309899569583,
      "grad_norm": 0.42827021604591475,
      "learning_rate": 1e-05,
      "loss": 0.4112,
      "step": 257
    },
    {
      "epoch": 0.3701578192252511,
      "grad_norm": 0.5228083119987939,
      "learning_rate": 1e-05,
      "loss": 0.3868,
      "step": 258
    },
    {
      "epoch": 0.3715925394548063,
      "grad_norm": 0.49582014370260546,
      "learning_rate": 1e-05,
      "loss": 0.4289,
      "step": 259
    },
    {
      "epoch": 0.37302725968436157,
      "grad_norm": 0.4899565897797266,
      "learning_rate": 1e-05,
      "loss": 0.4086,
      "step": 260
    },
    {
      "epoch": 0.3744619799139168,
      "grad_norm": 0.4566285052338205,
      "learning_rate": 1e-05,
      "loss": 0.4118,
      "step": 261
    },
    {
      "epoch": 0.37589670014347204,
      "grad_norm": 0.5283099116840692,
      "learning_rate": 1e-05,
      "loss": 0.4427,
      "step": 262
    },
    {
      "epoch": 0.37733142037302725,
      "grad_norm": 0.5359341847379151,
      "learning_rate": 1e-05,
      "loss": 0.4419,
      "step": 263
    },
    {
      "epoch": 0.3787661406025825,
      "grad_norm": 0.546656856936709,
      "learning_rate": 1e-05,
      "loss": 0.407,
      "step": 264
    },
    {
      "epoch": 0.3802008608321377,
      "grad_norm": 0.5019701316132693,
      "learning_rate": 1e-05,
      "loss": 0.4494,
      "step": 265
    },
    {
      "epoch": 0.381635581061693,
      "grad_norm": 0.4831656688074343,
      "learning_rate": 1e-05,
      "loss": 0.4151,
      "step": 266
    },
    {
      "epoch": 0.3830703012912482,
      "grad_norm": 0.5168254215213619,
      "learning_rate": 1e-05,
      "loss": 0.409,
      "step": 267
    },
    {
      "epoch": 0.38450502152080346,
      "grad_norm": 0.43391946441044943,
      "learning_rate": 1e-05,
      "loss": 0.3692,
      "step": 268
    },
    {
      "epoch": 0.38593974175035867,
      "grad_norm": 0.47006519837261174,
      "learning_rate": 1e-05,
      "loss": 0.3875,
      "step": 269
    },
    {
      "epoch": 0.38737446197991393,
      "grad_norm": 0.46520085403367306,
      "learning_rate": 1e-05,
      "loss": 0.4277,
      "step": 270
    },
    {
      "epoch": 0.38880918220946914,
      "grad_norm": 0.4959530685254724,
      "learning_rate": 1e-05,
      "loss": 0.3929,
      "step": 271
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 0.3899784955474501,
      "learning_rate": 1e-05,
      "loss": 0.4037,
      "step": 272
    },
    {
      "epoch": 0.3916786226685796,
      "grad_norm": 0.4353061238392487,
      "learning_rate": 1e-05,
      "loss": 0.3927,
      "step": 273
    },
    {
      "epoch": 0.3931133428981349,
      "grad_norm": 0.5104919379367013,
      "learning_rate": 1e-05,
      "loss": 0.406,
      "step": 274
    },
    {
      "epoch": 0.3945480631276901,
      "grad_norm": 0.5045653656520177,
      "learning_rate": 1e-05,
      "loss": 0.3914,
      "step": 275
    },
    {
      "epoch": 0.39598278335724535,
      "grad_norm": 0.5768254184501557,
      "learning_rate": 1e-05,
      "loss": 0.4391,
      "step": 276
    },
    {
      "epoch": 0.39741750358680056,
      "grad_norm": 0.4810487695887796,
      "learning_rate": 1e-05,
      "loss": 0.386,
      "step": 277
    },
    {
      "epoch": 0.3988522238163558,
      "grad_norm": 0.4746833373044808,
      "learning_rate": 1e-05,
      "loss": 0.358,
      "step": 278
    },
    {
      "epoch": 0.40028694404591103,
      "grad_norm": 0.49486373489001034,
      "learning_rate": 1e-05,
      "loss": 0.3959,
      "step": 279
    },
    {
      "epoch": 0.4017216642754663,
      "grad_norm": 0.5352013808577107,
      "learning_rate": 1e-05,
      "loss": 0.4061,
      "step": 280
    },
    {
      "epoch": 0.4031563845050215,
      "grad_norm": 0.43258934314265735,
      "learning_rate": 1e-05,
      "loss": 0.406,
      "step": 281
    },
    {
      "epoch": 0.40459110473457677,
      "grad_norm": 0.4227175257683693,
      "learning_rate": 1e-05,
      "loss": 0.4066,
      "step": 282
    },
    {
      "epoch": 0.406025824964132,
      "grad_norm": 0.47984914648271937,
      "learning_rate": 1e-05,
      "loss": 0.4107,
      "step": 283
    },
    {
      "epoch": 0.40746054519368724,
      "grad_norm": 0.5254134847469413,
      "learning_rate": 1e-05,
      "loss": 0.4099,
      "step": 284
    },
    {
      "epoch": 0.40889526542324245,
      "grad_norm": 0.4774793836563544,
      "learning_rate": 1e-05,
      "loss": 0.3828,
      "step": 285
    },
    {
      "epoch": 0.4103299856527977,
      "grad_norm": 0.4590367462162679,
      "learning_rate": 1e-05,
      "loss": 0.393,
      "step": 286
    },
    {
      "epoch": 0.4117647058823529,
      "grad_norm": 0.5019343028460593,
      "learning_rate": 1e-05,
      "loss": 0.3579,
      "step": 287
    },
    {
      "epoch": 0.4131994261119082,
      "grad_norm": 0.4720507728477997,
      "learning_rate": 1e-05,
      "loss": 0.4121,
      "step": 288
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 0.4055472457774507,
      "learning_rate": 1e-05,
      "loss": 0.4156,
      "step": 289
    },
    {
      "epoch": 0.41606886657101866,
      "grad_norm": 0.44036100480572316,
      "learning_rate": 1e-05,
      "loss": 0.411,
      "step": 290
    },
    {
      "epoch": 0.41750358680057387,
      "grad_norm": 0.5903435358549372,
      "learning_rate": 1e-05,
      "loss": 0.3983,
      "step": 291
    },
    {
      "epoch": 0.41893830703012913,
      "grad_norm": 0.4842256588356352,
      "learning_rate": 1e-05,
      "loss": 0.4001,
      "step": 292
    },
    {
      "epoch": 0.42037302725968434,
      "grad_norm": 0.4115480134348991,
      "learning_rate": 1e-05,
      "loss": 0.4068,
      "step": 293
    },
    {
      "epoch": 0.4218077474892396,
      "grad_norm": 0.4549697512756008,
      "learning_rate": 1e-05,
      "loss": 0.4345,
      "step": 294
    },
    {
      "epoch": 0.4232424677187948,
      "grad_norm": 0.5214243243194162,
      "learning_rate": 1e-05,
      "loss": 0.4328,
      "step": 295
    },
    {
      "epoch": 0.4246771879483501,
      "grad_norm": 0.5079652060142748,
      "learning_rate": 1e-05,
      "loss": 0.3854,
      "step": 296
    },
    {
      "epoch": 0.4261119081779053,
      "grad_norm": 0.45939486816520403,
      "learning_rate": 1e-05,
      "loss": 0.4418,
      "step": 297
    },
    {
      "epoch": 0.42754662840746055,
      "grad_norm": 0.45434066779009946,
      "learning_rate": 1e-05,
      "loss": 0.428,
      "step": 298
    },
    {
      "epoch": 0.42898134863701576,
      "grad_norm": 0.44676986738215846,
      "learning_rate": 1e-05,
      "loss": 0.4025,
      "step": 299
    },
    {
      "epoch": 0.430416068866571,
      "grad_norm": 0.42363826418479317,
      "learning_rate": 1e-05,
      "loss": 0.422,
      "step": 300
    },
    {
      "epoch": 0.43185078909612623,
      "grad_norm": 0.4591303031987548,
      "learning_rate": 1e-05,
      "loss": 0.4142,
      "step": 301
    },
    {
      "epoch": 0.4332855093256815,
      "grad_norm": 0.49228855129399945,
      "learning_rate": 1e-05,
      "loss": 0.418,
      "step": 302
    },
    {
      "epoch": 0.4347202295552367,
      "grad_norm": 0.43958513723716836,
      "learning_rate": 1e-05,
      "loss": 0.3858,
      "step": 303
    },
    {
      "epoch": 0.43615494978479197,
      "grad_norm": 0.5296107912823073,
      "learning_rate": 1e-05,
      "loss": 0.4028,
      "step": 304
    },
    {
      "epoch": 0.4375896700143472,
      "grad_norm": 0.4690713050130183,
      "learning_rate": 1e-05,
      "loss": 0.4023,
      "step": 305
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 0.45069266062379015,
      "learning_rate": 1e-05,
      "loss": 0.411,
      "step": 306
    },
    {
      "epoch": 0.44045911047345765,
      "grad_norm": 0.4794151478252184,
      "learning_rate": 1e-05,
      "loss": 0.4016,
      "step": 307
    },
    {
      "epoch": 0.4418938307030129,
      "grad_norm": 0.4973682730268541,
      "learning_rate": 1e-05,
      "loss": 0.4379,
      "step": 308
    },
    {
      "epoch": 0.4433285509325681,
      "grad_norm": 0.5076069468099442,
      "learning_rate": 1e-05,
      "loss": 0.4331,
      "step": 309
    },
    {
      "epoch": 0.4447632711621234,
      "grad_norm": 0.47043809425578736,
      "learning_rate": 1e-05,
      "loss": 0.4312,
      "step": 310
    },
    {
      "epoch": 0.44619799139167865,
      "grad_norm": 0.42597006563823436,
      "learning_rate": 1e-05,
      "loss": 0.4337,
      "step": 311
    },
    {
      "epoch": 0.44763271162123386,
      "grad_norm": 0.45082445304066016,
      "learning_rate": 1e-05,
      "loss": 0.3699,
      "step": 312
    },
    {
      "epoch": 0.4490674318507891,
      "grad_norm": 0.46347816236442996,
      "learning_rate": 1e-05,
      "loss": 0.4178,
      "step": 313
    },
    {
      "epoch": 0.45050215208034433,
      "grad_norm": 0.4619669019165837,
      "learning_rate": 1e-05,
      "loss": 0.3907,
      "step": 314
    },
    {
      "epoch": 0.4519368723098996,
      "grad_norm": 0.48608083324083823,
      "learning_rate": 1e-05,
      "loss": 0.4231,
      "step": 315
    },
    {
      "epoch": 0.4533715925394548,
      "grad_norm": 0.45165536408092277,
      "learning_rate": 1e-05,
      "loss": 0.4425,
      "step": 316
    },
    {
      "epoch": 0.45480631276901007,
      "grad_norm": 0.45694125731678065,
      "learning_rate": 1e-05,
      "loss": 0.4111,
      "step": 317
    },
    {
      "epoch": 0.4562410329985653,
      "grad_norm": 0.3999875362287056,
      "learning_rate": 1e-05,
      "loss": 0.3884,
      "step": 318
    },
    {
      "epoch": 0.45767575322812054,
      "grad_norm": 0.422657813618789,
      "learning_rate": 1e-05,
      "loss": 0.4169,
      "step": 319
    },
    {
      "epoch": 0.45911047345767575,
      "grad_norm": 0.4127093884380299,
      "learning_rate": 1e-05,
      "loss": 0.4249,
      "step": 320
    },
    {
      "epoch": 0.460545193687231,
      "grad_norm": 0.47029014974235556,
      "learning_rate": 1e-05,
      "loss": 0.3833,
      "step": 321
    },
    {
      "epoch": 0.4619799139167862,
      "grad_norm": 0.4616170785066939,
      "learning_rate": 1e-05,
      "loss": 0.4063,
      "step": 322
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 0.49449014470460567,
      "learning_rate": 1e-05,
      "loss": 0.4114,
      "step": 323
    },
    {
      "epoch": 0.4648493543758967,
      "grad_norm": 0.4565982861740676,
      "learning_rate": 1e-05,
      "loss": 0.4191,
      "step": 324
    },
    {
      "epoch": 0.46628407460545196,
      "grad_norm": 0.48386673119950213,
      "learning_rate": 1e-05,
      "loss": 0.4084,
      "step": 325
    },
    {
      "epoch": 0.46771879483500717,
      "grad_norm": 0.45074058483681034,
      "learning_rate": 1e-05,
      "loss": 0.4187,
      "step": 326
    },
    {
      "epoch": 0.46915351506456243,
      "grad_norm": 0.42451539574817404,
      "learning_rate": 1e-05,
      "loss": 0.3722,
      "step": 327
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 0.5620335201270614,
      "learning_rate": 1e-05,
      "loss": 0.4473,
      "step": 328
    },
    {
      "epoch": 0.4720229555236729,
      "grad_norm": 0.5217386497156461,
      "learning_rate": 1e-05,
      "loss": 0.4014,
      "step": 329
    },
    {
      "epoch": 0.4734576757532281,
      "grad_norm": 0.40652882419669295,
      "learning_rate": 1e-05,
      "loss": 0.4002,
      "step": 330
    },
    {
      "epoch": 0.4748923959827834,
      "grad_norm": 0.44499143253458073,
      "learning_rate": 1e-05,
      "loss": 0.407,
      "step": 331
    },
    {
      "epoch": 0.4763271162123386,
      "grad_norm": 0.491520491497045,
      "learning_rate": 1e-05,
      "loss": 0.4208,
      "step": 332
    },
    {
      "epoch": 0.47776183644189385,
      "grad_norm": 0.42044187839530767,
      "learning_rate": 1e-05,
      "loss": 0.4238,
      "step": 333
    },
    {
      "epoch": 0.47919655667144906,
      "grad_norm": 0.5040010577851817,
      "learning_rate": 1e-05,
      "loss": 0.3978,
      "step": 334
    },
    {
      "epoch": 0.4806312769010043,
      "grad_norm": 0.47151168470313065,
      "learning_rate": 1e-05,
      "loss": 0.3991,
      "step": 335
    },
    {
      "epoch": 0.48206599713055953,
      "grad_norm": 0.5124239678739015,
      "learning_rate": 1e-05,
      "loss": 0.452,
      "step": 336
    },
    {
      "epoch": 0.4835007173601148,
      "grad_norm": 0.5079029283679656,
      "learning_rate": 1e-05,
      "loss": 0.4482,
      "step": 337
    },
    {
      "epoch": 0.48493543758967,
      "grad_norm": 0.5687531183122966,
      "learning_rate": 1e-05,
      "loss": 0.4004,
      "step": 338
    },
    {
      "epoch": 0.48637015781922527,
      "grad_norm": 0.4226604674515071,
      "learning_rate": 1e-05,
      "loss": 0.3753,
      "step": 339
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 0.4927441807275704,
      "learning_rate": 1e-05,
      "loss": 0.3962,
      "step": 340
    },
    {
      "epoch": 0.48923959827833574,
      "grad_norm": 0.44562943739989147,
      "learning_rate": 1e-05,
      "loss": 0.3857,
      "step": 341
    },
    {
      "epoch": 0.49067431850789095,
      "grad_norm": 0.4953269059749673,
      "learning_rate": 1e-05,
      "loss": 0.4191,
      "step": 342
    },
    {
      "epoch": 0.4921090387374462,
      "grad_norm": 0.3803446739398316,
      "learning_rate": 1e-05,
      "loss": 0.3912,
      "step": 343
    },
    {
      "epoch": 0.4935437589670014,
      "grad_norm": 0.45047576578065707,
      "learning_rate": 1e-05,
      "loss": 0.4207,
      "step": 344
    },
    {
      "epoch": 0.4949784791965567,
      "grad_norm": 0.402023995074013,
      "learning_rate": 1e-05,
      "loss": 0.3759,
      "step": 345
    },
    {
      "epoch": 0.4964131994261119,
      "grad_norm": 0.4034115530453222,
      "learning_rate": 1e-05,
      "loss": 0.4061,
      "step": 346
    },
    {
      "epoch": 0.49784791965566716,
      "grad_norm": 0.45302946165698427,
      "learning_rate": 1e-05,
      "loss": 0.4437,
      "step": 347
    },
    {
      "epoch": 0.49928263988522237,
      "grad_norm": 0.4534334233403243,
      "learning_rate": 1e-05,
      "loss": 0.4116,
      "step": 348
    },
    {
      "epoch": 0.5007173601147776,
      "grad_norm": 0.4299007367621949,
      "learning_rate": 1e-05,
      "loss": 0.4005,
      "step": 349
    },
    {
      "epoch": 0.5021520803443329,
      "grad_norm": 0.48216943195494266,
      "learning_rate": 1e-05,
      "loss": 0.3987,
      "step": 350
    },
    {
      "epoch": 0.503586800573888,
      "grad_norm": 0.46403335629360654,
      "learning_rate": 1e-05,
      "loss": 0.3958,
      "step": 351
    },
    {
      "epoch": 0.5050215208034433,
      "grad_norm": 0.4524355786223171,
      "learning_rate": 1e-05,
      "loss": 0.4292,
      "step": 352
    },
    {
      "epoch": 0.5064562410329986,
      "grad_norm": 0.4850733828566083,
      "learning_rate": 1e-05,
      "loss": 0.4062,
      "step": 353
    },
    {
      "epoch": 0.5078909612625538,
      "grad_norm": 0.44888523526598945,
      "learning_rate": 1e-05,
      "loss": 0.3905,
      "step": 354
    },
    {
      "epoch": 0.509325681492109,
      "grad_norm": 0.42369273612657365,
      "learning_rate": 1e-05,
      "loss": 0.4118,
      "step": 355
    },
    {
      "epoch": 0.5107604017216643,
      "grad_norm": 0.5058800106186901,
      "learning_rate": 1e-05,
      "loss": 0.4259,
      "step": 356
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 0.40563791453497045,
      "learning_rate": 1e-05,
      "loss": 0.3739,
      "step": 357
    },
    {
      "epoch": 0.5136298421807748,
      "grad_norm": 0.38140334460475894,
      "learning_rate": 1e-05,
      "loss": 0.4167,
      "step": 358
    },
    {
      "epoch": 0.5150645624103299,
      "grad_norm": 0.4986482634994162,
      "learning_rate": 1e-05,
      "loss": 0.4322,
      "step": 359
    },
    {
      "epoch": 0.5164992826398852,
      "grad_norm": 0.4665376157462266,
      "learning_rate": 1e-05,
      "loss": 0.4158,
      "step": 360
    },
    {
      "epoch": 0.5179340028694405,
      "grad_norm": 0.5044026428225264,
      "learning_rate": 1e-05,
      "loss": 0.3974,
      "step": 361
    },
    {
      "epoch": 0.5193687230989957,
      "grad_norm": 0.45742106441059677,
      "learning_rate": 1e-05,
      "loss": 0.3837,
      "step": 362
    },
    {
      "epoch": 0.5208034433285509,
      "grad_norm": 0.4093888587751053,
      "learning_rate": 1e-05,
      "loss": 0.4212,
      "step": 363
    },
    {
      "epoch": 0.5222381635581061,
      "grad_norm": 0.4674378025464707,
      "learning_rate": 1e-05,
      "loss": 0.3624,
      "step": 364
    },
    {
      "epoch": 0.5236728837876614,
      "grad_norm": 0.5085230891644068,
      "learning_rate": 1e-05,
      "loss": 0.4247,
      "step": 365
    },
    {
      "epoch": 0.5251076040172167,
      "grad_norm": 0.3746504452150177,
      "learning_rate": 1e-05,
      "loss": 0.3866,
      "step": 366
    },
    {
      "epoch": 0.5265423242467718,
      "grad_norm": 0.4686386811687544,
      "learning_rate": 1e-05,
      "loss": 0.4002,
      "step": 367
    },
    {
      "epoch": 0.5279770444763271,
      "grad_norm": 0.48783905578530007,
      "learning_rate": 1e-05,
      "loss": 0.4337,
      "step": 368
    },
    {
      "epoch": 0.5294117647058824,
      "grad_norm": 0.48981968629945005,
      "learning_rate": 1e-05,
      "loss": 0.4053,
      "step": 369
    },
    {
      "epoch": 0.5308464849354376,
      "grad_norm": 0.4247236017903235,
      "learning_rate": 1e-05,
      "loss": 0.3687,
      "step": 370
    },
    {
      "epoch": 0.5322812051649928,
      "grad_norm": 0.5381481621882835,
      "learning_rate": 1e-05,
      "loss": 0.4134,
      "step": 371
    },
    {
      "epoch": 0.533715925394548,
      "grad_norm": 0.4188212124115993,
      "learning_rate": 1e-05,
      "loss": 0.4111,
      "step": 372
    },
    {
      "epoch": 0.5351506456241033,
      "grad_norm": 0.47271492839724827,
      "learning_rate": 1e-05,
      "loss": 0.372,
      "step": 373
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 0.5122205939982953,
      "learning_rate": 1e-05,
      "loss": 0.4386,
      "step": 374
    },
    {
      "epoch": 0.5380200860832137,
      "grad_norm": 0.41873346024623986,
      "learning_rate": 1e-05,
      "loss": 0.4238,
      "step": 375
    },
    {
      "epoch": 0.539454806312769,
      "grad_norm": 0.4294139697519751,
      "learning_rate": 1e-05,
      "loss": 0.4038,
      "step": 376
    },
    {
      "epoch": 0.5408895265423243,
      "grad_norm": 0.4274304826765745,
      "learning_rate": 1e-05,
      "loss": 0.4001,
      "step": 377
    },
    {
      "epoch": 0.5423242467718795,
      "grad_norm": 0.5488084032885013,
      "learning_rate": 1e-05,
      "loss": 0.4042,
      "step": 378
    },
    {
      "epoch": 0.5437589670014347,
      "grad_norm": 0.44812296013904784,
      "learning_rate": 1e-05,
      "loss": 0.4125,
      "step": 379
    },
    {
      "epoch": 0.5451936872309899,
      "grad_norm": 0.4265994979516801,
      "learning_rate": 1e-05,
      "loss": 0.4244,
      "step": 380
    },
    {
      "epoch": 0.5466284074605452,
      "grad_norm": 0.4336287271688747,
      "learning_rate": 1e-05,
      "loss": 0.4206,
      "step": 381
    },
    {
      "epoch": 0.5480631276901005,
      "grad_norm": 0.5059633687142283,
      "learning_rate": 1e-05,
      "loss": 0.3977,
      "step": 382
    },
    {
      "epoch": 0.5494978479196556,
      "grad_norm": 0.47945463145293976,
      "learning_rate": 1e-05,
      "loss": 0.4302,
      "step": 383
    },
    {
      "epoch": 0.5509325681492109,
      "grad_norm": 0.4572969621170344,
      "learning_rate": 1e-05,
      "loss": 0.4156,
      "step": 384
    },
    {
      "epoch": 0.5523672883787661,
      "grad_norm": 0.5083758365489516,
      "learning_rate": 1e-05,
      "loss": 0.4493,
      "step": 385
    },
    {
      "epoch": 0.5538020086083214,
      "grad_norm": 0.4824075701691557,
      "learning_rate": 1e-05,
      "loss": 0.4378,
      "step": 386
    },
    {
      "epoch": 0.5552367288378766,
      "grad_norm": 0.47748625434609965,
      "learning_rate": 1e-05,
      "loss": 0.4087,
      "step": 387
    },
    {
      "epoch": 0.5566714490674318,
      "grad_norm": 0.43232879627915327,
      "learning_rate": 1e-05,
      "loss": 0.33,
      "step": 388
    },
    {
      "epoch": 0.5581061692969871,
      "grad_norm": 0.417973902192956,
      "learning_rate": 1e-05,
      "loss": 0.4466,
      "step": 389
    },
    {
      "epoch": 0.5595408895265424,
      "grad_norm": 0.4181584202349778,
      "learning_rate": 1e-05,
      "loss": 0.4276,
      "step": 390
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 0.46184007824708995,
      "learning_rate": 1e-05,
      "loss": 0.4195,
      "step": 391
    },
    {
      "epoch": 0.5624103299856528,
      "grad_norm": 0.47834418819585633,
      "learning_rate": 1e-05,
      "loss": 0.4088,
      "step": 392
    },
    {
      "epoch": 0.563845050215208,
      "grad_norm": 0.49025997467436316,
      "learning_rate": 1e-05,
      "loss": 0.3822,
      "step": 393
    },
    {
      "epoch": 0.5652797704447633,
      "grad_norm": 0.43027838951784453,
      "learning_rate": 1e-05,
      "loss": 0.396,
      "step": 394
    },
    {
      "epoch": 0.5667144906743186,
      "grad_norm": 0.5167264998605668,
      "learning_rate": 1e-05,
      "loss": 0.4344,
      "step": 395
    },
    {
      "epoch": 0.5681492109038737,
      "grad_norm": 0.45367592410943725,
      "learning_rate": 1e-05,
      "loss": 0.4314,
      "step": 396
    },
    {
      "epoch": 0.569583931133429,
      "grad_norm": 0.549611843646957,
      "learning_rate": 1e-05,
      "loss": 0.4526,
      "step": 397
    },
    {
      "epoch": 0.5710186513629842,
      "grad_norm": 0.4912684592187761,
      "learning_rate": 1e-05,
      "loss": 0.407,
      "step": 398
    },
    {
      "epoch": 0.5724533715925395,
      "grad_norm": 0.45640871298353275,
      "learning_rate": 1e-05,
      "loss": 0.367,
      "step": 399
    },
    {
      "epoch": 0.5738880918220947,
      "grad_norm": 0.4710294819999673,
      "learning_rate": 1e-05,
      "loss": 0.4205,
      "step": 400
    },
    {
      "epoch": 0.5753228120516499,
      "grad_norm": 0.5023157971579247,
      "learning_rate": 1e-05,
      "loss": 0.4375,
      "step": 401
    },
    {
      "epoch": 0.5767575322812052,
      "grad_norm": 0.44473291965726885,
      "learning_rate": 1e-05,
      "loss": 0.4173,
      "step": 402
    },
    {
      "epoch": 0.5781922525107605,
      "grad_norm": 0.5161837987598579,
      "learning_rate": 1e-05,
      "loss": 0.4055,
      "step": 403
    },
    {
      "epoch": 0.5796269727403156,
      "grad_norm": 0.548978265878424,
      "learning_rate": 1e-05,
      "loss": 0.4033,
      "step": 404
    },
    {
      "epoch": 0.5810616929698709,
      "grad_norm": 0.47589091702998865,
      "learning_rate": 1e-05,
      "loss": 0.4154,
      "step": 405
    },
    {
      "epoch": 0.5824964131994261,
      "grad_norm": 0.4587257208029906,
      "learning_rate": 1e-05,
      "loss": 0.4099,
      "step": 406
    },
    {
      "epoch": 0.5839311334289814,
      "grad_norm": 0.49255870515583766,
      "learning_rate": 1e-05,
      "loss": 0.4254,
      "step": 407
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 0.5057057446209638,
      "learning_rate": 1e-05,
      "loss": 0.4017,
      "step": 408
    },
    {
      "epoch": 0.5868005738880918,
      "grad_norm": 0.4907527118322417,
      "learning_rate": 1e-05,
      "loss": 0.3873,
      "step": 409
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 0.40210830608745907,
      "learning_rate": 1e-05,
      "loss": 0.3969,
      "step": 410
    },
    {
      "epoch": 0.5896700143472023,
      "grad_norm": 0.6184905379196334,
      "learning_rate": 1e-05,
      "loss": 0.4396,
      "step": 411
    },
    {
      "epoch": 0.5911047345767575,
      "grad_norm": 0.4440733145072265,
      "learning_rate": 1e-05,
      "loss": 0.4467,
      "step": 412
    },
    {
      "epoch": 0.5925394548063128,
      "grad_norm": 0.42956844689556106,
      "learning_rate": 1e-05,
      "loss": 0.4244,
      "step": 413
    },
    {
      "epoch": 0.593974175035868,
      "grad_norm": 0.4516757626455943,
      "learning_rate": 1e-05,
      "loss": 0.3937,
      "step": 414
    },
    {
      "epoch": 0.5954088952654233,
      "grad_norm": 0.4900750946356358,
      "learning_rate": 1e-05,
      "loss": 0.3967,
      "step": 415
    },
    {
      "epoch": 0.5968436154949784,
      "grad_norm": 0.4555468890817932,
      "learning_rate": 1e-05,
      "loss": 0.3807,
      "step": 416
    },
    {
      "epoch": 0.5982783357245337,
      "grad_norm": 0.4753500836368565,
      "learning_rate": 1e-05,
      "loss": 0.4002,
      "step": 417
    },
    {
      "epoch": 0.599713055954089,
      "grad_norm": 0.4796564524243045,
      "learning_rate": 1e-05,
      "loss": 0.4377,
      "step": 418
    },
    {
      "epoch": 0.6011477761836442,
      "grad_norm": 0.4928688362609378,
      "learning_rate": 1e-05,
      "loss": 0.3908,
      "step": 419
    },
    {
      "epoch": 0.6025824964131994,
      "grad_norm": 0.48240372100542783,
      "learning_rate": 1e-05,
      "loss": 0.3998,
      "step": 420
    },
    {
      "epoch": 0.6040172166427547,
      "grad_norm": 0.468969981694024,
      "learning_rate": 1e-05,
      "loss": 0.4377,
      "step": 421
    },
    {
      "epoch": 0.6054519368723099,
      "grad_norm": 0.45100529943531215,
      "learning_rate": 1e-05,
      "loss": 0.4379,
      "step": 422
    },
    {
      "epoch": 0.6068866571018652,
      "grad_norm": 0.4644963329959155,
      "learning_rate": 1e-05,
      "loss": 0.4166,
      "step": 423
    },
    {
      "epoch": 0.6083213773314203,
      "grad_norm": 0.4180771577365148,
      "learning_rate": 1e-05,
      "loss": 0.4077,
      "step": 424
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 0.4516651880128279,
      "learning_rate": 1e-05,
      "loss": 0.4543,
      "step": 425
    },
    {
      "epoch": 0.6111908177905309,
      "grad_norm": 0.4203731438553909,
      "learning_rate": 1e-05,
      "loss": 0.4117,
      "step": 426
    },
    {
      "epoch": 0.6126255380200861,
      "grad_norm": 0.45298921409092835,
      "learning_rate": 1e-05,
      "loss": 0.3982,
      "step": 427
    },
    {
      "epoch": 0.6140602582496413,
      "grad_norm": 0.4544533684685561,
      "learning_rate": 1e-05,
      "loss": 0.4019,
      "step": 428
    },
    {
      "epoch": 0.6154949784791965,
      "grad_norm": 0.42917379632532454,
      "learning_rate": 1e-05,
      "loss": 0.4182,
      "step": 429
    },
    {
      "epoch": 0.6169296987087518,
      "grad_norm": 0.5196270380049678,
      "learning_rate": 1e-05,
      "loss": 0.4281,
      "step": 430
    },
    {
      "epoch": 0.6183644189383071,
      "grad_norm": 0.4035216330565001,
      "learning_rate": 1e-05,
      "loss": 0.3867,
      "step": 431
    },
    {
      "epoch": 0.6197991391678622,
      "grad_norm": 0.36658345035474643,
      "learning_rate": 1e-05,
      "loss": 0.4023,
      "step": 432
    },
    {
      "epoch": 0.6212338593974175,
      "grad_norm": 0.4305720327919435,
      "learning_rate": 1e-05,
      "loss": 0.3589,
      "step": 433
    },
    {
      "epoch": 0.6226685796269728,
      "grad_norm": 0.4546213150973353,
      "learning_rate": 1e-05,
      "loss": 0.3923,
      "step": 434
    },
    {
      "epoch": 0.624103299856528,
      "grad_norm": 0.47191780592089105,
      "learning_rate": 1e-05,
      "loss": 0.4196,
      "step": 435
    },
    {
      "epoch": 0.6255380200860832,
      "grad_norm": 0.5232812249150954,
      "learning_rate": 1e-05,
      "loss": 0.4101,
      "step": 436
    },
    {
      "epoch": 0.6269727403156384,
      "grad_norm": 0.44331026106381094,
      "learning_rate": 1e-05,
      "loss": 0.3986,
      "step": 437
    },
    {
      "epoch": 0.6284074605451937,
      "grad_norm": 0.4341489851086659,
      "learning_rate": 1e-05,
      "loss": 0.4202,
      "step": 438
    },
    {
      "epoch": 0.629842180774749,
      "grad_norm": 0.480285148025125,
      "learning_rate": 1e-05,
      "loss": 0.4431,
      "step": 439
    },
    {
      "epoch": 0.6312769010043041,
      "grad_norm": 0.4195259117814922,
      "learning_rate": 1e-05,
      "loss": 0.3968,
      "step": 440
    },
    {
      "epoch": 0.6327116212338594,
      "grad_norm": 0.452960660233609,
      "learning_rate": 1e-05,
      "loss": 0.4355,
      "step": 441
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 0.45523882707806407,
      "learning_rate": 1e-05,
      "loss": 0.3846,
      "step": 442
    },
    {
      "epoch": 0.6355810616929699,
      "grad_norm": 0.41222192267616664,
      "learning_rate": 1e-05,
      "loss": 0.3903,
      "step": 443
    },
    {
      "epoch": 0.6370157819225251,
      "grad_norm": 0.39945928045588674,
      "learning_rate": 1e-05,
      "loss": 0.4201,
      "step": 444
    },
    {
      "epoch": 0.6384505021520803,
      "grad_norm": 0.4493644585986977,
      "learning_rate": 1e-05,
      "loss": 0.3821,
      "step": 445
    },
    {
      "epoch": 0.6398852223816356,
      "grad_norm": 0.4609358008982966,
      "learning_rate": 1e-05,
      "loss": 0.3974,
      "step": 446
    },
    {
      "epoch": 0.6413199426111909,
      "grad_norm": 0.5004580760206547,
      "learning_rate": 1e-05,
      "loss": 0.466,
      "step": 447
    },
    {
      "epoch": 0.642754662840746,
      "grad_norm": 0.4879087289988932,
      "learning_rate": 1e-05,
      "loss": 0.4156,
      "step": 448
    },
    {
      "epoch": 0.6441893830703013,
      "grad_norm": 0.4790246145070188,
      "learning_rate": 1e-05,
      "loss": 0.3814,
      "step": 449
    },
    {
      "epoch": 0.6456241032998565,
      "grad_norm": 0.44696489252577093,
      "learning_rate": 1e-05,
      "loss": 0.3884,
      "step": 450
    },
    {
      "epoch": 0.6470588235294118,
      "grad_norm": 0.44239649721644286,
      "learning_rate": 1e-05,
      "loss": 0.4083,
      "step": 451
    },
    {
      "epoch": 0.648493543758967,
      "grad_norm": 0.47715202893623293,
      "learning_rate": 1e-05,
      "loss": 0.416,
      "step": 452
    },
    {
      "epoch": 0.6499282639885222,
      "grad_norm": 0.44038302158457254,
      "learning_rate": 1e-05,
      "loss": 0.3836,
      "step": 453
    },
    {
      "epoch": 0.6513629842180775,
      "grad_norm": 0.5205655396129544,
      "learning_rate": 1e-05,
      "loss": 0.3886,
      "step": 454
    },
    {
      "epoch": 0.6527977044476327,
      "grad_norm": 0.5275244600341435,
      "learning_rate": 1e-05,
      "loss": 0.4861,
      "step": 455
    },
    {
      "epoch": 0.6542324246771879,
      "grad_norm": 0.503243383741234,
      "learning_rate": 1e-05,
      "loss": 0.4077,
      "step": 456
    },
    {
      "epoch": 0.6556671449067432,
      "grad_norm": 0.4721310133803578,
      "learning_rate": 1e-05,
      "loss": 0.381,
      "step": 457
    },
    {
      "epoch": 0.6571018651362984,
      "grad_norm": 0.47935698641911006,
      "learning_rate": 1e-05,
      "loss": 0.4158,
      "step": 458
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 0.433176729943234,
      "learning_rate": 1e-05,
      "loss": 0.446,
      "step": 459
    },
    {
      "epoch": 0.6599713055954088,
      "grad_norm": 0.559194057342768,
      "learning_rate": 1e-05,
      "loss": 0.4516,
      "step": 460
    },
    {
      "epoch": 0.6614060258249641,
      "grad_norm": 0.4382154608796257,
      "learning_rate": 1e-05,
      "loss": 0.4017,
      "step": 461
    },
    {
      "epoch": 0.6628407460545194,
      "grad_norm": 0.4863667363430051,
      "learning_rate": 1e-05,
      "loss": 0.4138,
      "step": 462
    },
    {
      "epoch": 0.6642754662840746,
      "grad_norm": 0.44858403659813206,
      "learning_rate": 1e-05,
      "loss": 0.3964,
      "step": 463
    },
    {
      "epoch": 0.6657101865136298,
      "grad_norm": 0.4423646802107015,
      "learning_rate": 1e-05,
      "loss": 0.3765,
      "step": 464
    },
    {
      "epoch": 0.667144906743185,
      "grad_norm": 0.40781209314959554,
      "learning_rate": 1e-05,
      "loss": 0.4626,
      "step": 465
    },
    {
      "epoch": 0.6685796269727403,
      "grad_norm": 0.4827386931952783,
      "learning_rate": 1e-05,
      "loss": 0.4024,
      "step": 466
    },
    {
      "epoch": 0.6700143472022956,
      "grad_norm": 0.5133150930641596,
      "learning_rate": 1e-05,
      "loss": 0.391,
      "step": 467
    },
    {
      "epoch": 0.6714490674318508,
      "grad_norm": 0.4094256559743268,
      "learning_rate": 1e-05,
      "loss": 0.3751,
      "step": 468
    },
    {
      "epoch": 0.672883787661406,
      "grad_norm": 0.44890925620263883,
      "learning_rate": 1e-05,
      "loss": 0.4036,
      "step": 469
    },
    {
      "epoch": 0.6743185078909613,
      "grad_norm": 0.4299414167095877,
      "learning_rate": 1e-05,
      "loss": 0.4379,
      "step": 470
    },
    {
      "epoch": 0.6757532281205165,
      "grad_norm": 0.5568194708262858,
      "learning_rate": 1e-05,
      "loss": 0.4692,
      "step": 471
    },
    {
      "epoch": 0.6771879483500718,
      "grad_norm": 0.3764781160879416,
      "learning_rate": 1e-05,
      "loss": 0.4278,
      "step": 472
    },
    {
      "epoch": 0.6786226685796269,
      "grad_norm": 0.49214767963224976,
      "learning_rate": 1e-05,
      "loss": 0.4616,
      "step": 473
    },
    {
      "epoch": 0.6800573888091822,
      "grad_norm": 0.4410002343638214,
      "learning_rate": 1e-05,
      "loss": 0.4069,
      "step": 474
    },
    {
      "epoch": 0.6814921090387375,
      "grad_norm": 0.4992570370072736,
      "learning_rate": 1e-05,
      "loss": 0.3955,
      "step": 475
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 0.4718251661564357,
      "learning_rate": 1e-05,
      "loss": 0.4367,
      "step": 476
    },
    {
      "epoch": 0.6843615494978479,
      "grad_norm": 0.4832344931236191,
      "learning_rate": 1e-05,
      "loss": 0.4077,
      "step": 477
    },
    {
      "epoch": 0.6857962697274032,
      "grad_norm": 0.5132250328700194,
      "learning_rate": 1e-05,
      "loss": 0.3963,
      "step": 478
    },
    {
      "epoch": 0.6872309899569584,
      "grad_norm": 0.4866433439983362,
      "learning_rate": 1e-05,
      "loss": 0.4158,
      "step": 479
    },
    {
      "epoch": 0.6886657101865137,
      "grad_norm": 0.45316943995820047,
      "learning_rate": 1e-05,
      "loss": 0.4129,
      "step": 480
    },
    {
      "epoch": 0.6901004304160688,
      "grad_norm": 0.4134601873688425,
      "learning_rate": 1e-05,
      "loss": 0.4028,
      "step": 481
    },
    {
      "epoch": 0.6915351506456241,
      "grad_norm": 0.428551669204901,
      "learning_rate": 1e-05,
      "loss": 0.4059,
      "step": 482
    },
    {
      "epoch": 0.6929698708751794,
      "grad_norm": 0.41742699698709956,
      "learning_rate": 1e-05,
      "loss": 0.4213,
      "step": 483
    },
    {
      "epoch": 0.6944045911047346,
      "grad_norm": 0.42195040691441704,
      "learning_rate": 1e-05,
      "loss": 0.4234,
      "step": 484
    },
    {
      "epoch": 0.6958393113342898,
      "grad_norm": 0.4287468107917935,
      "learning_rate": 1e-05,
      "loss": 0.4011,
      "step": 485
    },
    {
      "epoch": 0.697274031563845,
      "grad_norm": 0.4537863374326254,
      "learning_rate": 1e-05,
      "loss": 0.43,
      "step": 486
    },
    {
      "epoch": 0.6987087517934003,
      "grad_norm": 0.4919606700626312,
      "learning_rate": 1e-05,
      "loss": 0.4101,
      "step": 487
    },
    {
      "epoch": 0.7001434720229556,
      "grad_norm": 0.503663182116779,
      "learning_rate": 1e-05,
      "loss": 0.3981,
      "step": 488
    },
    {
      "epoch": 0.7015781922525107,
      "grad_norm": 0.4519638845191635,
      "learning_rate": 1e-05,
      "loss": 0.4201,
      "step": 489
    },
    {
      "epoch": 0.703012912482066,
      "grad_norm": 0.4963691718773213,
      "learning_rate": 1e-05,
      "loss": 0.4025,
      "step": 490
    },
    {
      "epoch": 0.7044476327116213,
      "grad_norm": 0.4383725090142504,
      "learning_rate": 1e-05,
      "loss": 0.3958,
      "step": 491
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 0.4758736721170288,
      "learning_rate": 1e-05,
      "loss": 0.401,
      "step": 492
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 0.4660244119591862,
      "learning_rate": 1e-05,
      "loss": 0.4038,
      "step": 493
    },
    {
      "epoch": 0.7087517934002869,
      "grad_norm": 0.44708017812773415,
      "learning_rate": 1e-05,
      "loss": 0.4543,
      "step": 494
    },
    {
      "epoch": 0.7101865136298422,
      "grad_norm": 0.4983339211500581,
      "learning_rate": 1e-05,
      "loss": 0.4271,
      "step": 495
    },
    {
      "epoch": 0.7116212338593975,
      "grad_norm": 0.43294138299820234,
      "learning_rate": 1e-05,
      "loss": 0.4333,
      "step": 496
    },
    {
      "epoch": 0.7130559540889526,
      "grad_norm": 0.40073074818705123,
      "learning_rate": 1e-05,
      "loss": 0.399,
      "step": 497
    },
    {
      "epoch": 0.7144906743185079,
      "grad_norm": 0.4262867390294312,
      "learning_rate": 1e-05,
      "loss": 0.4102,
      "step": 498
    },
    {
      "epoch": 0.7159253945480631,
      "grad_norm": 0.4163742563943079,
      "learning_rate": 1e-05,
      "loss": 0.4069,
      "step": 499
    },
    {
      "epoch": 0.7173601147776184,
      "grad_norm": 0.5574845236230931,
      "learning_rate": 1e-05,
      "loss": 0.4779,
      "step": 500
    },
    {
      "epoch": 0.7187948350071736,
      "grad_norm": 0.42273347701851,
      "learning_rate": 1e-05,
      "loss": 0.4161,
      "step": 501
    },
    {
      "epoch": 0.7202295552367288,
      "grad_norm": 0.4652832354672299,
      "learning_rate": 1e-05,
      "loss": 0.3738,
      "step": 502
    },
    {
      "epoch": 0.7216642754662841,
      "grad_norm": 0.49353235640164816,
      "learning_rate": 1e-05,
      "loss": 0.4245,
      "step": 503
    },
    {
      "epoch": 0.7230989956958394,
      "grad_norm": 0.498179935919401,
      "learning_rate": 1e-05,
      "loss": 0.4066,
      "step": 504
    },
    {
      "epoch": 0.7245337159253945,
      "grad_norm": 0.4671938261601669,
      "learning_rate": 1e-05,
      "loss": 0.4303,
      "step": 505
    },
    {
      "epoch": 0.7259684361549498,
      "grad_norm": 0.48660557959538714,
      "learning_rate": 1e-05,
      "loss": 0.3608,
      "step": 506
    },
    {
      "epoch": 0.727403156384505,
      "grad_norm": 0.48600266751657056,
      "learning_rate": 1e-05,
      "loss": 0.4462,
      "step": 507
    },
    {
      "epoch": 0.7288378766140603,
      "grad_norm": 0.5526582804316233,
      "learning_rate": 1e-05,
      "loss": 0.3936,
      "step": 508
    },
    {
      "epoch": 0.7302725968436155,
      "grad_norm": 0.44191460275746397,
      "learning_rate": 1e-05,
      "loss": 0.4167,
      "step": 509
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 0.45238885481358654,
      "learning_rate": 1e-05,
      "loss": 0.4201,
      "step": 510
    },
    {
      "epoch": 0.733142037302726,
      "grad_norm": 0.5176220660936106,
      "learning_rate": 1e-05,
      "loss": 0.4049,
      "step": 511
    },
    {
      "epoch": 0.7345767575322812,
      "grad_norm": 0.5128675328501542,
      "learning_rate": 1e-05,
      "loss": 0.4115,
      "step": 512
    },
    {
      "epoch": 0.7360114777618364,
      "grad_norm": 0.43348575072908146,
      "learning_rate": 1e-05,
      "loss": 0.3957,
      "step": 513
    },
    {
      "epoch": 0.7374461979913917,
      "grad_norm": 0.5051190926614342,
      "learning_rate": 1e-05,
      "loss": 0.4333,
      "step": 514
    },
    {
      "epoch": 0.7388809182209469,
      "grad_norm": 0.5081018669072445,
      "learning_rate": 1e-05,
      "loss": 0.4454,
      "step": 515
    },
    {
      "epoch": 0.7403156384505022,
      "grad_norm": 0.4076255556919915,
      "learning_rate": 1e-05,
      "loss": 0.4135,
      "step": 516
    },
    {
      "epoch": 0.7417503586800573,
      "grad_norm": 0.4633373913067201,
      "learning_rate": 1e-05,
      "loss": 0.4388,
      "step": 517
    },
    {
      "epoch": 0.7431850789096126,
      "grad_norm": 0.417705210190693,
      "learning_rate": 1e-05,
      "loss": 0.4063,
      "step": 518
    },
    {
      "epoch": 0.7446197991391679,
      "grad_norm": 0.40955649920583426,
      "learning_rate": 1e-05,
      "loss": 0.4487,
      "step": 519
    },
    {
      "epoch": 0.7460545193687231,
      "grad_norm": 0.47836804071772576,
      "learning_rate": 1e-05,
      "loss": 0.3939,
      "step": 520
    },
    {
      "epoch": 0.7474892395982783,
      "grad_norm": 0.49108289862285626,
      "learning_rate": 1e-05,
      "loss": 0.4004,
      "step": 521
    },
    {
      "epoch": 0.7489239598278336,
      "grad_norm": 0.47480278781134355,
      "learning_rate": 1e-05,
      "loss": 0.3918,
      "step": 522
    },
    {
      "epoch": 0.7503586800573888,
      "grad_norm": 0.44010938758379553,
      "learning_rate": 1e-05,
      "loss": 0.4269,
      "step": 523
    },
    {
      "epoch": 0.7517934002869441,
      "grad_norm": 0.46285642588577824,
      "learning_rate": 1e-05,
      "loss": 0.3917,
      "step": 524
    },
    {
      "epoch": 0.7532281205164992,
      "grad_norm": 0.5215565382495229,
      "learning_rate": 1e-05,
      "loss": 0.4194,
      "step": 525
    },
    {
      "epoch": 0.7546628407460545,
      "grad_norm": 0.4483110957964267,
      "learning_rate": 1e-05,
      "loss": 0.4191,
      "step": 526
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 0.3889793443279553,
      "learning_rate": 1e-05,
      "loss": 0.4192,
      "step": 527
    },
    {
      "epoch": 0.757532281205165,
      "grad_norm": 0.3901799583434606,
      "learning_rate": 1e-05,
      "loss": 0.3854,
      "step": 528
    },
    {
      "epoch": 0.7589670014347202,
      "grad_norm": 0.3764402339211672,
      "learning_rate": 1e-05,
      "loss": 0.3913,
      "step": 529
    },
    {
      "epoch": 0.7604017216642754,
      "grad_norm": 0.40856108768377714,
      "learning_rate": 1e-05,
      "loss": 0.3834,
      "step": 530
    },
    {
      "epoch": 0.7618364418938307,
      "grad_norm": 0.4433267181855269,
      "learning_rate": 1e-05,
      "loss": 0.3951,
      "step": 531
    },
    {
      "epoch": 0.763271162123386,
      "grad_norm": 0.5139638848677895,
      "learning_rate": 1e-05,
      "loss": 0.3854,
      "step": 532
    },
    {
      "epoch": 0.7647058823529411,
      "grad_norm": 0.45716528177493637,
      "learning_rate": 1e-05,
      "loss": 0.3802,
      "step": 533
    },
    {
      "epoch": 0.7661406025824964,
      "grad_norm": 0.539958139922035,
      "learning_rate": 1e-05,
      "loss": 0.4223,
      "step": 534
    },
    {
      "epoch": 0.7675753228120517,
      "grad_norm": 0.48519899167210395,
      "learning_rate": 1e-05,
      "loss": 0.4356,
      "step": 535
    },
    {
      "epoch": 0.7690100430416069,
      "grad_norm": 0.5363452663916428,
      "learning_rate": 1e-05,
      "loss": 0.3986,
      "step": 536
    },
    {
      "epoch": 0.7704447632711621,
      "grad_norm": 0.46338071537952064,
      "learning_rate": 1e-05,
      "loss": 0.3832,
      "step": 537
    },
    {
      "epoch": 0.7718794835007173,
      "grad_norm": 0.47786974109202934,
      "learning_rate": 1e-05,
      "loss": 0.421,
      "step": 538
    },
    {
      "epoch": 0.7733142037302726,
      "grad_norm": 0.46101236201783713,
      "learning_rate": 1e-05,
      "loss": 0.4145,
      "step": 539
    },
    {
      "epoch": 0.7747489239598279,
      "grad_norm": 0.5322649184993891,
      "learning_rate": 1e-05,
      "loss": 0.3888,
      "step": 540
    },
    {
      "epoch": 0.776183644189383,
      "grad_norm": 0.46492671469950475,
      "learning_rate": 1e-05,
      "loss": 0.383,
      "step": 541
    },
    {
      "epoch": 0.7776183644189383,
      "grad_norm": 0.4834803100736878,
      "learning_rate": 1e-05,
      "loss": 0.4296,
      "step": 542
    },
    {
      "epoch": 0.7790530846484935,
      "grad_norm": 0.42620819890692074,
      "learning_rate": 1e-05,
      "loss": 0.3904,
      "step": 543
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 0.4811257982299738,
      "learning_rate": 1e-05,
      "loss": 0.4135,
      "step": 544
    },
    {
      "epoch": 0.7819225251076041,
      "grad_norm": 0.45285885723082525,
      "learning_rate": 1e-05,
      "loss": 0.4213,
      "step": 545
    },
    {
      "epoch": 0.7833572453371592,
      "grad_norm": 0.46457119368595456,
      "learning_rate": 1e-05,
      "loss": 0.3654,
      "step": 546
    },
    {
      "epoch": 0.7847919655667145,
      "grad_norm": 0.4219807098369039,
      "learning_rate": 1e-05,
      "loss": 0.4024,
      "step": 547
    },
    {
      "epoch": 0.7862266857962698,
      "grad_norm": 0.3774619046552146,
      "learning_rate": 1e-05,
      "loss": 0.3819,
      "step": 548
    },
    {
      "epoch": 0.787661406025825,
      "grad_norm": 0.36950346489726604,
      "learning_rate": 1e-05,
      "loss": 0.4143,
      "step": 549
    },
    {
      "epoch": 0.7890961262553802,
      "grad_norm": 0.41936029648565465,
      "learning_rate": 1e-05,
      "loss": 0.395,
      "step": 550
    },
    {
      "epoch": 0.7905308464849354,
      "grad_norm": 0.4587741823595063,
      "learning_rate": 1e-05,
      "loss": 0.4199,
      "step": 551
    },
    {
      "epoch": 0.7919655667144907,
      "grad_norm": 0.5270524953137835,
      "learning_rate": 1e-05,
      "loss": 0.4011,
      "step": 552
    },
    {
      "epoch": 0.793400286944046,
      "grad_norm": 0.41994350393487967,
      "learning_rate": 1e-05,
      "loss": 0.4072,
      "step": 553
    },
    {
      "epoch": 0.7948350071736011,
      "grad_norm": 0.4390879948547963,
      "learning_rate": 1e-05,
      "loss": 0.4043,
      "step": 554
    },
    {
      "epoch": 0.7962697274031564,
      "grad_norm": 0.4971959851895582,
      "learning_rate": 1e-05,
      "loss": 0.4101,
      "step": 555
    },
    {
      "epoch": 0.7977044476327116,
      "grad_norm": 0.5204663784172473,
      "learning_rate": 1e-05,
      "loss": 0.4832,
      "step": 556
    },
    {
      "epoch": 0.7991391678622669,
      "grad_norm": 0.4426903950501249,
      "learning_rate": 1e-05,
      "loss": 0.4267,
      "step": 557
    },
    {
      "epoch": 0.8005738880918221,
      "grad_norm": 0.4727551197004978,
      "learning_rate": 1e-05,
      "loss": 0.4302,
      "step": 558
    },
    {
      "epoch": 0.8020086083213773,
      "grad_norm": 0.4632728908024517,
      "learning_rate": 1e-05,
      "loss": 0.4007,
      "step": 559
    },
    {
      "epoch": 0.8034433285509326,
      "grad_norm": 0.399159566007681,
      "learning_rate": 1e-05,
      "loss": 0.4423,
      "step": 560
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 0.43655403604622695,
      "learning_rate": 1e-05,
      "loss": 0.4089,
      "step": 561
    },
    {
      "epoch": 0.806312769010043,
      "grad_norm": 0.4849680720608781,
      "learning_rate": 1e-05,
      "loss": 0.4411,
      "step": 562
    },
    {
      "epoch": 0.8077474892395983,
      "grad_norm": 0.4887764155885806,
      "learning_rate": 1e-05,
      "loss": 0.4276,
      "step": 563
    },
    {
      "epoch": 0.8091822094691535,
      "grad_norm": 0.4426384368605043,
      "learning_rate": 1e-05,
      "loss": 0.3715,
      "step": 564
    },
    {
      "epoch": 0.8106169296987088,
      "grad_norm": 0.41924944135162806,
      "learning_rate": 1e-05,
      "loss": 0.4283,
      "step": 565
    },
    {
      "epoch": 0.812051649928264,
      "grad_norm": 0.43754662940587596,
      "learning_rate": 1e-05,
      "loss": 0.3953,
      "step": 566
    },
    {
      "epoch": 0.8134863701578192,
      "grad_norm": 0.39640099737955925,
      "learning_rate": 1e-05,
      "loss": 0.4233,
      "step": 567
    },
    {
      "epoch": 0.8149210903873745,
      "grad_norm": 0.44835448351722146,
      "learning_rate": 1e-05,
      "loss": 0.4032,
      "step": 568
    },
    {
      "epoch": 0.8163558106169297,
      "grad_norm": 0.49391512527041287,
      "learning_rate": 1e-05,
      "loss": 0.3869,
      "step": 569
    },
    {
      "epoch": 0.8177905308464849,
      "grad_norm": 0.43934069276411886,
      "learning_rate": 1e-05,
      "loss": 0.3881,
      "step": 570
    },
    {
      "epoch": 0.8192252510760402,
      "grad_norm": 0.5296126294214349,
      "learning_rate": 1e-05,
      "loss": 0.4559,
      "step": 571
    },
    {
      "epoch": 0.8206599713055954,
      "grad_norm": 0.4352046247715399,
      "learning_rate": 1e-05,
      "loss": 0.4301,
      "step": 572
    },
    {
      "epoch": 0.8220946915351507,
      "grad_norm": 0.4427053492676585,
      "learning_rate": 1e-05,
      "loss": 0.4488,
      "step": 573
    },
    {
      "epoch": 0.8235294117647058,
      "grad_norm": 0.4470272427932781,
      "learning_rate": 1e-05,
      "loss": 0.3726,
      "step": 574
    },
    {
      "epoch": 0.8249641319942611,
      "grad_norm": 0.45173855729427265,
      "learning_rate": 1e-05,
      "loss": 0.4143,
      "step": 575
    },
    {
      "epoch": 0.8263988522238164,
      "grad_norm": 0.5057783619090152,
      "learning_rate": 1e-05,
      "loss": 0.424,
      "step": 576
    },
    {
      "epoch": 0.8278335724533716,
      "grad_norm": 0.4443654206819271,
      "learning_rate": 1e-05,
      "loss": 0.3966,
      "step": 577
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 0.4190121591660691,
      "learning_rate": 1e-05,
      "loss": 0.4093,
      "step": 578
    },
    {
      "epoch": 0.830703012912482,
      "grad_norm": 0.4350114609647533,
      "learning_rate": 1e-05,
      "loss": 0.4148,
      "step": 579
    },
    {
      "epoch": 0.8321377331420373,
      "grad_norm": 0.45940929770061106,
      "learning_rate": 1e-05,
      "loss": 0.4,
      "step": 580
    },
    {
      "epoch": 0.8335724533715926,
      "grad_norm": 0.45931852781280824,
      "learning_rate": 1e-05,
      "loss": 0.4023,
      "step": 581
    },
    {
      "epoch": 0.8350071736011477,
      "grad_norm": 0.444625908836589,
      "learning_rate": 1e-05,
      "loss": 0.4123,
      "step": 582
    },
    {
      "epoch": 0.836441893830703,
      "grad_norm": 0.4072630823893848,
      "learning_rate": 1e-05,
      "loss": 0.4182,
      "step": 583
    },
    {
      "epoch": 0.8378766140602583,
      "grad_norm": 0.43873627243558355,
      "learning_rate": 1e-05,
      "loss": 0.4092,
      "step": 584
    },
    {
      "epoch": 0.8393113342898135,
      "grad_norm": 0.4044701110742508,
      "learning_rate": 1e-05,
      "loss": 0.4029,
      "step": 585
    },
    {
      "epoch": 0.8407460545193687,
      "grad_norm": 0.4229708926582644,
      "learning_rate": 1e-05,
      "loss": 0.4321,
      "step": 586
    },
    {
      "epoch": 0.8421807747489239,
      "grad_norm": 0.49823605473898797,
      "learning_rate": 1e-05,
      "loss": 0.4223,
      "step": 587
    },
    {
      "epoch": 0.8436154949784792,
      "grad_norm": 0.4444404135698383,
      "learning_rate": 1e-05,
      "loss": 0.3513,
      "step": 588
    },
    {
      "epoch": 0.8450502152080345,
      "grad_norm": 0.4588565889624845,
      "learning_rate": 1e-05,
      "loss": 0.4326,
      "step": 589
    },
    {
      "epoch": 0.8464849354375896,
      "grad_norm": 0.49382017520006033,
      "learning_rate": 1e-05,
      "loss": 0.3921,
      "step": 590
    },
    {
      "epoch": 0.8479196556671449,
      "grad_norm": 0.4397267746309617,
      "learning_rate": 1e-05,
      "loss": 0.3953,
      "step": 591
    },
    {
      "epoch": 0.8493543758967002,
      "grad_norm": 0.49126932340768875,
      "learning_rate": 1e-05,
      "loss": 0.4195,
      "step": 592
    },
    {
      "epoch": 0.8507890961262554,
      "grad_norm": 0.4438168220428814,
      "learning_rate": 1e-05,
      "loss": 0.4056,
      "step": 593
    },
    {
      "epoch": 0.8522238163558106,
      "grad_norm": 0.44745173371645636,
      "learning_rate": 1e-05,
      "loss": 0.41,
      "step": 594
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 0.4533205721000295,
      "learning_rate": 1e-05,
      "loss": 0.3873,
      "step": 595
    },
    {
      "epoch": 0.8550932568149211,
      "grad_norm": 0.4623808055084583,
      "learning_rate": 1e-05,
      "loss": 0.3952,
      "step": 596
    },
    {
      "epoch": 0.8565279770444764,
      "grad_norm": 0.38584845532026274,
      "learning_rate": 1e-05,
      "loss": 0.3994,
      "step": 597
    },
    {
      "epoch": 0.8579626972740315,
      "grad_norm": 0.450464941087315,
      "learning_rate": 1e-05,
      "loss": 0.401,
      "step": 598
    },
    {
      "epoch": 0.8593974175035868,
      "grad_norm": 0.46314531667386794,
      "learning_rate": 1e-05,
      "loss": 0.4285,
      "step": 599
    },
    {
      "epoch": 0.860832137733142,
      "grad_norm": 0.42510029138559324,
      "learning_rate": 1e-05,
      "loss": 0.4111,
      "step": 600
    },
    {
      "epoch": 0.8622668579626973,
      "grad_norm": 0.40549235486100593,
      "learning_rate": 1e-05,
      "loss": 0.4237,
      "step": 601
    },
    {
      "epoch": 0.8637015781922525,
      "grad_norm": 0.5045894766608194,
      "learning_rate": 1e-05,
      "loss": 0.3927,
      "step": 602
    },
    {
      "epoch": 0.8651362984218077,
      "grad_norm": 0.4339678642520226,
      "learning_rate": 1e-05,
      "loss": 0.3883,
      "step": 603
    },
    {
      "epoch": 0.866571018651363,
      "grad_norm": 0.4655690057293677,
      "learning_rate": 1e-05,
      "loss": 0.4399,
      "step": 604
    },
    {
      "epoch": 0.8680057388809183,
      "grad_norm": 0.4807138848805581,
      "learning_rate": 1e-05,
      "loss": 0.4099,
      "step": 605
    },
    {
      "epoch": 0.8694404591104734,
      "grad_norm": 0.49873954239974605,
      "learning_rate": 1e-05,
      "loss": 0.3889,
      "step": 606
    },
    {
      "epoch": 0.8708751793400287,
      "grad_norm": 0.4506640783818462,
      "learning_rate": 1e-05,
      "loss": 0.3897,
      "step": 607
    },
    {
      "epoch": 0.8723098995695839,
      "grad_norm": 0.44977004355047695,
      "learning_rate": 1e-05,
      "loss": 0.389,
      "step": 608
    },
    {
      "epoch": 0.8737446197991392,
      "grad_norm": 0.48155461978007735,
      "learning_rate": 1e-05,
      "loss": 0.431,
      "step": 609
    },
    {
      "epoch": 0.8751793400286944,
      "grad_norm": 0.4904193226327705,
      "learning_rate": 1e-05,
      "loss": 0.4409,
      "step": 610
    },
    {
      "epoch": 0.8766140602582496,
      "grad_norm": 0.4755024973640959,
      "learning_rate": 1e-05,
      "loss": 0.4357,
      "step": 611
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.4629194990370138,
      "learning_rate": 1e-05,
      "loss": 0.3948,
      "step": 612
    },
    {
      "epoch": 0.8794835007173601,
      "grad_norm": 0.4973241949752734,
      "learning_rate": 1e-05,
      "loss": 0.4706,
      "step": 613
    },
    {
      "epoch": 0.8809182209469153,
      "grad_norm": 0.5226056144068053,
      "learning_rate": 1e-05,
      "loss": 0.3923,
      "step": 614
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 0.4656712721029848,
      "learning_rate": 1e-05,
      "loss": 0.3891,
      "step": 615
    },
    {
      "epoch": 0.8837876614060258,
      "grad_norm": 0.47498446464987193,
      "learning_rate": 1e-05,
      "loss": 0.3886,
      "step": 616
    },
    {
      "epoch": 0.8852223816355811,
      "grad_norm": 0.49248246699187764,
      "learning_rate": 1e-05,
      "loss": 0.4223,
      "step": 617
    },
    {
      "epoch": 0.8866571018651362,
      "grad_norm": 0.51653186936775,
      "learning_rate": 1e-05,
      "loss": 0.4249,
      "step": 618
    },
    {
      "epoch": 0.8880918220946915,
      "grad_norm": 0.4662624577101061,
      "learning_rate": 1e-05,
      "loss": 0.4217,
      "step": 619
    },
    {
      "epoch": 0.8895265423242468,
      "grad_norm": 0.4610910095074605,
      "learning_rate": 1e-05,
      "loss": 0.3971,
      "step": 620
    },
    {
      "epoch": 0.890961262553802,
      "grad_norm": 0.460138007038316,
      "learning_rate": 1e-05,
      "loss": 0.3892,
      "step": 621
    },
    {
      "epoch": 0.8923959827833573,
      "grad_norm": 0.46021695229980814,
      "learning_rate": 1e-05,
      "loss": 0.4136,
      "step": 622
    },
    {
      "epoch": 0.8938307030129125,
      "grad_norm": 0.45158951421590005,
      "learning_rate": 1e-05,
      "loss": 0.4456,
      "step": 623
    },
    {
      "epoch": 0.8952654232424677,
      "grad_norm": 0.4814017697946009,
      "learning_rate": 1e-05,
      "loss": 0.4357,
      "step": 624
    },
    {
      "epoch": 0.896700143472023,
      "grad_norm": 0.4523276889272581,
      "learning_rate": 1e-05,
      "loss": 0.3701,
      "step": 625
    },
    {
      "epoch": 0.8981348637015782,
      "grad_norm": 0.47764914299818,
      "learning_rate": 1e-05,
      "loss": 0.4034,
      "step": 626
    },
    {
      "epoch": 0.8995695839311334,
      "grad_norm": 0.42273083435469927,
      "learning_rate": 1e-05,
      "loss": 0.4252,
      "step": 627
    },
    {
      "epoch": 0.9010043041606887,
      "grad_norm": 0.4327799003986909,
      "learning_rate": 1e-05,
      "loss": 0.4313,
      "step": 628
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 0.48532863478231103,
      "learning_rate": 1e-05,
      "loss": 0.4277,
      "step": 629
    },
    {
      "epoch": 0.9038737446197992,
      "grad_norm": 0.40716628636634955,
      "learning_rate": 1e-05,
      "loss": 0.4236,
      "step": 630
    },
    {
      "epoch": 0.9053084648493543,
      "grad_norm": 0.5257900996797849,
      "learning_rate": 1e-05,
      "loss": 0.4297,
      "step": 631
    },
    {
      "epoch": 0.9067431850789096,
      "grad_norm": 0.4140778432419935,
      "learning_rate": 1e-05,
      "loss": 0.4085,
      "step": 632
    },
    {
      "epoch": 0.9081779053084649,
      "grad_norm": 0.45498696159033014,
      "learning_rate": 1e-05,
      "loss": 0.3691,
      "step": 633
    },
    {
      "epoch": 0.9096126255380201,
      "grad_norm": 0.4067864441091805,
      "learning_rate": 1e-05,
      "loss": 0.3716,
      "step": 634
    },
    {
      "epoch": 0.9110473457675753,
      "grad_norm": 0.4107838871136827,
      "learning_rate": 1e-05,
      "loss": 0.3951,
      "step": 635
    },
    {
      "epoch": 0.9124820659971306,
      "grad_norm": 0.49392708335471175,
      "learning_rate": 1e-05,
      "loss": 0.4301,
      "step": 636
    },
    {
      "epoch": 0.9139167862266858,
      "grad_norm": 0.4516241342062836,
      "learning_rate": 1e-05,
      "loss": 0.4217,
      "step": 637
    },
    {
      "epoch": 0.9153515064562411,
      "grad_norm": 0.42965193165668625,
      "learning_rate": 1e-05,
      "loss": 0.43,
      "step": 638
    },
    {
      "epoch": 0.9167862266857962,
      "grad_norm": 0.4450501083017864,
      "learning_rate": 1e-05,
      "loss": 0.3736,
      "step": 639
    },
    {
      "epoch": 0.9182209469153515,
      "grad_norm": 0.405927078350166,
      "learning_rate": 1e-05,
      "loss": 0.4064,
      "step": 640
    },
    {
      "epoch": 0.9196556671449068,
      "grad_norm": 0.43544823505159236,
      "learning_rate": 1e-05,
      "loss": 0.4328,
      "step": 641
    },
    {
      "epoch": 0.921090387374462,
      "grad_norm": 0.4762294556005275,
      "learning_rate": 1e-05,
      "loss": 0.385,
      "step": 642
    },
    {
      "epoch": 0.9225251076040172,
      "grad_norm": 0.4883522697353608,
      "learning_rate": 1e-05,
      "loss": 0.4189,
      "step": 643
    },
    {
      "epoch": 0.9239598278335724,
      "grad_norm": 0.3634482325220786,
      "learning_rate": 1e-05,
      "loss": 0.4193,
      "step": 644
    },
    {
      "epoch": 0.9253945480631277,
      "grad_norm": 0.48162547747343587,
      "learning_rate": 1e-05,
      "loss": 0.3887,
      "step": 645
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 0.41191223901432106,
      "learning_rate": 1e-05,
      "loss": 0.3832,
      "step": 646
    },
    {
      "epoch": 0.9282639885222381,
      "grad_norm": 0.41412984519786533,
      "learning_rate": 1e-05,
      "loss": 0.3839,
      "step": 647
    },
    {
      "epoch": 0.9296987087517934,
      "grad_norm": 0.49152158168365,
      "learning_rate": 1e-05,
      "loss": 0.3907,
      "step": 648
    },
    {
      "epoch": 0.9311334289813487,
      "grad_norm": 0.4472023854653147,
      "learning_rate": 1e-05,
      "loss": 0.4132,
      "step": 649
    },
    {
      "epoch": 0.9325681492109039,
      "grad_norm": 0.4459703709114727,
      "learning_rate": 1e-05,
      "loss": 0.4557,
      "step": 650
    },
    {
      "epoch": 0.9340028694404591,
      "grad_norm": 0.4437308401063714,
      "learning_rate": 1e-05,
      "loss": 0.3926,
      "step": 651
    },
    {
      "epoch": 0.9354375896700143,
      "grad_norm": 0.3979181135036668,
      "learning_rate": 1e-05,
      "loss": 0.3884,
      "step": 652
    },
    {
      "epoch": 0.9368723098995696,
      "grad_norm": 0.4135220195850876,
      "learning_rate": 1e-05,
      "loss": 0.4121,
      "step": 653
    },
    {
      "epoch": 0.9383070301291249,
      "grad_norm": 0.48746194394107367,
      "learning_rate": 1e-05,
      "loss": 0.4376,
      "step": 654
    },
    {
      "epoch": 0.93974175035868,
      "grad_norm": 0.4521685872489136,
      "learning_rate": 1e-05,
      "loss": 0.4017,
      "step": 655
    },
    {
      "epoch": 0.9411764705882353,
      "grad_norm": 0.45762200588795293,
      "learning_rate": 1e-05,
      "loss": 0.3874,
      "step": 656
    },
    {
      "epoch": 0.9426111908177905,
      "grad_norm": 0.456136098923667,
      "learning_rate": 1e-05,
      "loss": 0.4461,
      "step": 657
    },
    {
      "epoch": 0.9440459110473458,
      "grad_norm": 0.4617946564424204,
      "learning_rate": 1e-05,
      "loss": 0.4287,
      "step": 658
    },
    {
      "epoch": 0.945480631276901,
      "grad_norm": 0.4464892715795597,
      "learning_rate": 1e-05,
      "loss": 0.3876,
      "step": 659
    },
    {
      "epoch": 0.9469153515064562,
      "grad_norm": 0.521489083961126,
      "learning_rate": 1e-05,
      "loss": 0.4307,
      "step": 660
    },
    {
      "epoch": 0.9483500717360115,
      "grad_norm": 0.4052165330527032,
      "learning_rate": 1e-05,
      "loss": 0.3853,
      "step": 661
    },
    {
      "epoch": 0.9497847919655668,
      "grad_norm": 0.49758198755381533,
      "learning_rate": 1e-05,
      "loss": 0.4292,
      "step": 662
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 0.5126772621033614,
      "learning_rate": 1e-05,
      "loss": 0.3753,
      "step": 663
    },
    {
      "epoch": 0.9526542324246772,
      "grad_norm": 0.46350638721405557,
      "learning_rate": 1e-05,
      "loss": 0.3774,
      "step": 664
    },
    {
      "epoch": 0.9540889526542324,
      "grad_norm": 0.45907169882582033,
      "learning_rate": 1e-05,
      "loss": 0.4628,
      "step": 665
    },
    {
      "epoch": 0.9555236728837877,
      "grad_norm": 0.46305785576825076,
      "learning_rate": 1e-05,
      "loss": 0.3825,
      "step": 666
    },
    {
      "epoch": 0.9569583931133429,
      "grad_norm": 0.43863776175657126,
      "learning_rate": 1e-05,
      "loss": 0.4125,
      "step": 667
    },
    {
      "epoch": 0.9583931133428981,
      "grad_norm": 0.49311688291751377,
      "learning_rate": 1e-05,
      "loss": 0.3835,
      "step": 668
    },
    {
      "epoch": 0.9598278335724534,
      "grad_norm": 0.456934630033007,
      "learning_rate": 1e-05,
      "loss": 0.425,
      "step": 669
    },
    {
      "epoch": 0.9612625538020086,
      "grad_norm": 0.48686407597883363,
      "learning_rate": 1e-05,
      "loss": 0.4013,
      "step": 670
    },
    {
      "epoch": 0.9626972740315638,
      "grad_norm": 0.4921065626180016,
      "learning_rate": 1e-05,
      "loss": 0.3979,
      "step": 671
    },
    {
      "epoch": 0.9641319942611191,
      "grad_norm": 0.45095185552508177,
      "learning_rate": 1e-05,
      "loss": 0.3819,
      "step": 672
    },
    {
      "epoch": 0.9655667144906743,
      "grad_norm": 0.45777802417412683,
      "learning_rate": 1e-05,
      "loss": 0.4074,
      "step": 673
    },
    {
      "epoch": 0.9670014347202296,
      "grad_norm": 0.44503872526322846,
      "learning_rate": 1e-05,
      "loss": 0.3604,
      "step": 674
    },
    {
      "epoch": 0.9684361549497847,
      "grad_norm": 0.442916793539951,
      "learning_rate": 1e-05,
      "loss": 0.434,
      "step": 675
    },
    {
      "epoch": 0.96987087517934,
      "grad_norm": 0.45132263693543234,
      "learning_rate": 1e-05,
      "loss": 0.405,
      "step": 676
    },
    {
      "epoch": 0.9713055954088953,
      "grad_norm": 0.44899626667860143,
      "learning_rate": 1e-05,
      "loss": 0.3963,
      "step": 677
    },
    {
      "epoch": 0.9727403156384505,
      "grad_norm": 0.44966035987764813,
      "learning_rate": 1e-05,
      "loss": 0.3932,
      "step": 678
    },
    {
      "epoch": 0.9741750358680057,
      "grad_norm": 0.4474361189688219,
      "learning_rate": 1e-05,
      "loss": 0.4261,
      "step": 679
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.46242077430218465,
      "learning_rate": 1e-05,
      "loss": 0.4064,
      "step": 680
    },
    {
      "epoch": 0.9770444763271162,
      "grad_norm": 0.5085658902102875,
      "learning_rate": 1e-05,
      "loss": 0.4656,
      "step": 681
    },
    {
      "epoch": 0.9784791965566715,
      "grad_norm": 0.4321585312696986,
      "learning_rate": 1e-05,
      "loss": 0.4255,
      "step": 682
    },
    {
      "epoch": 0.9799139167862266,
      "grad_norm": 0.39465158801474937,
      "learning_rate": 1e-05,
      "loss": 0.3873,
      "step": 683
    },
    {
      "epoch": 0.9813486370157819,
      "grad_norm": 0.4190196301445815,
      "learning_rate": 1e-05,
      "loss": 0.4378,
      "step": 684
    },
    {
      "epoch": 0.9827833572453372,
      "grad_norm": 0.5273890825099122,
      "learning_rate": 1e-05,
      "loss": 0.3826,
      "step": 685
    },
    {
      "epoch": 0.9842180774748924,
      "grad_norm": 0.3971111232352037,
      "learning_rate": 1e-05,
      "loss": 0.4114,
      "step": 686
    },
    {
      "epoch": 0.9856527977044476,
      "grad_norm": 0.42198054805087987,
      "learning_rate": 1e-05,
      "loss": 0.3783,
      "step": 687
    },
    {
      "epoch": 0.9870875179340028,
      "grad_norm": 0.4902024300096817,
      "learning_rate": 1e-05,
      "loss": 0.3984,
      "step": 688
    },
    {
      "epoch": 0.9885222381635581,
      "grad_norm": 0.40897134032570837,
      "learning_rate": 1e-05,
      "loss": 0.3862,
      "step": 689
    },
    {
      "epoch": 0.9899569583931134,
      "grad_norm": 0.4148700644942736,
      "learning_rate": 1e-05,
      "loss": 0.4191,
      "step": 690
    },
    {
      "epoch": 0.9913916786226685,
      "grad_norm": 0.49094687286873473,
      "learning_rate": 1e-05,
      "loss": 0.4599,
      "step": 691
    },
    {
      "epoch": 0.9928263988522238,
      "grad_norm": 0.5165327629619043,
      "learning_rate": 1e-05,
      "loss": 0.4016,
      "step": 692
    },
    {
      "epoch": 0.994261119081779,
      "grad_norm": 0.4307655159698764,
      "learning_rate": 1e-05,
      "loss": 0.411,
      "step": 693
    },
    {
      "epoch": 0.9956958393113343,
      "grad_norm": 0.434282262141524,
      "learning_rate": 1e-05,
      "loss": 0.3892,
      "step": 694
    },
    {
      "epoch": 0.9971305595408895,
      "grad_norm": 0.4453756585858163,
      "learning_rate": 1e-05,
      "loss": 0.4246,
      "step": 695
    },
    {
      "epoch": 0.9985652797704447,
      "grad_norm": 0.38776739449547004,
      "learning_rate": 1e-05,
      "loss": 0.4261,
      "step": 696
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.4523217331447312,
      "learning_rate": 1e-05,
      "loss": 0.3591,
      "step": 697
    },
    {
      "epoch": 1.0,
      "step": 697,
      "total_flos": 228711057948672.0,
      "train_loss": 0.41238670258474147,
      "train_runtime": 6874.1722,
      "train_samples_per_second": 2.431,
      "train_steps_per_second": 0.101
    }
  ],
  "logging_steps": 1,
  "max_steps": 697,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 228711057948672.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
