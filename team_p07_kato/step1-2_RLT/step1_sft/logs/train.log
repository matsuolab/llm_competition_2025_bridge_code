[2025-08-15 02:25:00,711][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:00,712][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:00,727][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:00,728][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:00,762][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:00,763][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:00,848][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:00,849][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:00,900][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:00,901][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:00,927][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:00,928][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:00,940][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:00,941][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:00,950][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:00,952][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,000][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,001][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,026][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,028][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,036][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,038][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,039][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,040][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,065][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,067][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,072][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,073][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,096][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,097][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,113][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,114][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,120][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,122][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,132][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,133][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,137][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,139][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,144][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,146][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,157][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,158][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,165][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,166][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,181][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,183][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:01,189][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: teacher_sft
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: custom_bespoke_stratos_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: ${dataset_id_or_path}
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: 24
per_device_eval_batch_size: 4

[2025-08-15 02:25:01,189][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:25:02,722][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,723][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,740][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,838][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,861][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,861][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,863][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,893][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,904][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,905][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,919][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,952][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,966][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:02,986][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,018][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,025][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,048][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,049][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,074][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,087][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,097][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,182][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,184][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:03,191][__main__][INFO] - Accumulation steps 24 ----
[2025-08-15 02:25:04,365][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,365][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,365][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,367][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,367][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,367][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,367][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,516][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,516][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,516][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,516][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,516][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,516][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,517][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,539][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,539][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,540][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,542][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,543][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,543][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:25:04,543][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:12,446][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,448][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,471][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,473][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,495][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,496][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,607][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,608][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,615][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,616][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,761][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,763][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,859][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,862][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,866][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,868][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,873][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,874][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,989][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,990][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:12,991][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:12,991][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,010][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,011][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,019][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,021][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,103][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,104][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,116][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,118][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,122][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,123][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,131][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,131][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,132][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,134][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,135][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,137][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,144][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,145][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,185][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,185][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,186][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,187][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,207][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,208][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:13,214][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:39:13,215][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:39:14,018][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,437][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,451][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,455][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,458][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,512][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,518][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,520][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,522][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,623][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,631][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,649][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,658][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,696][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,741][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,745][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,753][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,755][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,767][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,773][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,774][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,805][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,815][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:14,822][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:39:15,177][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,181][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,239][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,239][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,241][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,241][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,261][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,290][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,308][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,308][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,312][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,343][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,349][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,356][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,362][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,365][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,383][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,395][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,414][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,414][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:15,432][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:17,960][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:18,531][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:18,535][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:39:28,187][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpseo_6qpx/test.c -o /tmp/tmpseo_6qpx/test.o
[2025-08-15 02:39:28,187][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpzqw_ig1b/test.c -o /tmp/tmpzqw_ig1b/test.o
[2025-08-15 02:39:28,187][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpcaqxp6ra/test.c -o /tmp/tmpcaqxp6ra/test.o
[2025-08-15 02:39:28,187][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp5qlpew4h/test.c -o /tmp/tmp5qlpew4h/test.o
[2025-08-15 02:39:28,187][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpepjczqw0/test.c -o /tmp/tmpepjczqw0/test.o
[2025-08-15 02:39:28,187][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpg0pig66x/test.c -o /tmp/tmpg0pig66x/test.o
[2025-08-15 02:39:28,187][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmps37bd2q0/test.c -o /tmp/tmps37bd2q0/test.o
[2025-08-15 02:39:28,187][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp2k7qh6pc/test.c -o /tmp/tmp2k7qh6pc/test.o
[2025-08-15 02:39:28,211][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpazhj4ah7/test.c -o /tmp/tmpazhj4ah7/test.o
[2025-08-15 02:39:28,211][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp6ok2_3mm/test.c -o /tmp/tmp6ok2_3mm/test.o
[2025-08-15 02:39:28,211][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpcjql2lsd/test.c -o /tmp/tmpcjql2lsd/test.o
[2025-08-15 02:39:28,211][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp__3go3jc/test.c -o /tmp/tmp__3go3jc/test.o
[2025-08-15 02:39:28,211][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpkyy0r204/test.c -o /tmp/tmpkyy0r204/test.o
[2025-08-15 02:39:28,211][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp_8f2wbfn/test.c -o /tmp/tmp_8f2wbfn/test.o
[2025-08-15 02:39:28,211][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpgpcqc4ht/test.c -o /tmp/tmpgpcqc4ht/test.o
[2025-08-15 02:39:28,211][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpsn839sgf/test.c -o /tmp/tmpsn839sgf/test.o
[2025-08-15 02:39:28,219][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmps37bd2q0/test.o -laio -o /tmp/tmps37bd2q0/a.out
[2025-08-15 02:39:28,219][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp5qlpew4h/test.o -laio -o /tmp/tmp5qlpew4h/a.out
[2025-08-15 02:39:28,219][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpepjczqw0/test.o -laio -o /tmp/tmpepjczqw0/a.out
[2025-08-15 02:39:28,219][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpcaqxp6ra/test.o -laio -o /tmp/tmpcaqxp6ra/a.out
[2025-08-15 02:39:28,220][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpseo_6qpx/test.o -laio -o /tmp/tmpseo_6qpx/a.out
[2025-08-15 02:39:28,220][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpg0pig66x/test.o -laio -o /tmp/tmpg0pig66x/a.out
[2025-08-15 02:39:28,220][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpzqw_ig1b/test.o -laio -o /tmp/tmpzqw_ig1b/a.out
[2025-08-15 02:39:28,221][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp2k7qh6pc/test.o -laio -o /tmp/tmp2k7qh6pc/a.out
[2025-08-15 02:39:28,236][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp6dmr5yu0/test.c -o /tmp/tmp6dmr5yu0/test.o
[2025-08-15 02:39:28,236][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpbyk99rp5/test.c -o /tmp/tmpbyk99rp5/test.o
[2025-08-15 02:39:28,237][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp9uejfz21/test.c -o /tmp/tmp9uejfz21/test.o
[2025-08-15 02:39:28,237][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp_dv577qq/test.c -o /tmp/tmp_dv577qq/test.o
[2025-08-15 02:39:28,237][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpz5yd815_/test.c -o /tmp/tmpz5yd815_/test.o
[2025-08-15 02:39:28,237][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpd2kf2tek/test.c -o /tmp/tmpd2kf2tek/test.o
[2025-08-15 02:39:28,237][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpqc4koi94/test.c -o /tmp/tmpqc4koi94/test.o
[2025-08-15 02:39:28,237][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpplevh17s/test.c -o /tmp/tmpplevh17s/test.o
[2025-08-15 02:39:28,243][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpsn839sgf/test.o -laio -o /tmp/tmpsn839sgf/a.out
[2025-08-15 02:39:28,243][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpazhj4ah7/test.o -laio -o /tmp/tmpazhj4ah7/a.out
[2025-08-15 02:39:28,243][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp6ok2_3mm/test.o -laio -o /tmp/tmp6ok2_3mm/a.out
[2025-08-15 02:39:28,244][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp_8f2wbfn/test.o -laio -o /tmp/tmp_8f2wbfn/a.out
[2025-08-15 02:39:28,244][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpcjql2lsd/test.o -laio -o /tmp/tmpcjql2lsd/a.out
[2025-08-15 02:39:28,244][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpgpcqc4ht/test.o -laio -o /tmp/tmpgpcqc4ht/a.out
[2025-08-15 02:39:28,245][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp__3go3jc/test.o -laio -o /tmp/tmp__3go3jc/a.out
[2025-08-15 02:39:28,245][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpkyy0r204/test.o -laio -o /tmp/tmpkyy0r204/a.out
[2025-08-15 02:39:28,269][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpz5yd815_/test.o -laio -o /tmp/tmpz5yd815_/a.out
[2025-08-15 02:39:28,270][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpplevh17s/test.o -laio -o /tmp/tmpplevh17s/a.out
[2025-08-15 02:39:28,270][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpd2kf2tek/test.o -laio -o /tmp/tmpd2kf2tek/a.out
[2025-08-15 02:39:28,270][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpbyk99rp5/test.o -laio -o /tmp/tmpbyk99rp5/a.out
[2025-08-15 02:39:28,271][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp6dmr5yu0/test.o -laio -o /tmp/tmp6dmr5yu0/a.out
[2025-08-15 02:39:28,271][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp_dv577qq/test.o -laio -o /tmp/tmp_dv577qq/a.out
[2025-08-15 02:39:28,271][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp9uejfz21/test.o -laio -o /tmp/tmp9uejfz21/a.out
[2025-08-15 02:39:28,271][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpqc4koi94/test.o -laio -o /tmp/tmpqc4koi94/a.out
[2025-08-15 02:39:28,676][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpcdcjbmpz/test.c -o /tmp/tmpcdcjbmpz/test.o
[2025-08-15 02:39:28,676][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp4dy2wyel/test.c -o /tmp/tmp4dy2wyel/test.o
[2025-08-15 02:39:28,676][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpc_bspi4v/test.c -o /tmp/tmpc_bspi4v/test.o
[2025-08-15 02:39:28,691][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpimvrbm68/test.c -o /tmp/tmpimvrbm68/test.o
[2025-08-15 02:39:28,696][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpc_bspi4v/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpc_bspi4v/a.out
[2025-08-15 02:39:28,696][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp4dy2wyel/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp4dy2wyel/a.out
[2025-08-15 02:39:28,696][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpcdcjbmpz/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpcdcjbmpz/a.out
[2025-08-15 02:39:28,707][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpucd3hnx6/test.c -o /tmp/tmpucd3hnx6/test.o
[2025-08-15 02:39:28,708][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpimvrbm68/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpimvrbm68/a.out
[2025-08-15 02:39:28,712][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpld20qkb7/test.c -o /tmp/tmpld20qkb7/test.o
[2025-08-15 02:39:28,725][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpucd3hnx6/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpucd3hnx6/a.out
[2025-08-15 02:39:28,736][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpld20qkb7/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpld20qkb7/a.out
[2025-08-15 02:39:28,751][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpsfsol6m0/test.c -o /tmp/tmpsfsol6m0/test.o
[2025-08-15 02:39:28,772][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpsfsol6m0/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpsfsol6m0/a.out
[2025-08-15 02:39:28,775][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp74mm_et7/test.c -o /tmp/tmp74mm_et7/test.o
[2025-08-15 02:39:28,785][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpoiw9xs8l/test.c -o /tmp/tmpoiw9xs8l/test.o
[2025-08-15 02:39:28,792][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp74mm_et7/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp74mm_et7/a.out
[2025-08-15 02:39:28,803][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpoiw9xs8l/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpoiw9xs8l/a.out
[2025-08-15 02:39:28,808][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp2hh3yytb/test.c -o /tmp/tmp2hh3yytb/test.o
[2025-08-15 02:39:28,828][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp2hh3yytb/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp2hh3yytb/a.out
[2025-08-15 02:39:28,832][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpk5fi5m7j/test.c -o /tmp/tmpk5fi5m7j/test.o
[2025-08-15 02:39:28,835][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpmfohxnrc/test.c -o /tmp/tmpmfohxnrc/test.o
[2025-08-15 02:39:28,845][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpymc2k8d1/test.c -o /tmp/tmpymc2k8d1/test.o
[2025-08-15 02:39:28,850][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpk5fi5m7j/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpk5fi5m7j/a.out
[2025-08-15 02:39:28,853][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpmfohxnrc/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpmfohxnrc/a.out
[2025-08-15 02:39:28,863][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpymc2k8d1/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpymc2k8d1/a.out
[2025-08-15 02:39:28,892][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpf2y73g7p/test.c -o /tmp/tmpf2y73g7p/test.o
[2025-08-15 02:39:28,900][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpciiph414/test.c -o /tmp/tmpciiph414/test.o
[2025-08-15 02:39:28,906][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp5fsvjwre/test.c -o /tmp/tmp5fsvjwre/test.o
[2025-08-15 02:39:28,911][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpf2y73g7p/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpf2y73g7p/a.out
[2025-08-15 02:39:28,911][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp7w92_ixm/test.c -o /tmp/tmp7w92_ixm/test.o
[2025-08-15 02:39:28,918][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp9id9sdhr/test.c -o /tmp/tmp9id9sdhr/test.o
[2025-08-15 02:39:28,919][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpciiph414/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpciiph414/a.out
[2025-08-15 02:39:28,925][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp5fsvjwre/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp5fsvjwre/a.out
[2025-08-15 02:39:28,929][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp7w92_ixm/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp7w92_ixm/a.out
[2025-08-15 02:39:28,936][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpqnqyvb39/test.c -o /tmp/tmpqnqyvb39/test.o
[2025-08-15 02:39:28,937][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp9id9sdhr/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp9id9sdhr/a.out
[2025-08-15 02:39:28,955][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpqnqyvb39/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpqnqyvb39/a.out
[2025-08-15 02:39:28,971][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp3uob55bb/test.c -o /tmp/tmp3uob55bb/test.o
[2025-08-15 02:39:28,971][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpvlb5p2nb/test.c -o /tmp/tmpvlb5p2nb/test.o
[2025-08-15 02:39:28,979][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpagj76cn_/test.c -o /tmp/tmpagj76cn_/test.o
[2025-08-15 02:39:28,985][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp0r9zlmg4/test.c -o /tmp/tmp0r9zlmg4/test.o
[2025-08-15 02:39:28,992][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp3uob55bb/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp3uob55bb/a.out
[2025-08-15 02:39:28,994][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpvlb5p2nb/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpvlb5p2nb/a.out
[2025-08-15 02:39:28,995][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmplzzhnokq/test.c -o /tmp/tmplzzhnokq/test.o
[2025-08-15 02:39:29,000][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpagj76cn_/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpagj76cn_/a.out
[2025-08-15 02:39:29,006][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp0r9zlmg4/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp0r9zlmg4/a.out
[2025-08-15 02:39:29,013][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmplzzhnokq/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmplzzhnokq/a.out
[2025-08-15 02:40:16,251][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,252][__main__][INFO] - Training  2025-08-15 02:40:16.252125
[2025-08-15 02:40:16,260][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,261][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,262][__main__][INFO] - Training  2025-08-15 02:40:16.262022
[2025-08-15 02:40:16,262][__main__][INFO] - Training  2025-08-15 02:40:16.262885
[2025-08-15 02:40:16,311][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,311][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,313][__main__][INFO] - Training  2025-08-15 02:40:16.313088
[2025-08-15 02:40:16,312][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,313][__main__][INFO] - Training  2025-08-15 02:40:16.313874
[2025-08-15 02:40:16,315][__main__][INFO] - Training  2025-08-15 02:40:16.315235
[2025-08-15 02:40:16,316][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,317][__main__][INFO] - Training  2025-08-15 02:40:16.317231
[2025-08-15 02:40:16,318][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,319][__main__][INFO] - Training  2025-08-15 02:40:16.319443
[2025-08-15 02:40:16,320][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,321][__main__][INFO] - Training  2025-08-15 02:40:16.321390
[2025-08-15 02:40:16,321][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,322][__main__][INFO] - Training  2025-08-15 02:40:16.322183
[2025-08-15 02:40:16,325][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,326][__main__][INFO] - Training  2025-08-15 02:40:16.326001
[2025-08-15 02:40:16,332][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,333][__main__][INFO] - Training  2025-08-15 02:40:16.333454
[2025-08-15 02:40:16,359][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,360][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,360][__main__][INFO] - Training  2025-08-15 02:40:16.360427
[2025-08-15 02:40:16,360][__main__][INFO] - Training  2025-08-15 02:40:16.360407
[2025-08-15 02:40:16,360][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,360][__main__][INFO] - Training  2025-08-15 02:40:16.360620
[2025-08-15 02:40:16,362][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,363][__main__][INFO] - Training  2025-08-15 02:40:16.363609
[2025-08-15 02:40:16,367][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,368][__main__][INFO] - Training  2025-08-15 02:40:16.368063
[2025-08-15 02:40:16,368][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,368][__main__][INFO] - Training  2025-08-15 02:40:16.368255
[2025-08-15 02:40:16,368][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,368][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,368][__main__][INFO] - Training  2025-08-15 02:40:16.368452
[2025-08-15 02:40:16,368][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,368][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,369][__main__][INFO] - Training  2025-08-15 02:40:16.369039
[2025-08-15 02:40:16,371][__main__][INFO] - Training  2025-08-15 02:40:16.370991
[2025-08-15 02:40:16,369][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,370][__main__][INFO] - Training  2025-08-15 02:40:16.370310
[2025-08-15 02:40:16,372][__main__][INFO] - Training  2025-08-15 02:40:16.372796
[2025-08-15 02:40:16,375][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 02:40:16,376][__main__][INFO] - Training  2025-08-15 02:40:16.375979
[2025-08-15 02:59:29,755][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:29,755][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:29,756][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:29,756][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:29,788][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:29,788][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:29,790][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:29,790][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:29,906][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:29,907][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:29,972][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:29,973][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,023][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,024][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,084][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,087][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,091][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,094][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,100][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,102][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,137][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,139][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,157][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,158][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,179][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,179][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,184][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,186][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,187][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,188][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,193][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,194][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,215][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,215][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,216][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,216][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,223][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,224][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,227][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,228][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,253][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,255][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,267][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,269][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,282][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,283][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:30,306][__main__][INFO] - Configuration:
save_final_model: false
save_strategy: epoch
save_steps: 1
push_to_hub: false
tags: null
call_post_training: null
logging_strategy: steps
logging_steps: 1
report_to: wandb
wandb_project: step1_sft_jk
wandb_group_name: bespoke_17k
wandb_run_name: teacher_${trainer_log_name}_${max_seq_length}ctx
results_dir: results
exp_name: ${now:%Y.%m.%d}${now:%H%M%S}
output_dir: ${results_dir}/pre_rl_model
resume_from: null
seed: 42
model_log_name: qwen3-14b
model_name_or_path: Qwen/Qwen3-14B
tokenizer_name_or_path: ${model_name_or_path}
model_args:
  _target_: hydra_utils.trl.ModelConfig
  model_name_or_path: ${model_name_or_path}
  trust_remote_code: true
  use_peft: ${use_peft}
  load_in_4bit: ${load_in_4bit}
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
use_peft: false
load_in_4bit: false
tokenizer:
  _target_: hydra_utils.transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: ${tokenizer_name_or_path}
  trust_remote_code: true
  padding_side: left
  model_revision: ${model_args.model_revision}
unsafe_tokenizer_loading: false
make_tokenizer_fn:
  _target_: hydra_utils.fix_pad_token
  tokenizer: ${tokenizer}
  model_name: ${model_name_or_path}
  unsafe: ${unsafe_tokenizer_loading}
peft_config:
  _target_: hydra_utils.trl.get_peft_config
  model_args: ${model_args}
data_log_name: bespoke_stratos_17k_teacher_style_${system_prompt_style}_maskA${mask_teacher_answer}
dataset_id_or_path: bespokelabs/Bespoke-Stratos-17k
dataset_local_directory: ${dataset_id_or_path}
make_dataset_fn:
  dataset_id_or_path: ${dataset_id_or_path}
  dataset_local_directory: ${dataset_local_directory}
  model_name_or_path: ${model_name_or_path}
  _target_: custom_data.sft_data.load_formatted_sft_dataset
  process_line_fn:
    _target_: custom_data.teacher_data.get_process_line_fn
    dataset_id_or_path: ${dataset_id_or_path}
    system_prompt_style: ${system_prompt_style}
    return_student_prompt_info: ${return_student_prompt_info}
    add_text_completions: ${add_text_completions}
  completion_only_training: ${completion_only_training}
  custom_start_of_response: ${custom_start_of_response}
  keep_columns: ${keep_columns}
  add_dataset_indices: ${add_dataset_indices}
dataset_configs:
- all
completion_only_training: true
custom_start_of_response:
  _target_: custom_data.teacher_data.get_teacher_data_mask_delimiter
  system_prompt_style: ${system_prompt_style}
  mask_teacher_answer: ${mask_teacher_answer}
  dataset_id_or_path: ${dataset_id_or_path}
keep_columns: null
add_dataset_indices: false
system_prompt_style: stratos
mask_teacher_answer: true
return_student_prompt_info: true
add_text_completions: false
max_steps: -1
num_train_epochs: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  _target_: builtins.dict
  use_reentrant: false
learning_rate: 1.0e-05
weight_decay: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
lr_scheduler_kwargs: null
warmup_ratio: 0.05
bf16: true
tf32: true
ddp_timeout: 180000000
trainer_log_name: sft
trainer_args:
  _target_: hydra_utils.trl.SFTConfig
  output_dir: ${output_dir}
  max_steps: ${max_steps}
  num_train_epochs: ${num_train_epochs}
  per_device_train_batch_size: ${per_device_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_checkpointing: ${gradient_checkpointing}
  gradient_checkpointing_kwargs: ${gradient_checkpointing_kwargs}
  learning_rate: ${learning_rate}
  weight_decay: ${weight_decay}
  adam_beta1: ${adam_beta1}
  adam_beta2: ${adam_beta2}
  adam_epsilon: ${adam_epsilon}
  max_grad_norm: ${max_grad_norm}
  lr_scheduler_type: ${lr_scheduler_type}
  lr_scheduler_kwargs: ${lr_scheduler_kwargs}
  warmup_ratio: ${warmup_ratio}
  logging_strategy: ${logging_strategy}
  logging_steps: ${logging_steps}
  save_strategy: ${save_strategy}
  save_steps: ${save_steps}
  report_to: ${report_to}
  run_name: ${wandb_run_name}
  bf16: ${bf16}
  tf32: ${tf32}
  seed: ${seed}
  ddp_timeout: ${ddp_timeout}
  max_seq_length: ${max_seq_length}
  packing: ${packing}
  do_eval: ${do_eval}
trainer:
  _target_: hydra_utils.trl.SFTTrainer
  model: ${model_name_or_path}
  args: ${trainer_args}
  peft_config: ${peft_config}
max_seq_length: 16384
do_eval: false
packing: false
eval_strategy: steps
eval_steps: 100
train_batch_size: 24
per_device_train_batch_size: 1
gradient_accumulation_steps: ???
per_device_eval_batch_size: 4

[2025-08-15 02:59:30,307][__main__][INFO] - Injected DeepSpeed config: cfgs/ds_config_zero3.json
[2025-08-15 02:59:31,809][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:31,817][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:31,819][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:31,936][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:31,936][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:31,946][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:31,968][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,004][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,023][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,067][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,075][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,085][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,086][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,094][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,109][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,124][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,143][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,147][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,161][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,198][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,224][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,253][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,279][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,279][__main__][INFO] - Accumulation steps 1 ----
[2025-08-15 02:59:32,939][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:32,940][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:32,940][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:32,942][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:32,942][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:32,942][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:32,942][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,203][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,204][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,205][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,205][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,205][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,205][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,206][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,217][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,217][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,220][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,220][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,220][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,220][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:33,220][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:35,649][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:35,730][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:35,755][datasets][INFO] - PyTorch version 2.6.0+cu124 available.
[2025-08-15 02:59:42,582][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpe6o03y4d/test.c -o /tmp/tmpe6o03y4d/test.o
[2025-08-15 02:59:42,583][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpbfl2h_vn/test.c -o /tmp/tmpbfl2h_vn/test.o
[2025-08-15 02:59:42,583][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmphf9s0sf3/test.c -o /tmp/tmphf9s0sf3/test.o
[2025-08-15 02:59:42,583][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpz6wxl3n8/test.c -o /tmp/tmpz6wxl3n8/test.o
[2025-08-15 02:59:42,597][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmppuvky_er/test.c -o /tmp/tmppuvky_er/test.o
[2025-08-15 02:59:42,597][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpo985z95h/test.c -o /tmp/tmpo985z95h/test.o
[2025-08-15 02:59:42,604][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpyajb0pfx/test.c -o /tmp/tmpyajb0pfx/test.o
[2025-08-15 02:59:42,605][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmptlu772w6/test.c -o /tmp/tmptlu772w6/test.o
[2025-08-15 02:59:42,606][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp3eqzhxu_/test.c -o /tmp/tmp3eqzhxu_/test.o
[2025-08-15 02:59:42,606][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpj0lsrhy2/test.c -o /tmp/tmpj0lsrhy2/test.o
[2025-08-15 02:59:42,610][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpbfl2h_vn/test.o -laio -o /tmp/tmpbfl2h_vn/a.out
[2025-08-15 02:59:42,610][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmphf9s0sf3/test.o -laio -o /tmp/tmphf9s0sf3/a.out
[2025-08-15 02:59:42,610][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpe6o03y4d/test.o -laio -o /tmp/tmpe6o03y4d/a.out
[2025-08-15 02:59:42,611][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpz6wxl3n8/test.o -laio -o /tmp/tmpz6wxl3n8/a.out
[2025-08-15 02:59:42,623][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmptlu772w6/test.o -laio -o /tmp/tmptlu772w6/a.out
[2025-08-15 02:59:42,624][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpo985z95h/test.o -laio -o /tmp/tmpo985z95h/a.out
[2025-08-15 02:59:42,624][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmppuvky_er/test.o -laio -o /tmp/tmppuvky_er/a.out
[2025-08-15 02:59:42,624][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpyajb0pfx/test.o -laio -o /tmp/tmpyajb0pfx/a.out
[2025-08-15 02:59:42,625][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpj0lsrhy2/test.o -laio -o /tmp/tmpj0lsrhy2/a.out
[2025-08-15 02:59:42,627][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp3eqzhxu_/test.o -laio -o /tmp/tmp3eqzhxu_/a.out
[2025-08-15 02:59:42,704][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmprflxsmgl/test.c -o /tmp/tmprflxsmgl/test.o
[2025-08-15 02:59:42,720][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmprflxsmgl/test.o -laio -o /tmp/tmprflxsmgl/a.out
[2025-08-15 02:59:42,778][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpem4gp3ze/test.c -o /tmp/tmpem4gp3ze/test.o
[2025-08-15 02:59:42,789][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpj659f351/test.c -o /tmp/tmpj659f351/test.o
[2025-08-15 02:59:42,797][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpem4gp3ze/test.o -laio -o /tmp/tmpem4gp3ze/a.out
[2025-08-15 02:59:42,806][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpj659f351/test.o -laio -o /tmp/tmpj659f351/a.out
[2025-08-15 02:59:42,814][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpaoe4j31j/test.c -o /tmp/tmpaoe4j31j/test.o
[2025-08-15 02:59:42,834][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpaoe4j31j/test.o -laio -o /tmp/tmpaoe4j31j/a.out
[2025-08-15 02:59:43,029][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpdnsqj53d/test.c -o /tmp/tmpdnsqj53d/test.o
[2025-08-15 02:59:43,047][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpdnsqj53d/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpdnsqj53d/a.out
[2025-08-15 02:59:43,052][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp6d2t1wh3/test.c -o /tmp/tmp6d2t1wh3/test.o
[2025-08-15 02:59:43,056][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpszzjsuai/test.c -o /tmp/tmpszzjsuai/test.o
[2025-08-15 02:59:43,068][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp6d2t1wh3/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp6d2t1wh3/a.out
[2025-08-15 02:59:43,080][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpszzjsuai/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpszzjsuai/a.out
[2025-08-15 02:59:43,093][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp8wrwvdyk/test.c -o /tmp/tmp8wrwvdyk/test.o
[2025-08-15 02:59:43,093][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpwjlfeg2g/test.c -o /tmp/tmpwjlfeg2g/test.o
[2025-08-15 02:59:43,093][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpx1m8u1mm/test.c -o /tmp/tmpx1m8u1mm/test.o
[2025-08-15 02:59:43,094][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp8en1c2i8/test.c -o /tmp/tmp8en1c2i8/test.o
[2025-08-15 02:59:43,097][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmphier2i53/test.c -o /tmp/tmphier2i53/test.o
[2025-08-15 02:59:43,098][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpyq3won7l/test.c -o /tmp/tmpyq3won7l/test.o
[2025-08-15 02:59:43,099][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp7aggvti5/test.c -o /tmp/tmp7aggvti5/test.o
[2025-08-15 02:59:43,099][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp0xu_h4sa/test.c -o /tmp/tmp0xu_h4sa/test.o
[2025-08-15 02:59:43,119][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp7aggvti5/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp7aggvti5/a.out
[2025-08-15 02:59:43,120][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp8wrwvdyk/test.o -laio -o /tmp/tmp8wrwvdyk/a.out
[2025-08-15 02:59:43,120][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpwjlfeg2g/test.o -laio -o /tmp/tmpwjlfeg2g/a.out
[2025-08-15 02:59:43,120][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmphier2i53/test.o -laio -o /tmp/tmphier2i53/a.out
[2025-08-15 02:59:43,120][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpyq3won7l/test.o -laio -o /tmp/tmpyq3won7l/a.out
[2025-08-15 02:59:43,120][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpx1m8u1mm/test.o -laio -o /tmp/tmpx1m8u1mm/a.out
[2025-08-15 02:59:43,121][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp8en1c2i8/test.o -laio -o /tmp/tmp8en1c2i8/a.out
[2025-08-15 02:59:43,121][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp0xu_h4sa/test.o -laio -o /tmp/tmp0xu_h4sa/a.out
[2025-08-15 02:59:43,145][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpjmdswha7/test.c -o /tmp/tmpjmdswha7/test.o
[2025-08-15 02:59:43,163][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpjmdswha7/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpjmdswha7/a.out
[2025-08-15 02:59:43,191][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpy623mrsu/test.c -o /tmp/tmpy623mrsu/test.o
[2025-08-15 02:59:43,208][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpy623mrsu/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpy623mrsu/a.out
[2025-08-15 02:59:43,216][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmprfpf5rbp/test.c -o /tmp/tmprfpf5rbp/test.o
[2025-08-15 02:59:43,235][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmprfpf5rbp/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmprfpf5rbp/a.out
[2025-08-15 02:59:43,264][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpt3g52980/test.c -o /tmp/tmpt3g52980/test.o
[2025-08-15 02:59:43,273][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp42puyfmh/test.c -o /tmp/tmp42puyfmh/test.o
[2025-08-15 02:59:43,290][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpt3g52980/test.o -laio -o /tmp/tmpt3g52980/a.out
[2025-08-15 02:59:43,291][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp42puyfmh/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp42puyfmh/a.out
[2025-08-15 02:59:43,296][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp1cddrmmu/test.c -o /tmp/tmp1cddrmmu/test.o
[2025-08-15 02:59:43,304][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp_kawv2zq/test.c -o /tmp/tmp_kawv2zq/test.o
[2025-08-15 02:59:43,305][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpnlt474mk/test.c -o /tmp/tmpnlt474mk/test.o
[2025-08-15 02:59:43,317][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp1cddrmmu/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp1cddrmmu/a.out
[2025-08-15 02:59:43,322][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp_kawv2zq/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp_kawv2zq/a.out
[2025-08-15 02:59:43,324][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpnlt474mk/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpnlt474mk/a.out
[2025-08-15 02:59:43,394][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmprbsuth_b/test.c -o /tmp/tmprbsuth_b/test.o
[2025-08-15 02:59:43,418][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmprbsuth_b/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmprbsuth_b/a.out
[2025-08-15 02:59:43,424][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp3eo9yo7m/test.c -o /tmp/tmp3eo9yo7m/test.o
[2025-08-15 02:59:43,427][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpwk62wyc3/test.c -o /tmp/tmpwk62wyc3/test.o
[2025-08-15 02:59:43,449][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp3eo9yo7m/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp3eo9yo7m/a.out
[2025-08-15 02:59:43,449][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpwk62wyc3/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpwk62wyc3/a.out
[2025-08-15 02:59:43,534][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmppa_b4abj/test.c -o /tmp/tmppa_b4abj/test.o
[2025-08-15 02:59:43,552][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmppa_b4abj/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmppa_b4abj/a.out
[2025-08-15 02:59:43,574][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp_grxcd32/test.c -o /tmp/tmp_grxcd32/test.o
[2025-08-15 02:59:43,591][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp_grxcd32/test.o -laio -o /tmp/tmp_grxcd32/a.out
[2025-08-15 02:59:43,595][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpo54iehhm/test.c -o /tmp/tmpo54iehhm/test.o
[2025-08-15 02:59:43,621][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmptl2en26k/test.c -o /tmp/tmptl2en26k/test.o
[2025-08-15 02:59:43,626][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpo54iehhm/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpo54iehhm/a.out
[2025-08-15 02:59:43,628][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpbysian29/test.c -o /tmp/tmpbysian29/test.o
[2025-08-15 02:59:43,639][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmptl2en26k/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmptl2en26k/a.out
[2025-08-15 02:59:43,645][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpbysian29/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpbysian29/a.out
[2025-08-15 02:59:43,742][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmppuva85ya/test.c -o /tmp/tmppuva85ya/test.o
[2025-08-15 02:59:43,766][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmppuva85ya/test.o -laio -o /tmp/tmppuva85ya/a.out
[2025-08-15 02:59:43,766][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpyl3ec4bg/test.c -o /tmp/tmpyl3ec4bg/test.o
[2025-08-15 02:59:43,772][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpbufgt1wv/test.c -o /tmp/tmpbufgt1wv/test.o
[2025-08-15 02:59:43,786][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpyl3ec4bg/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpyl3ec4bg/a.out
[2025-08-15 02:59:43,789][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpbufgt1wv/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpbufgt1wv/a.out
[2025-08-15 02:59:43,810][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpfkmwhqyv/test.c -o /tmp/tmpfkmwhqyv/test.o
[2025-08-15 02:59:43,826][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp06__6ekj/test.c -o /tmp/tmp06__6ekj/test.o
[2025-08-15 02:59:43,826][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpfkmwhqyv/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpfkmwhqyv/a.out
[2025-08-15 02:59:43,846][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp06__6ekj/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp06__6ekj/a.out
[2025-08-15 02:59:43,893][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmp3infpcn6/test.c -o /tmp/tmp3infpcn6/test.o
[2025-08-15 02:59:43,912][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmp3infpcn6/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmp3infpcn6/a.out
[2025-08-15 02:59:44,085][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -O2 -isystem /home/Competition2025/P07/shareP07/sakana_rlt_honban/include -fPIC -c /tmp/tmpgml1rmmr/test.c -o /tmp/tmpgml1rmmr/test.o
[2025-08-15 02:59:44,103][root][INFO] - gcc -pthread -B /home/Competition2025/P07/shareP07/sakana_rlt_honban/compiler_compat /tmp/tmpgml1rmmr/test.o -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/ -L/home/Competition2025/P07/shareP07/sakana_rlt_honban/lib64 -lcufile -o /tmp/tmpgml1rmmr/a.out
[2025-08-15 03:00:22,071][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,071][__main__][INFO] - Training  2025-08-15 03:00:22.071703
[2025-08-15 03:00:22,101][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,101][__main__][INFO] - Training  2025-08-15 03:00:22.101722
[2025-08-15 03:00:22,106][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,107][__main__][INFO] - Training  2025-08-15 03:00:22.107075
[2025-08-15 03:00:22,111][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,111][__main__][INFO] - Training  2025-08-15 03:00:22.111371
[2025-08-15 03:00:22,130][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,130][__main__][INFO] - Training  2025-08-15 03:00:22.130233
[2025-08-15 03:00:22,132][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,133][__main__][INFO] - Training  2025-08-15 03:00:22.132986
[2025-08-15 03:00:22,215][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,215][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,215][__main__][INFO] - Training  2025-08-15 03:00:22.215613
[2025-08-15 03:00:22,215][__main__][INFO] - Training  2025-08-15 03:00:22.215626
[2025-08-15 03:00:22,220][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,220][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,221][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,222][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,235][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,235][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,235][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,235][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,275][__main__][INFO] - Training  2025-08-15 03:00:22.275212
[2025-08-15 03:00:22,275][__main__][INFO] - Training  2025-08-15 03:00:22.275249
[2025-08-15 03:00:22,275][__main__][INFO] - Training  2025-08-15 03:00:22.275285
[2025-08-15 03:00:22,275][__main__][INFO] - Training  2025-08-15 03:00:22.275323
[2025-08-15 03:00:22,275][__main__][INFO] - Training  2025-08-15 03:00:22.275376
[2025-08-15 03:00:22,275][__main__][INFO] - Training  2025-08-15 03:00:22.275398
[2025-08-15 03:00:22,275][__main__][INFO] - Training  2025-08-15 03:00:22.275441
[2025-08-15 03:00:22,275][__main__][INFO] - Training  2025-08-15 03:00:22.275415
[2025-08-15 03:00:22,301][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,301][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,302][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,303][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,310][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,311][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,319][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,319][__main__][INFO] - No existing checkpoint, initializing new model
[2025-08-15 03:00:22,333][__main__][INFO] - Training  2025-08-15 03:00:22.333675
[2025-08-15 03:00:22,333][__main__][INFO] - Training  2025-08-15 03:00:22.333687
[2025-08-15 03:00:22,333][__main__][INFO] - Training  2025-08-15 03:00:22.333717
[2025-08-15 03:00:22,333][__main__][INFO] - Training  2025-08-15 03:00:22.333766
[2025-08-15 03:00:22,333][__main__][INFO] - Training  2025-08-15 03:00:22.333800
[2025-08-15 03:00:22,333][__main__][INFO] - Training  2025-08-15 03:00:22.333867
[2025-08-15 03:00:22,333][__main__][INFO] - Training  2025-08-15 03:00:22.333838
[2025-08-15 03:00:22,333][__main__][INFO] - Training  2025-08-15 03:00:22.333914
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003216
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003648
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003757
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003350
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003550
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003668
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003225
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003684
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003780
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003238
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003587
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003722
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003276
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003545
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003698
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003241
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003545
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003755
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003288
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003569
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003761
[2025-08-15 04:55:05,004][__main__][INFO] - Training complete 2025-08-15 04:55:05.004110
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003624
[2025-08-15 04:55:05,003][__main__][INFO] - Training complete 2025-08-15 04:55:05.003765
