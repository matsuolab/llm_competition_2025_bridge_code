# SFT Training Configuration for Qwen3-32B

# Environment settings
gpus_per_node: 8
nnodes: 2
wandb_entity: "LLMcompe-Team-Watanabe"
wandb_project_name: "verl_test"
wandb_run_name: "qwen3_235B_SFT_test"

# Network settings
master_addr: "localhost"
master_port: 29500
node_rank: 0

# Data configuration
data:
  train_files: "/home/Competition2025/P04/shareP04/data/textbookreasoning/train.parquet"
  val_files: "/home/Competition2025/P04/shareP04/data/textbookreasoning/test.parquet"
  prompt_key: "extra_info"
  response_key: "extra_info"
  prompt_dict_keys: ["question"]
  response_dict_keys: ["answer"]
  micro_batch_size_per_gpu: 1
  train_batch_size: 16
  max_length: 4096
  truncation: right

# Model configuration
model:
  partial_pretrain: "/home/Competition2025/P04/shareP04/models/Qwen3-235B-A22B"
  fsdp_config:
    model_dtype: bf16
  lora_rank: 8
  lora_alpha: 16
  target_modules: ["q_proj","k_proj","v_proj","o_proj"]
  enable_gradient_checkpointing: True
  use_liger: False
  strategy: fsdp

# Trainer configuration
trainer:
  total_epochs: 1
  save_freq: 200
  test_freq: 50000
  logger:
    - console
    - wandb
  default_local_dir: "$HOME/training/multinode/sft/checkpoints/Qwen3-235B/textbookreasoning"